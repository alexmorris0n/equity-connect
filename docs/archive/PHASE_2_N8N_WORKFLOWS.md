# ðŸ”„ Phase 2: n8n Workflows for BatchData + Waterfall Skip-Trace

## ðŸ“‹ Overview

Now that the database is ready, we need to build the n8n workflows that orchestrate the entire lead generation pipeline.

---

## ðŸŽ¯ Workflows to Build

### **1. BatchData Pull Worker** (Primary)
**Trigger**: Cron schedule (hourly, Mon-Fri 9am-5pm)
**Purpose**: Idempotently pull leads from BatchData API with stop-when-known logic

**Flow:**
```
1. Cron Trigger (every hour)
   â†“
2. Function: Build query_sig from parameters
   â†“
3. Supabase: Get bookmark (last_page_fetched)
   â†“
4. HTTP Request: BatchData API (page = bookmark + 1)
   â†“
5. Function: Check stop-when-known
   â”œâ”€ IF seen before â†’ End (caught up)
   â””â”€ IF new â†’ Continue
   â†“
6. Split In Batches: For each record
   â”œâ”€ Function: Compute addr_hash
   â”œâ”€ Supabase RPC: upsert_lead()
   â””â”€ Queue: Add to enrichment pipeline
   â†“
7. Supabase: Insert lead_source_events
   â†“
8. Supabase: Update source_bookmarks
   â†“
9. Loop to step 4 (next page) or End
```

**Parameters** (per market):
- `zip_codes`: ['90028', '90038', '90046']
- `filters`: {owner_occupied: true, property_type: 'single_family', age: '>= 62', equity: '> 100000'}
- `sort`: 'created_desc'

---

### **2. Enrichment Pipeline** (Waterfall)
**Trigger**: Pipeline event (stage='enrich')
**Purpose**: Run 3-stage waterfall enrichment

**Flow:**
```
1. Trigger: pipeline_events WHERE stage='enrich'
   â†“
2. Supabase: Get lead by ID
   â†“
3. STAGE 1 - Melissa Enrichment
   â”œâ”€ HTTP: Melissa Personator API
   â”œâ”€ Function: Extract MAK, phones, emails
   â”œâ”€ Supabase RPC: merge_contact_point() for each
   â””â”€ Supabase: Update lead with MAK
   â†“
4. Function: compute_quality_score()
   â”œâ”€ IF score >= 60 â†’ Skip to Stage 4 (verified as contactable)
   â””â”€ IF score < 60 â†’ Continue to Stage 2
   â†“
5. STAGE 2 - BatchData Skip-Trace (only if needed)
   â”œâ”€ HTTP: BatchData Skip-Trace API
   â”œâ”€ Function: Extract phones, emails
   â””â”€ Supabase RPC: merge_contact_point() for each
   â†“
6. Function: compute_quality_score() again
   â†“
7. STAGE 3 - Verification
   â”œâ”€ For each email:
   â”‚  â”œâ”€ HTTP: SMTP Verification or Instantly API
   â”‚  â””â”€ Supabase: Update verified=true if pass
   â”œâ”€ For each phone:
   â”‚  â”œâ”€ HTTP: SignalWire Number Lookup
   â”‚  â””â”€ Supabase: Update verified=true + type if pass
   â†“
8. Function: compute_quality_score() final
   â†“
9. Supabase RPC: update_lead_status_from_score()
   â†“
10. IF status='contactable'
    â””â”€ Queue: Add to campaign_feeder
```

---

### **3. Campaign Feeder** (Daily)
**Trigger**: Cron schedule (daily 8am)
**Purpose**: Move contactable leads to Instantly campaigns

**Flow:**
```
1. Cron Trigger (daily 8am)
   â†“
2. Supabase: Query vw_campaign_ready_leads LIMIT 250
   â†“
3. Split In Batches: For each lead
   â”œâ”€ Function: Build Instantly payload
   â”œâ”€ HTTP: Instantly Add Contact API
   â”œâ”€ Function: Generate microsite URL
   â”œâ”€ Supabase: Create microsite record
   â”œâ”€ Supabase: Update lead
   â”‚  â”œâ”€ added_to_campaign_at = NOW()
   â”‚  â”œâ”€ microsite_url = url
   â”‚  â””â”€ campaign_status = 'queued'
   â””â”€ Supabase: Insert interaction record
   â†“
4. Function: Log completion (success count)
```

---

### **4. Error Handler** (Monitor)
**Trigger**: Cron schedule (every 5 min)
**Purpose**: Retry failed operations from DLQ

**Flow:**
```
1. Cron Trigger (every 5 min)
   â†“
2. Supabase: SELECT * FROM dlq WHERE retry_after <= NOW() AND attempts < 3
   â†“
3. Split In Batches: For each failed item
   â”œâ”€ Switch: Based on stage
   â”‚  â”œâ”€ 'pull' â†’ Retry BatchData pull
   â”‚  â”œâ”€ 'enrich' â†’ Retry enrichment
   â”‚  â”œâ”€ 'verify' â†’ Retry verification
   â”‚  â””â”€ 'campaign' â†’ Retry campaign add
   â”œâ”€ On Success:
   â”‚  â””â”€ Supabase: DELETE FROM dlq WHERE id = ...
   â””â”€ On Failure:
      â””â”€ Supabase: UPDATE dlq SET attempts = attempts + 1, retry_after = NOW() + (attempts * 5 min)
```

---

## ðŸ› ï¸ Required n8n Nodes

### **Core Nodes:**
- âœ… **Cron** - Schedule triggers
- âœ… **HTTP Request** - API calls (BatchData, Melissa, SignalWire, Instantly)
- âœ… **Supabase** - Database operations
- âœ… **Function** - JavaScript logic
- âœ… **Split In Batches** - Process arrays
- âœ… **IF** - Conditional branching
- âœ… **Switch** - Multi-way branching
- âœ… **Merge** - Combine data flows

### **Custom Functions:**
```javascript
// compute_addr_hash
function computeAddrHash(line1, city, state, zip) {
  const normalized = [
    line1?.toUpperCase().trim(),
    city?.toUpperCase().trim(),
    state?.toUpperCase().trim(),
    zip?.substring(0, 5)
  ].join('|');
  
  return crypto.createHash('sha256').update(normalized).digest('hex');
}

// build_query_sig
function buildQuerySig(params) {
  const normalized = JSON.stringify(params, Object.keys(params).sort());
  return crypto.createHash('sha256').update(normalized).digest('hex');
}

// stop_when_known
async function stopWhenKnown(source, querySig, vendorIds) {
  const { data } = await supabase.rpc('has_vendor_ids_been_seen', {
    p_source: source,
    p_query_sig: querySig,
    p_vendor_ids: vendorIds
  });
  return data; // true/false
}
```

---

## ðŸ“Š Workflow Parameters

### **BatchData Pull Worker:**
```json
{
  "market": "hollywood",
  "zip_codes": ["90028", "90038", "90046"],
  "filters": {
    "owner_occupied": true,
    "property_type": "single_family",
    "age_min": 62,
    "equity_min": 100000
  },
  "sort": "created_desc",
  "page_size": 100
}
```

### **Enrichment Pipeline:**
```json
{
  "melissa_api_key": "{{$env.MELISSA_API_KEY}}",
  "batchdata_api_key": "{{$env.BATCHDATA_API_KEY}}",
  "signalwire_project": "{{$env.SIGNALWIRE_PROJECT}}",
  "signalwire_token": "{{$env.SIGNALWIRE_TOKEN}}",
  "quality_threshold": 60,
  "skip_stage2_if_contactable": true
}
```

### **Campaign Feeder:**
```json
{
  "daily_cap": 250,
  "instantly_api_key": "{{$env.INSTANTLY_API_KEY}}",
  "instantly_campaign_id": "{{$env.INSTANTLY_CAMPAIGN_ID}}",
  "base_url": "https://equityconnect.com"
}
```

---

## ðŸ” Environment Variables Needed

```env
# BatchData
BATCHDATA_API_KEY=your_key_here
BATCHDATA_BASE_URL=https://api.batchdata.io

# Melissa
MELISSA_API_KEY=your_key_here
MELISSA_BASE_URL=https://personator.melissadata.net

# SignalWire
SIGNALWIRE_PROJECT=your_project_id
SIGNALWIRE_TOKEN=your_token
SIGNALWIRE_SPACE=your_space.signalwire.com

# Instantly
INSTANTLY_API_KEY=your_key_here
INSTANTLY_API_BASE=https://api.instantly.ai

# Supabase
SUPABASE_URL=https://mxnqfwuhvurajrgoefyg.supabase.co
SUPABASE_SERVICE_KEY=your_service_role_key

# Application
NEXT_PUBLIC_BASE_URL=https://equityconnect.com
```

---

## ðŸ“ Implementation Order

1. âœ… **BatchData Pull Worker** - Start getting data
2. âœ… **Enrichment Pipeline** - Process the data
3. âœ… **Campaign Feeder** - Send to Instantly
4. âœ… **Error Handler** - Monitor and retry

---

## ðŸ§ª Testing Strategy

### **1. Test BatchData Pull (Dry Run)**
- Use test API credentials
- Pull 1 page only
- Verify `lead_source_events` recorded
- Verify `source_bookmarks` updated
- Check for duplicates (should update, not insert)

### **2. Test Enrichment (Single Lead)**
- Pick one test lead
- Run through all 3 stages
- Verify phones/emails merged correctly
- Verify quality_score computed
- Verify status updated based on score

### **3. Test Campaign Feeder (1 Lead)**
- Query `vw_campaign_ready_leads LIMIT 1`
- Send to Instantly (test campaign)
- Verify microsite created
- Verify `added_to_campaign_at` set

### **4. Test Stop-When-Known**
- Pull same page twice
- Second pull should detect known IDs and stop
- Verify no duplicate inserts

---

## ðŸš€ Ready to Build?

Now that the database is ready, we can build these workflows in n8n.

**Next**: Build the BatchData Pull Worker workflow first?


