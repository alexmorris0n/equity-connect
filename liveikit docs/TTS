LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Overview

---

# Text-to-speech (TTS) models

> Voices and plugins to add realtime speech to your voice agents.

## Overview

Voice agent speech is produced by a TTS model, configured with a voice profile that specifies tone, accent, and other qualitative characteristics of the speech. The TTS model runs on output from an [LLM](https://docs.livekit.io/agents/models/llm.md) model to speak the agent response to the user.

You can choose a voice model served through LiveKit Inference or you can use a plugin to connect directly to a wider range of model providers with your own account.

## LiveKit Inference

The following models are available in [LiveKit Inference](https://docs.livekit.io/agents/models.md#inference). Refer to the guide for each model for more details on additional configuration options. A limited selection of [Suggested voices](#voices) are available, as well as a wider selection through each provider's documentation.

- **[Cartesia](https://docs.livekit.io/agents/models/tts/inference/cartesia.md)**: Reference for Cartesia TTS in LiveKit Inference.

- **[ElevenLabs](https://docs.livekit.io/agents/models/tts/inference/elevenlabs.md)**: Reference for ElevenLabs TTS with LiveKit Inference.

- **[Inworld](https://docs.livekit.io/agents/models/tts/inference/inworld.md)**: Reference for Inworld TTS in LiveKit Inference.

- **[Rime](https://docs.livekit.io/agents/models/tts/inference/rime.md)**: Reference for Rime TTS in LiveKit Inference.

### Suggested voices

The following voices are good choices for overall quality and performance. Each provider has a much larger selection of voices to choose from, which you can find in their documentation. In addition to the voices below, you can choose to use other voices through LiveKit Inference.

Click the copy icon to copy the voice ID to use in your agent session.

| Provider | Name | Description | Language | ID |
| -------- | ---- | ----------- | -------- | -------- |
| Cartesia | Blake | Energetic American adult male | `en-US` | `cartesia/sonic-3:a167e0f3-df7e-4d52-a9c3-f949145efdab` |
| Cartesia | Daniela | Calm and trusting Mexican female | `es-MX` | `cartesia/sonic-3:5c5ad5e7-1020-476b-8b91-fdcbe9cc313c` |
| Cartesia | Jacqueline | Confident, young American adult female | `en-US` | `cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc` |
| Cartesia | Robyn | Neutral, mature Australian female | `en-AU` | `cartesia/sonic-3:f31cc6a7-c1e8-4764-980c-60a361443dd1` |
| ElevenLabs | Alice | Clear and engaging, friendly British woman | `en-GB` | `elevenlabs/eleven_turbo_v2_5:Xb7hH8MSUJpSbSDYk0k2` |
| ElevenLabs | Chris | Natural and real American male | `en-US` | `elevenlabs/eleven_turbo_v2_5:iP95p4xoKVk53GoZ742B` |
| ElevenLabs | Eric | A smooth tenor Mexican male | `es-MX` | `elevenlabs/eleven_turbo_v2_5:cjVigY5qzO86Huf0OWal` |
| ElevenLabs | Jessica | Young and popular, playful American female | `en-US` | `elevenlabs/eleven_turbo_v2_5:cgSgspJ2msm6clMCkdW9` |
| Rime | Astra | Chipper, upbeat American female | `en-US` | `rime/arcana:astra` |
| Rime | Celeste | Chill Gen-Z American female | `en-US` | `rime/arcana:celeste` |
| Rime | Luna | Chill but excitable American female | `en-US` | `rime/arcana:luna` |
| Rime | Ursa | Young, emo American male | `en-US` | `rime/arcana:ursa` |
| Inworld | Ashley | Warm, natural American female | `en-US` | `inworld/inworld-tts-1:Ashley` |
| Inworld | Diego | Soothing, gentle Mexican male | `es-MX` | `inworld/inworld-tts-1:Diego ` |
| Inworld | Edward | Fast-talking, emphatic American male | `en-US` | `inworld/inworld-tts-1:Edward` |
| Inworld | Olivia | Upbeat, friendly British female | `en-GB` | `inworld/inworld-tts-1:Olivia` |

## Usage

To set up TTS in an `AgentSession`, provide a descriptor with both the desired model and voice. LiveKit Inference manages the connection to the model automatically. Consult the [Suggested voices](#voices) list for suggeted voices, or view the model reference for more voices.

**Python**:

```python
from livekit.agents import AgentSession

session = AgentSession(
    tts="cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc",
    # ... llm, stt, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

const session = new AgentSession({
    tts: "cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc",
    // ... llm, stt, etc.
})

```

### Additional parameters

More configuration options, such as custom pronunciation, are available for each model. To set additional parameters, use the `TTS` class from the `inference` module. Consult each model reference for examples and available parameters.

## Plugins

The LiveKit Agents framework also includes a variety of open source [plugins](https://docs.livekit.io/agents/models.md#plugins) for a wide range of TTS providers. Plugins are especially useful if you need custom voices, including voice cloning support. These plugins require authentication with the provider yourself, usually via an API key. You are responsible for setting up your own account and managing your own billing and credentials. The plugins are listed below, along with their availability for Python or Node.js.

| Provider | Python | Node.js |
| -------- | ------ | ------- |
| [Amazon Polly](https://docs.livekit.io/agents/models/tts/plugins/aws.md) | âœ“ | â€” |
| [Azure AI Speech](https://docs.livekit.io/agents/models/tts/plugins/azure.md) | âœ“ | â€” |
| [Azure OpenAI](https://docs.livekit.io/agents/models/tts/plugins/azure-openai.md) | âœ“ | â€” |
| [Baseten](https://docs.livekit.io/agents/models/tts/plugins/baseten.md) | âœ“ | â€” |
| [Cartesia](https://docs.livekit.io/agents/models/tts/plugins/cartesia.md) | âœ“ | âœ“ |
| [Deepgram](https://docs.livekit.io/agents/models/tts/plugins/deepgram.md) | âœ“ | â€” |
| [ElevenLabs](https://docs.livekit.io/agents/models/tts/plugins/elevenlabs.md) | âœ“ | âœ“ |
| [Gemini](https://docs.livekit.io/agents/models/tts/plugins/gemini.md) | âœ“ | â€” |
| [Google Cloud](https://docs.livekit.io/agents/models/tts/plugins/google.md) | âœ“ | â€” |
| [Groq](https://docs.livekit.io/agents/models/tts/plugins/groq.md) | âœ“ | â€” |
| [Hume](https://docs.livekit.io/agents/models/tts/plugins/hume.md) | âœ“ | â€” |
| [Inworld](https://docs.livekit.io/agents/models/tts/plugins/inworld.md) | âœ“ | â€” |
| [LMNT](https://docs.livekit.io/agents/models/tts/plugins/lmnt.md) | âœ“ | â€” |
| [MiniMax](https://docs.livekit.io/agents/models/tts/plugins/minimax.md) | âœ“ | â€” |
| [Neuphonic](https://docs.livekit.io/agents/models/tts/plugins/neuphonic.md) | âœ“ | âœ“ |
| [OpenAI](https://docs.livekit.io/agents/models/tts/plugins/openai.md) | âœ“ | âœ“ |
| [Resemble AI](https://docs.livekit.io/agents/models/tts/plugins/resemble.md) | âœ“ | âœ“ |
| [Rime](https://docs.livekit.io/agents/models/tts/plugins/rime.md) | âœ“ | âœ“ |
| [Sarvam](https://docs.livekit.io/agents/models/tts/plugins/sarvam.md) | âœ“ | â€” |
| [Smallest AI](https://docs.livekit.io/agents/models/tts/plugins/smallestai.md) | âœ“ | â€” |
| [Speechify](https://docs.livekit.io/agents/models/tts/plugins/speechify.md) | âœ“ | â€” |
| [Spitch](https://docs.livekit.io/agents/models/tts/plugins/spitch.md) | âœ“ | â€” |

Have another provider in mind? LiveKit is open source and welcomes [new plugin contributions](https://docs.livekit.io/agents/models.md#contribute).

## Advanced features

The following sections cover more advanced topics common to all TTS providers. For more detailed reference on individual provider configuration, consult the model reference or plugin documentation for that provider.

### Custom TTS

To create an entirely custom TTS, implement the [TTS node](https://docs.livekit.io/agents/build/nodes.md#tts_node) in your agent.

### Standalone usage

You can use a `TTS` instance as a standalone component by creating a stream. Use `push_text` to add text to the stream, and then consume a stream of `SynthesizedAudio` to publish as [realtime audio](https://docs.livekit.io/home/client/tracks.md) to another participant.

Here is an example of a standalone TTS app:

** Filename: `agent.py`**

```python
from livekit import agents, rtc
from livekit.agents import AgentServer
from livekit.agents.tts import SynthesizedAudio
from livekit.plugins import cartesia
from typing import AsyncIterable


server = AgentServer()

@server.rtc_session()
async def my_agent(ctx: agents.JobContext):
    text_stream: AsyncIterable[str] = ... # you need to provide a stream of text
    audio_source = rtc.AudioSource(44100, 1)

    track = rtc.LocalAudioTrack.create_audio_track("agent-audio", audio_source)
    await ctx.room.local_participant.publish_track(track)

    tts = cartesia.TTS(model="sonic-english")
    tts_stream = tts.stream()

    # create a task to consume and publish audio frames
    ctx.create_task(send_audio(tts_stream))

    # push text into the stream, TTS stream will emit audio frames along with events
    # indicating sentence (or segment) boundaries.
    async for text in text_stream:
        tts_stream.push_text(text)
    tts_stream.end_input()

    async def send_audio(audio_stream: AsyncIterable[SynthesizedAudio]):
        async for a in audio_stream:
            await audio_source.capture_frame(a.audio.frame)

if __name__ == "__main__":
    agents.cli.run_app(server)

```

## Additional resources

The following resources cover related topics that may be useful for your application.

- **[Agent speech docs](https://docs.livekit.io/agents/build/audio.md)**: Explore the speech capabilities and features of LiveKit Agents.

- **[Pipeline nodes](https://docs.livekit.io/agents/build/nodes.md)**: Learn how to customize the behavior of your agent by overriding nodes in the voice pipeline.

- **[Inference pricing](https://livekit.io/pricing/inference#tts)**: The latest pricing information for TTS models in LiveKit Inference.

---

This document was rendered at 2025-11-19T10:47:22.234Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts.md](https://docs.livekit.io/agents/models/tts.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º LiveKit Inference â€º Cartesia

---

# Cartesia TTS

> Reference for Cartesia TTS in LiveKit Inference.

## Overview

LiveKit Inference offers voice models powered by Cartesia. Pricing information is available on the [pricing page](https://livekit.io/pricing/inference#tts).

| Model ID | Languages |
| -------- | --------- |
| `cartesia/sonic-3` | `en`, `de`, `es`, `fr`, `ja`, `pt`, `zh`, `hi`, `ko`, `it`, `nl`, `pl`, `ru`, `sv`, `tr`, `tl`, `bg`, `ro`, `ar`, `cs`, `el`, `fi`, `hr`, `ms`, `sk`, `da`, `ta`, `uk`, `hu`, `no`, `vi`, `bn`, `th`, `he`, `ka`, `id`, `te`, `gu`, `kn`, `ml`, `mr`, `pa` |
| `cartesia/sonic-2` | `en`, `fr`, `de`, `es`, `pt`, `zh`, `ja`, `hi`, `it`, `ko`, `nl`, `pl`, `ru`, `sv`, `tr` |
| `cartesia/sonic-turbo` | `en`, `fr`, `de`, `es`, `pt`, `zh`, `ja`, `hi`, `it`, `ko`, `nl`, `pl`, `ru`, `sv`, `tr` |
| `cartesia/sonic` | `en`, `fr`, `de`, `es`, `pt`, `zh`, `ja`, `hi`, `it`, `ko`, `nl`, `pl`, `ru`, `sv`, `tr` |

## Usage

To use Cartesia, pass a descriptor with the model and voice to the `tts` argument in your `AgentSession`:

**Python**:

```python
from livekit.agents import AgentSession

session = AgentSession(
    tts="cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc",
    # ... llm, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

session = new AgentSession({
    tts="cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc",
    // ... tts, stt, vad, turn_detection, etc.
});

```

### Parameters

To customize additional parameters, use the `TTS` class from the `inference` module:

**Python**:

```python
from livekit.agents import AgentSession, inference

session = AgentSession(
    tts=inference.TTS(
        model="cartesia/sonic-3", 
        voice="9626c31c-bec5-4cca-baa8-f8ba9e84c8bc", 
        language="en",
        extra_kwargs={
            "speed": 1.5,
            "volume": 1.2,
            "emotion": "excited"
        }
    ),
    # ... tts, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

session = new AgentSession({
    tts: new inference.TTS({ 
        model: "cartesia/sonic-3", 
        voice: "9626c31c-bec5-4cca-baa8-f8ba9e84c8bc", 
        language: "en",
        modelOptions: {
            speed: 1.5,
            volume: 1.2,
            emotion: "excited"
        }
    }),
    // ... tts, stt, vad, turn_detection, etc.
});

```

- **`model`** _(string)_: The model ID from the [models list](#models).

- **`voice`** _(string)_: See [voices](#voices) for guidance on selecting a voice.

- **`language`** _(string)_ (optional): Language code for the input text. If not set, the model default applies.

- **`extra_kwargs`** _(dict)_ (optional): Additional parameters to pass to the Cartesia TTS API, including `emotion`, `speed`, and `volume`. See the provider's [documentation](#additional-resources) for more information.

In Node.js this parameter is called `modelOptions`.

## Voices

LiveKit Inference supports all of the default "Cartesia Voices" available in the Cartesia API. You can explore the available voices in the [Cartesia voice library](https://play.cartesia.ai/voices) (free account required), and use the voice by copying its ID into your LiveKit agent session.

> â„¹ï¸ **Custom voices unavailable**
> 
> Custom Cartesia voices, including voice cloning, are not yet supported in LiveKit Inference. To use custom voices, create your own Cartesia account and use the [Cartesia plugin](https://docs.livekit.io/agents/models/tts/plugins/cartesia.md) for LiveKit Agents instead.

The following is a small sample of the Cartesia voices available in LiveKit Inference.

| Provider | Name | Description | Language | ID |
| -------- | ---- | ----------- | -------- | -------- |
| Cartesia | Blake | Energetic American adult male | `en-US` | `cartesia/sonic-3:a167e0f3-df7e-4d52-a9c3-f949145efdab` |
| Cartesia | Daniela | Calm and trusting Mexican female | `es-MX` | `cartesia/sonic-3:5c5ad5e7-1020-476b-8b91-fdcbe9cc313c` |
| Cartesia | Jacqueline | Confident, young American adult female | `en-US` | `cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc` |
| Cartesia | Robyn | Neutral, mature Australian female | `en-AU` | `cartesia/sonic-3:f31cc6a7-c1e8-4764-980c-60a361443dd1` |

## Additional resources

The following links provide more information about Cartesia in LiveKit Inference.

- **[Cartesia Plugin](https://docs.livekit.io/agents/models/tts/plugins/cartesia.md)**: Plugin to use your own Cartesia account instead of LiveKit Inference.

- **[Cartesia docs](https://cartesia.ai/docs)**: Cartesia's official API documentation.

---

This document was rendered at 2025-11-19T10:47:30.737Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/inference/cartesia.md](https://docs.livekit.io/agents/models/tts/inference/cartesia.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º LiveKit Inference â€º ElevenLabs

---

# ElevenLabs TTS

> Reference for ElevenLabs TTS in LiveKit Inference.

## Overview

LiveKit Inference offers voice models powered by ElevenLabs. Pricing information is available on the [pricing page](https://livekit.io/pricing/inference#tts).

| Model ID | Languages |
| -------- | --------- |
| `elevenlabs/eleven_flash_v2` | `en` |
| `elevenlabs/eleven_flash_v2_5` | `en`, `ja`, `zh`, `de`, `hi`, `fr`, `ko`, `pt`, `it`, `es`, `id`, `nl`, `tr`, `fil`, `pl`, `sv`, `bg`, `ro`, `ar`, `cs`, `el`, `fi`, `hr`, `ms`, `sk`, `da`, `ta`, `uk`, `ru`, `hu`, `no`, `vi` |
| `elevenlabs/eleven_turbo_v2` | `en` |
| `elevenlabs/eleven_turbo_v2_5` | `en`, `ja`, `zh`, `de`, `hi`, `fr`, `ko`, `pt`, `it`, `es`, `id`, `nl`, `tr`, `fil`, `pl`, `sv`, `bg`, `ro`, `ar`, `cs`, `el`, `fi`, `hr`, `ms`, `sk`, `da`, `ta`, `uk`, `ru`, `hu`, `no`, `vi` |
| `elevenlabs/eleven_multilingual_v2` | `en`, `ja`, `zh`, `de`, `hi`, `fr`, `ko`, `pt`, `it`, `es`, `id`, `nl`, `tr`, `fil`, `pl`, `sv`, `bg`, `ro`, `ar`, `cs`, `el`, `fi`, `hr`, `ms`, `sk`, `da`, `ta`, `uk`, `ru` |

## Usage

To use ElevenLabs, pass a descriptor with the model and voice to the `tts` argument in your `AgentSession`:

**Python**:

```python
from livekit.agents import AgentSession

session = AgentSession(
    tts="elevenlabs/eleven_turbo_v2_5:Xb7hH8MSUJpSbSDYk0k2",
    # ... llm, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

session = new AgentSession({
    tts: "elevenlabs/eleven_turbo_v2_5:Xb7hH8MSUJpSbSDYk0k2",
    // ... tts, stt, vad, turn_detection, etc.
});

```

### Parameters

To customize additional parameters, use the `TTS` class from the `inference` module:

**Python**:

```python
from livekit.agents import AgentSession, inference

session = AgentSession(
    tts=inference.TTS(
        model="elevenlabs/eleven_turbo_v2_5", 
        voice="Xb7hH8MSUJpSbSDYk0k2", 
        language="en"
    ),
    # ... tts, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

session = new AgentSession({
    tts: new inference.TTS({ 
        model: "elevenlabs/eleven_turbo_v2_5", 
        voice: "Xb7hH8MSUJpSbSDYk0k2", 
        language: "en" 
    }),
    // ... tts, stt, vad, turn_detection, etc.
});

```

- **`model`** _(string)_: The model ID from the [models list](#models).

- **`voice`** _(string)_: See [voices](#voices) for guidance on selecting a voice.

- **`language`** _(string)_ (optional): Language code for the input text. If not set, the model default applies.

- **`extra_kwargs`** _(dict)_ (optional): Additional parameters to pass to the ElevenLabs TTS API, including `inactivity_timeout`, and `apply_text_normalization`. See the provider's [documentation](#additional-resources) for more information.

In Node.js this parameter is called `modelOptions`.

## Voices

LiveKit Inference supports all of the default voices available in the ElevenLabs API. You can explore the available voices in the [ElevenLabs voice library](https://elevenlabs.io/app/default-voices) (free account required), and use the voice by copying its ID into your LiveKit agent session.

> â„¹ï¸ **Custom & community voices unavailable**
> 
> Custom and community ElevenLabs voices, including voice cloning, are not yet supported in LiveKit Inference. To use these voices, create your own ElevenLabs account and use the [ElevenLabs plugin](https://docs.livekit.io/agents/models/tts/plugins/elevenlabs.md) for LiveKit Agents instead.

The following is a small sample of the ElevenLabs voices available in LiveKit Inference.

| Provider | Name | Description | Language | ID |
| -------- | ---- | ----------- | -------- | -------- |
| ElevenLabs | Alice | Clear and engaging, friendly British woman | `en-GB` | `elevenlabs/eleven_turbo_v2_5:Xb7hH8MSUJpSbSDYk0k2` |
| ElevenLabs | Chris | Natural and real American male | `en-US` | `elevenlabs/eleven_turbo_v2_5:iP95p4xoKVk53GoZ742B` |
| ElevenLabs | Eric | A smooth tenor Mexican male | `es-MX` | `elevenlabs/eleven_turbo_v2_5:cjVigY5qzO86Huf0OWal` |
| ElevenLabs | Jessica | Young and popular, playful American female | `en-US` | `elevenlabs/eleven_turbo_v2_5:cgSgspJ2msm6clMCkdW9` |

## Additional resources

The following links provide more information about ElevenLabs in LiveKit Inference.

- **[ElevenLabs Plugin](https://docs.livekit.io/agents/models/tts/plugins/elevenlabs.md)**: Plugin to use your own ElevenLabs account instead of LiveKit Inference.

- **[ElevenLabs docs](https://elevenlabs.io/docs)**: ElevenLabs's official API documentation.

---

This document was rendered at 2025-11-19T10:47:38.803Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/inference/elevenlabs.md](https://docs.livekit.io/agents/models/tts/inference/elevenlabs.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º LiveKit Inference â€º Inworld

---

# Inworld TTS

> Reference for Inworld TTS in LiveKit Inference.

## Overview

LiveKit Inference offers voice models powered by Inworld. Pricing information is available on the [pricing page](https://livekit.io/pricing/inference#tts).

| Model ID | Languages |
| -------- | --------- |
| `inworld/inworld-tts-1-max` | `en`, `es`, `fr`, `ko`, `nl`, `zh`, `de`, `it`, `ja`, `pl`, `pt`, `ru` |
| `inworld/inworld-tts-1` | `en`, `es`, `fr`, `ko`, `nl`, `zh`, `de`, `it`, `ja`, `pl`, `pt`, `ru` |

## Usage

To use Inworld, pass a descriptor with the model and voice to the `tts` argument in your `AgentSession`:

**Python**:

```python
from livekit.agents import AgentSession

session = AgentSession(
    tts="inworld/inworld-tts-1-max:Ashley",
    # ... llm, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

session = new AgentSession({
    tts: "inworld/inworld-tts-1-max:Ashley",
    // ... llm, stt, vad, turn_detection, etc.
});

```

### Parameters

To customize additional parameters, use the `TTS` class from the `inference` module:

**Python**:

```python
from livekit.agents import AgentSession, inference

session = AgentSession(
    tts=inference.TTS(
        model="inworld/inworld-tts-1-max", 
        voice="Ashley", 
        language="en"
    ),
    # ... llm, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

session = new AgentSession({
    tts: new inference.TTS({ 
        model: "inworld/inworld-tts-1-max", 
        voice: "Ashley", 
        language: "en" 
    }),
    // ... llm, stt, vad, turn_detection, etc.
});

```

- **`model`** _(string)_: The model ID from the [models list](#models).

- **`voice`** _(string)_: See [voices](#voices) for guidance on selecting a voice.

- **`language`** _(string)_ (optional): Language code for the input text. If not set, the model default applies.

- **`extra_kwargs`** _(dict)_ (optional): Additional parameters to pass to the Inworld TTS API. See the provider's [documentation](#additional-resources) for more information.

In Node.js this parameter is called `modelOptions`.

## Voices

LiveKit Inference supports all of the default voices available in the Inworld API. You can explore the available voices in the [Inworld TTS Playground](https://docs.inworld.ai/docs/tts/tts-playground) (free account required), and use the voice by copying its name into your LiveKit agent session.

> â„¹ï¸ **Cloned voices unavailable**
> 
> Cloned voices are not yet supported in LiveKit Inference. To use these voices, create your own Inworld account and use the [Inworld plugin](https://docs.livekit.io/agents/models/tts/plugins/inworld.md) for LiveKit Agents instead.

The following is a small sample of the Inworld voices available in LiveKit Inference.

| Provider | Name | Description | Language | ID |
| -------- | ---- | ----------- | -------- | -------- |
| Inworld | Ashley | Warm, natural American female | `en-US` | `inworld/inworld-tts-1:Ashley` |
| Inworld | Diego | Soothing, gentle Mexican male | `es-MX` | `inworld/inworld-tts-1:Diego ` |
| Inworld | Edward | Fast-talking, emphatic American male | `en-US` | `inworld/inworld-tts-1:Edward` |
| Inworld | Olivia | Upbeat, friendly British female | `en-GB` | `inworld/inworld-tts-1:Olivia` |

## Additional resources

The following links provide more information about Inworld in LiveKit Inference.

- **[Inworld Plugin](https://docs.livekit.io/agents/models/tts/plugins/inworld.md)**: Plugin to use your own Inworld account instead of LiveKit Inference.

- **[Inworld TTS docs](https://docs.inworld.ai/docs/tts/tts)**: Inworld's official API documentation.

---

This document was rendered at 2025-11-19T10:47:44.500Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/inference/inworld.md](https://docs.livekit.io/agents/models/tts/inference/inworld.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º LiveKit Inference â€º Rime

---

# Rime TTS

> Reference for Rime TTS in LiveKit Inference.

## Overview

LiveKit Inference offers voice models powered by Rime. Pricing information is available on the [pricing page](https://livekit.io/pricing/inference#tts).

| Model ID | Languages |
| -------- | --------- |
| `rime/arcana` | `en`, `es`, `fr`, `de` |
| `rime/mistv2` | `en`, `es`, `fr`, `de` |
| `rime/mist` | `en`, `es`, `fr`, `de` |

## Usage

The simplest way to use Rime TTS is to pass it to the `tts` argument in your `AgentSession`, including the model and voice to use:

**Python**:

```python
from livekit.agents import AgentSession

session = AgentSession(
    tts="rime/arcana:astra",
    # ... llm, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

session = new AgentSession({
    tts: "rime/arcana:astra",
    // ... tts, stt, vad, turn_detection, etc.
});

```

### Parameters

To customize additional parameters, use the `TTS` class from the `inference` module:

**Python**:

```python
from livekit.agents import AgentSession, inference

session = AgentSession(
    tts=inference.TTS(
        model="rime/arcana", 
        voice="astra", 
        language="en"
    ),
    # ... tts, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

session = new AgentSession({
    tts: new inference.TTS({ 
        model: "rime/arcana", 
        voice: "astra", 
        language: "en" 
    }),
    // ... tts, stt, vad, turn_detection, etc.
});

```

- **`model`** _(string)_: The model ID from the [models list](#models).

- **`voice`** _(string)_: See [voices](#voices) for guidance on selecting a voice.

- **`language`** _(string)_ (optional): Two-letter language code for the input text. Note that the Rime API uses three-letter abbreviations (e.g. `eng` for English), but LiveKit Inference uses two-letter codes instead for consistency with other providers.

- **`extra_kwargs`** _(dict)_ (optional): Additional parameters to pass to the Rime TTS API. See the provider's [documentation](#additional-resources) for more information.

In Node.js this parameter is called `modelOptions`.

## Voices

LiveKit Inference supports all of the voices available in the Rime API. You can view the default voices and explore the wider set in the API in the [Rime voices documentation](https://docs.rime.ai/api-reference/voices), and use the voice by copying its name into your LiveKit agent session.

The following is a small sample of the Rime voices available in LiveKit Inference.

| Provider | Name | Description | Language | ID |
| -------- | ---- | ----------- | -------- | -------- |
| Rime | Astra | Chipper, upbeat American female | `en-US` | `rime/arcana:astra` |
| Rime | Celeste | Chill Gen-Z American female | `en-US` | `rime/arcana:celeste` |
| Rime | Luna | Chill but excitable American female | `en-US` | `rime/arcana:luna` |
| Rime | Ursa | Young, emo American male | `en-US` | `rime/arcana:ursa` |

## Additional resources

The following links provide more information about Rime in LiveKit Inference.

- **[Rime Plugin](https://docs.livekit.io/agents/models/tts/plugins/rime.md)**: Plugin to use your own Rime account instead of LiveKit Inference.

- **[Rime TTS docs](https://docs.rime.ai/)**: Rime's official API documentation.

---

This document was rendered at 2025-11-19T10:47:49.507Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/inference/rime.md](https://docs.livekit.io/agents/models/tts/inference/rime.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Partner spotlight â€º AWS â€º Amazon Polly TTS Plugin

---

# Amazon Polly TTS plugin guide

> How to use the Amazon Polly TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Amazon Polly](https://aws.amazon.com/polly/) as a TTS provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[aws]~=1.2"

```

### Authentication

The Amazon Polly plugin requires an [AWS API key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html).

Set the following environment variables in your `.env` file:

```shell
AWS_ACCESS_KEY_ID=<aws-access-key-id>
AWS_SECRET_ACCESS_KEY=<aws-secret-access-key>
AWS_DEFAULT_REGION=<aws-deployment-region>

```

### Usage

Use an Amazon Polly TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import aws

session = AgentSession(
   tts=aws.TTS(
      voice="Ruth",
      speech_engine="generative",
      language="en-US",
   ),
   # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/aws/tts.html.md) for a complete list of all available parameters.

- **`voice`** _(TTSModels)_ (optional) - Default: `Ruth`: Voice to use for the synthesis. For a full list, see [Available voices](https://docs.aws.amazon.com/polly/latest/dg/available-voices.html).

- **`language`** _(TTSLanguages | str)_ (optional): Language code for the Synthesize Speech request. This is only necessary if using a bilingual voice, such as Aditi, which can be used for either Indian English (en-IN) or Hindi (hi-IN). To learn more, see [Languages in Amazon Polly](https://docs.aws.amazon.com/polly/latest/dg/supported-languages.html).

- **`speech_engine`** _(TTSSpeechEngine)_ (optional) - Default: `generative`: The voice engine to use for the synthesis. Valid values are `standard`, `neural`, `long-form`, and `generative`. To learn more, see [Amazon Polly voice engines](https://docs.aws.amazon.com/polly/latest/dg/voice-engines-polly.html).

- **`text_type`** _(TTSTextType)_ (optional) - Default: `text`: The type of text to synthesize. Valid values are `text` or `ssml`. Use `ssml` to synthesize speech from SSML-enchanced text. To learn more, see [Controlling speech and pronunciation](#pronunciation).

## Controlling speech and pronunciation

Amazon Polly supports Speech Synthesis Markup Language (SSML) for customizing generated speech. To learn more, see [Generating speech from SSML documents](https://docs.aws.amazon.com/polly/latest/dg/ssml.html) and [Supported SSML tags](https://docs.aws.amazon.com/polly/latest/dg/supportedtags.html).

## Additional resources

The following resources provide more information about using Amazon Polly with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-aws/)**: The `livekit-plugins-aws` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/aws/index.html.md#livekit.plugins.aws.TTS)**: Reference for the Amazon Polly TTS plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-aws)**: View the source or contribute to the LiveKit Amazon Polly TTS plugin.

- **[AWS docs](https://docs.aws.amazon.com/polly/latest/dg/what-is.html)**: Amazon Polly's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Amazon Polly.

---

This document was rendered at 2025-11-19T10:48:02.696Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/aws.md](https://docs.livekit.io/agents/models/tts/plugins/aws.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Partner spotlight â€º Azure â€º Azure AI Speech TTS Plugin

---

# Azure Speech TTS plugin guide

> How to use the Azure Speech TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Azure Speech](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/overview) as a TTS provider for your voice agents.

## Quick reference

This section provides a brief overview of the Azure Speech TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[azure]~=1.2"

```

### Authentication

The Azure Speech plugin requires an [Azure Speech key](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-to-text?tabs=macos,terminal&pivots=programming-language-python#prerequisites).

Set the following environment variables in your `.env` file:

```shell
AZURE_SPEECH_KEY=<azure-speech-key>
AZURE_SPEECH_REGION=<azure-speech-region>
AZURE_SPEECH_HOST=<azure-speech-host>

```

### Usage

Use an Azure Speech TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import azure

session = AgentSession(
    tts=azure.TTS(
        speech_key="<speech_service_key>",
        speech_region="<speech_service_region>",
    ),
    # ... llm, stt, etc.
)

```

> â„¹ï¸ **Note**
> 
> To create an instance of `azure.TTS`, one of the following options must be met:
> 
> - `speech_host` must be set, _or_
> - `speech_key` _and_ `speech_region` must both be set, _or_
> - `speech_auth_token` _and_ `speech_region` must both be set.

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/azure/index.html.md#livekit.plugins.azure.TTS) for a complete list of all available parameters.

- **`voice`** _(string)_ (optional): Voice for text-to-speech. To learn more, see [Select synthesis language and voice](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-speech-synthesis#select-synthesis-language-and-voice).

- **`language`** _(string)_ (optional): Language of the input text. To learn more, see [Select synthesis language and voice](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-speech-synthesis#select-synthesis-language-and-voice).

- **`prosody`** _(ProsodyConfig)_ (optional): Specify changes to pitch, rate, and volume for the speech output. To learn more, see [Adjust prosody](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/speech-synthesis-markup-voice#adjust-prosody).

- **`speech_key`** _(string)_ (optional) - Environment: `AZURE_SPEECH_KEY`: Azure Speech speech-to-text key. To learn more, see [Azure Speech prerequisites](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-to-text#prerequisites).

- **`speech_region`** _(string)_ (optional) - Environment: `AZURE_SPEECH_REGION`: Azure Speech speech-to-text region. To learn more, see [Azure Speech prerequisites](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-to-text#prerequisites).

- **`speech_host`** _(string)_ (optional) - Environment: `AZURE_SPEECH_HOST`: Azure Speech endpoint.

- **`speech_auth_token`** _(string)_ (optional): Azure Speech authentication token.

## Controlling speech and pronunciation

Azure Speech TTS supports Speech Synthesis Markup Language (SSML) for customizing generated speech. To learn more, see [SSML overview](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/speech-synthesis-markup).

## Additional resources

The following resources provide more information about using Azure Speech with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-azure/)**: The `livekit-plugins-azure` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/azure/index.html.md#livekit.plugins.azure.TTS)**: Reference for the Azure Speech TTS plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-azure)**: View the source or contribute to the LiveKit Azure Speech TTS plugin.

- **[Azure Speech docs](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/overview)**: Azure Speech's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Azure Speech.

- **[Azure ecosystem guide](https://docs.livekit.io/agents/integrations/azure.md)**: Overview of the entire Azure AI and LiveKit Agents integration.

---

This document was rendered at 2025-11-19T10:48:08.233Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/azure.md](https://docs.livekit.io/agents/models/tts/plugins/azure.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Partner spotlight â€º Azure â€º Azure OpenAI TTS Plugin

---

# Azure OpenAI TTS plugin guide

> How to use the Azure OpenAI TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service/) as a TTS provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Support for Azure OpenAI TTS is available in the `openai` plugin.

Install the plugin from PyPI:

```shell
uv add "livekit-agents[openai]~=1.2"

```

### Authentication

The Azure OpenAI TTS requires [authentication](https://learn.microsoft.com/en-us/azure/api-management/api-management-authenticate-authorize-azure-openai) using an API key or a managed identity.

Set the following environment variables in your `.env` file:

```shell
AZURE_OPENAI_API_KEY=<azure-openai-api-key>
AZURE_OPENAI_AD_TOKEN=<azure-openai-ad-token>
AZURE_OPENAI_ENDPOINT=<azure-openai-endpoint>

```

### Usage

Use Azure OpenAI TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import openai

session = AgentSession(
   tts=openai.TTS.with_azure(
      model="gpt-4o-mini-tts",
      voice="coral",
   )
   # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. For a complete reference of all available parameters, see the [plugin reference](https://docs.livekit.io/reference/python/livekit/plugins/openai/index.html.md#livekit.plugins.openai.TTS.create_azure_client).

- **`model`** _(string)_ (optional) - Default: `gpt-4o-mini-tts`: ID of the model to use for TTS. To learn more, see [Text to speech models](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-audio#text-to-speech-models-preview).

- **`voice`** _(string)_ (optional) - Default: `ash`: OpenAI text-to-speech voice. To learn more, see the list of supported voices for `voice` in the [Azure documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference-preview#createspeechrequest).

- **`instructions`** _(string)_ (optional) - Default: ``: Instructions to control tone, style, and other characteristics of the speech.

- **`azure_endpoint`** _(string)_ (optional) - Environment: `AZURE_OPENAI_ENDPOINT`: Azure OpenAI endpoint in the following format: `https://{your-resource-name}.openai.azure.com`.

- **`azure_deployment`** _(string)_ (optional): Name of your model deployment.

- **`api_version`** _(string)_ (optional) - Environment: `OPENAI_API_VERSION`: OpenAI REST API version used for the request.

- **`api_key`** _(string)_ (optional) - Environment: `AZURE_OPENAI_API_KEY`: Azure OpenAI API key.

- **`azure_ad_token`** _(string)_ (optional) - Environment: `AZURE_OPENAI_AD_TOKEN`: Azure Active Directory token.

- **`organization`** _(string)_ (optional) - Environment: `OPENAI_ORG_ID`: OpenAI organization ID.

- **`project`** _(string)_ (optional) - Environment: `OPENAI_PROJECT_ID`: OpenAI project ID.

## Additional resources

The following resources provide more information about using Azure OpenAI with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-openai/)**: The `livekit-plugins-openai` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/openai/index.html.md#livekit.plugins.openai.TTS.with_azure)**: Reference for the Azure OpenAI TTS plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-openai)**: View the source or contribute to the LiveKit Azure OpenAI plugin.

- **[Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/)**: Azure OpenAI documentation.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Azure OpenAI.

- **[Azure ecosystem guide](https://docs.livekit.io/agents/integrations/azure.md)**: Overview of the entire Azure AI and LiveKit Agents integration.

---

This document was rendered at 2025-11-19T10:48:13.198Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/azure-openai.md](https://docs.livekit.io/agents/models/tts/plugins/azure-openai.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º Baseten

---

# Baseten TTS plugin guide

> How to use the Baseten TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Baseten](https://www.baseten.co/) as a TTS provider for your voice agents.

## Quick reference

This section provides a quick reference for the Baseten TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[baseten]~=1.2"

```

### Authentication

The Baseten plugin requires a [Baseten API key](https://app.baseten.co/settings/api-keys).

Set the following in your `.env` file:

```shell
BASETEN_API_KEY=<your-baseten-api-key>

```

### Model deployment

You must deploy a TTS model such as [Orpheus](https://www.baseten.co/library/orpheus-tts/) to Baseten to use it with LiveKit Agents. Your deployment includes a private model endpoint URL to provide to the LiveKit Agents integration.

### Usage

Use Baseten TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import baseten

session = AgentSession(
   tts=baseten.TTS(
      model_endpoint="<your-model-endpoint>",
      voice="tara",
   )
   # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/baseten/index.html.md#livekit.plugins.baseten.TTS) for a complete list of all available parameters.

- **`model_endpoint`** _(string)_ (optional) - Environment: `BASETEN_MODEL_ENDPOINT`: The endpoint URL for your deployed model. You can find this in your Baseten dashboard.

- **`voice`** _(string)_ (optional) - Default: `tara`: The voice to use for speech synthesis.

- **`language`** _(string)_ (optional) - Default: `en`: Language of output audio in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) format.

- **`temperature`** _(float)_ (optional) - Default: `0.6`: Controls the randomness of the generated speech. Higher values make the output more random.

## Additional resources

The following resources provide more information about using Baseten with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-baseten/)**: The `livekit-plugins-baseten` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/baseten/index.html.md#livekit.plugins.baseten.TTS)**: Reference for the Baseten TTS plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-baseten)**: View the source or contribute to the LiveKit Baseten TTS plugin.

- **[Baseten docs](https://docs.baseten.co/)**: Baseten's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Baseten.

- **[Baseten STT](https://docs.livekit.io/agents/models/stt/plugins/baseten.md)**: Guide to the Baseten STT plugin with LiveKit Agents.

---

This document was rendered at 2025-11-19T10:48:18.144Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/baseten.md](https://docs.livekit.io/agents/models/tts/plugins/baseten.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º Cartesia

---

# Cartesia TTS plugin guide

> How to use the Cartesia TTS plugin for LiveKit Agents.

Available in:
- [x] Node.js
- [x] Python

## Overview

This plugin allows you to use [Cartesia](https://www.cartesia.ai/) as a TTS provider for your voice agents.

> ðŸ’¡ **LiveKit Inference**
> 
> Cartesia TTS is also available in LiveKit Inference, with billing and integration handled automatically. See [the docs](https://docs.livekit.io/agents/models/tts/inference/cartesia.md) for more information.

## Quick reference

This section includes a brief overview of the Cartesia TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

**Python**:

```shell
uv add "livekit-agents[cartesia]~=1.2"

```

---

**Node.js**:

```shell
pnpm add @livekit/agents-plugin-cartesia@1.x

```

### Authentication

The Cartesia plugin requires a [Cartesia API key](https://play.cartesia.ai/keys).

Set `CARTESIA_API_KEY` in your `.env` file.

### Usage

Use Cartesia TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

**Python**:

```python
from livekit.plugins import cartesia

session = AgentSession(
   tts=cartesia.TTS(
      model="sonic-3",
      voice="f786b574-daa5-4673-aa0c-cbe3e8534c02",
   )
   # ... llm, stt, etc.
)

```

---

**Node.js**:

```typescript
import * as cartesia from '@livekit/agents-plugin-cartesia';

const session = new voice.AgentSession({
    tts: cartesia.TTS(
        model: "sonic-3",
        voice: "f786b574-daa5-4673-aa0c-cbe3e8534c02",
    ),
    // ... llm, stt, etc.
});

```

### Parameters

This section describes some of the available parameters. See the plugin reference links in the [Additional resources](#additional-resources) section for a complete list of all available parameters.

- **`model`** _(string)_ (optional) - Default: `sonic-3`: ID of the model to use for generation. See [supported models](https://docs.cartesia.ai/build-with-cartesia/models/tts).

- **`voice`** _(string | list[float])_ (optional) - Default: `794f9389-aac1-45b6-b726-9d9369183238`: ID of the voice to use for generation, or an embedding array. See [official documentation](https://docs.cartesia.ai/api-reference/tts/tts#send.Generation%20Request.voice).

- **`language`** _(string)_ (optional) - Default: `en`: Language of input text in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) format. For a list of languages support by model, see [supported models](https://docs.cartesia.ai/build-with-cartesia/models/tts).

- **`emotion`** _(string)_ (optional): See [Emotion Controls](https://docs.cartesia.ai/build-with-cartesia/sonic-3/volume-speed-emotion#emotion-controls-beta) for Sonic 3 for supported values.

- **`speed`** _(float)_ (optional) - Default: `1`: Speed of the speech, where 1.0 is the default speed. See [Speed and Volume Controls](https://docs.cartesia.ai/build-with-cartesia/sonic-3/volume-speed-emotion#speed-and-volume-controls) for Sonic 3 for more information.

- **`volume`** _(float)_ (optional) - Default: `1`: Volume of the speech, where 1.0 is the default volume. See [Speed and Volume Controls](https://docs.cartesia.ai/build-with-cartesia/sonic-3/volume-speed-emotion#speed-and-volume-controls) for Sonic 3 for more information.

## Customizing pronunciation

Cartesia TTS allows you to customize pronunciation using Speech Synthesis Markup Language (SSML). To learn more, see [Specify Custom Pronunciations](https://docs.cartesia.ai/build-with-cartesia/sonic-3/custom-pronunciations).

## Transcription timing

Cartesia TTS supports aligned transcription forwarding, which improves transcription synchronization in your frontend. Set `use_tts_aligned_transcript=True` in your `AgentSession` configuration to enable this feature. To learn more, see [the docs](https://docs.livekit.io/agents/build/text.md#tts-aligned-transcriptions).

## Additional resources

The following resources provide more information about using Cartesia with LiveKit Agents.

- **[Cartesia docs](https://docs.cartesia.ai/build-with-cartesia/models/tts)**: Cartesia TTS docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Cartesia TTS.

- **[Cartesia STT](https://docs.livekit.io/agents/models/stt/plugins/cartesia.md)**: Guide to the Cartesia STT plugin with LiveKit Agents.

---

This document was rendered at 2025-11-19T10:48:23.912Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/cartesia.md](https://docs.livekit.io/agents/models/tts/plugins/cartesia.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º Deepgram

---

# Deepgram TTS plugin guide

> How to use the Deepgram TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Deepgram](https://deepgram.com/) as a TTS provider in your voice agents.

## Quick reference

This section provides a quick reference for the Deepgram TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[deepgram]~=1.2"

```

### Authentication

The Deepgram plugin requires a [Deepgram API key](https://console.deepgram.com/).

Set `DEEPGRAM_API_KEY` in your `.env` file.

### Usage

Use Deepgram TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import deepgram

session = AgentSession(
   tts=deepgram.TTS(
      model="aura-asteria-en",
   )
   # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/deepgram/index.html.md#livekit.plugins.deepgram.TTS) for a complete list of all available parameters.

- **`model`** _(string)_ (optional) - Default: `aura-asteria-en`: ID of the model to use for generation. To learn more, see [supported models](https://developers.deepgram.com/docs/tts-models).

## Prompting

Deepgram supports filler words and natural pauses through prompting. To learn more, see [Text to Speech Prompting](https://developers.deepgram.com/docs/text-to-speech-prompting).

## Additional resources

The following resources provide more information about using Deepgram with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-deepgram/)**: The `livekit-plugins-deepgram` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/deepgram/index.html.md#livekit.plugins.deepgram.TTS)**: Reference for the Deepgram TTS plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-deepgram)**: View the source or contribute to the LiveKit Deepgram TTS plugin.

- **[Deepgram docs](https://developers.deepgram.com/docs)**: Deepgram's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Deepgram.

- **[Deepgram STT](https://docs.livekit.io/agents/models/stt/plugins/deepgram.md)**: Guide to the Deepgram STT plugin with LiveKit Agents.

---

This document was rendered at 2025-11-19T10:48:29.784Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/deepgram.md](https://docs.livekit.io/agents/models/tts/plugins/deepgram.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º ElevenLabs

---

# ElevenLabs TTS plugin guide

> How to use the ElevenLabs TTS plugin for LiveKit Agents.

Available in:
- [x] Node.js
- [x] Python

## Overview

This plugin allows you to use [ElevenLabs](https://elevenlabs.io/) as a TTS provider for your voice agents.

> ðŸ’¡ **LiveKit Inference**
> 
> ElevenLabs TTS is also available in LiveKit Inference, with billing and integration handled automatically. See [the docs](https://docs.livekit.io/agents/models/tts/inference/elevenlabs.md) for more information.

## Quick reference

This section provides a quick reference for the ElevenLabs TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

**Python**:

```shell
uv add "livekit-agents[elevenlabs]~=1.2"

```

---

**Node.js**:

```shell
pnpm add @livekit/agents-plugin-elevenlabs@1.x

```

### Authentication

The ElevenLabs plugin requires an [ElevenLabs API key](https://elevenlabs.io/app/settings/api-keys).

Set `ELEVEN_API_KEY` in your `.env` file.

### Usage

Use ElevenLabs TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

**Python**:

```python
from livekit.plugins import elevenlabs

session = AgentSession(
   tts=elevenlabs.TTS(
      voice_id="ODq5zmih8GrVes37Dizd",
      model="eleven_multilingual_v2"
   )
   # ... llm, stt, etc.
)

```

---

**Node.js**:

```typescript
import * as elevenlabs from '@livekit/agents-plugin-elevenlabs';

const session = new voice.AgentSession({
    tts: new elevenlabs.TTS(
      voice: { id: "ODq5zmih8GrVes37Dizd" },
      model: "eleven_multilingual_v2"
    ),
    // ... llm, stt, etc.
});

```

### Parameters

This section describes some of the parameters you can set when you create an ElevenLabs TTS. See the plugin reference links in the [Additional resources](#additional-resources) section for a complete list of all available parameters.

- **`model`** _(string)_ (optional) - Default: `eleven_flash_v2_5`: ID of the model to use for generation. To learn more, see the [ElevenLabs documentation](https://elevenlabs.io/docs/api-reference/text-to-speech/convert#/docs/api-reference/text-to-speech/convert#request.body.model_id).

- **`voice_id`** _(string)_ (optional) - Default: `EXAVITQu4vr4xnSDxMaL`: ID of the voice to use for generation. To learn more, see the [ElevenLabs documentation](https://elevenlabs.io/docs/api-reference/text-to-speech/convert).

- **`voice_settings`** _(VoiceSettings)_ (optional): Voice configuration. To learn more, see the [ElevenLabs documentation](https://elevenlabs.io/docs/api-reference/text-to-speech/convert#request.body.voice_settings).

- - **`stability`** _(float)_ (optional):
- - **`similarity_boost`** _(float)_ (optional):
- - **`style`** _(float)_ (optional):
- - **`use_speaker_boost`** _(bool)_ (optional):
- - **`speed`** _(float)_ (optional):

- **`language`** _(string)_ (optional) - Default: `en`: Language of output audio in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) format. To learn more, see the [ElevenLabs documentation](https://elevenlabs.io/docs/api-reference/text-to-speech/convert#request.body.language_code).

- **`streaming_latency`** _(int)_ (optional) - Default: `3`: Latency in seconds for streaming.

- **`enable_ssml_parsing`** _(bool)_ (optional) - Default: `false`: Enable Speech Synthesis Markup Language (SSML) parsing for input text. Set to `true` to [customize pronunciation](#customizing-pronunciation) using SSML.

- **`chunk_length_schedule`** _(list[int])_ (optional) - Default: `[80, 120, 200, 260]`: Schedule for chunk lengths. Valid values range from `50` to `500`.

## Customizing pronunciation

ElevenLabs supports custom pronunciation for specific words or phrases with SSML `phoneme` tags. This is useful to ensure correct pronunciation of certain words, even when missing from the voice's lexicon. To learn more, see [Pronunciation](https://elevenlabs.io/docs/best-practices/prompting#pronunciation).

## Transcription timing

ElevenLabs TTS supports aligned transcription forwarding, which improves transcription synchronization in your frontend. Set `use_tts_aligned_transcript=True` in your `AgentSession` configuration to enable this feature. To learn more, see [the docs](https://docs.livekit.io/agents/build/text.md#tts-aligned-transcriptions).

## Additional resources

The following resources provide more information about using ElevenLabs with LiveKit Agents.

- **[ElevenLabs docs](https://elevenlabs.io/docs)**: ElevenLabs TTS docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and ElevenLabs TTS.

---

This document was rendered at 2025-11-19T10:48:35.746Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/elevenlabs.md](https://docs.livekit.io/agents/models/tts/plugins/elevenlabs.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º Gemini

---

# Gemini TTS plugin guide

> How to use the Gemini TTS plugin for LiveKit Agents.

Available in (BETA):
- [x] Node.js
- [x] Python

## Overview

This plugin allows you to use [Gemini TTS](https://ai.google.dev/gemini-api/docs/speech-generation) as a TTS provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

**Python**:

```shell
uv add "livekit-agents[google]~=1.2"

```

---

**Node.js**:

```shell
pnpm add @livekit/agents-plugin-google@1.x

```

### Authentication

Credentials must be provided by one of the following methods:

- For Vertex AI, you must set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to the path of the service account key file. For more information about mounting files as secrets when deploying to LiveKit Cloud, see [File-mounted secrets](https://docs.livekit.io/agents/ops/deployment/secrets.md#file-mounted-secrets) .
- To use Gemini API: Set the `api_key` argument or the `GOOGLE_API_KEY` environment variable.

### Usage

Use a Gemini TTS in an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

**Python**:

```python
from livekit.plugins import google

session = AgentSession(
  tts = google.beta.GeminiTTS(
   model="gemini-2.5-flash-preview-tts",
   voice_name="Zephyr",
   instructions="Speak in a friendly and engaging tone.",
  ),
    # ... llm, stt, etc.
  )

```

---

**Node.js**:

```typescript
import * as google from '@livekit/agents-plugin-google';

const session = new voice.AgentSession({
    tts: new google.beta.TTS(
        model: "gemini-2.5-flash-preview-tts",
        voiceName: "Zephyr",
        instructions: "Speak in a friendly and engaging tone.",
    ),
    // ... llm, stt, etc.
});

```

### Parameters

This section describes some of the available parameters. See the plugin reference links in the [Additional resources](#additional-resources) section for a complete list of all available parameters.

- **`model`** _(string)_ (optional) - Default: `gemini-2.5-flash-preview-tts`: The model to use for speech generation. For a list of models, see [Supported models](https://ai.google.dev/gemini-api/docs/speech-generation#supported-models).

- **`voice_name`** _(string)_ (optional) - Default: `Kore`: Voice name. For supported voices, see [Voice options](https://ai.google.dev/gemini-api/docs/speech-generation#voices).

- **`voice_name`** _(string)_ (optional): Name of the voice to use for speech. For a full list of voices, see [Supported voices and languages](https://cloud.google.com/text-to-speech/docs/voices).

- **`instructions`** _(string)_ (optional): Prompt to control the style, tone, accent, and pace. To learn more, see [Controlling speech style with prompts](https://ai.google.dev/gemini-api/docs/speech-generation#controllable).

## Additional resources

The following resources provide more information about using Gemini TTS with LiveKit Agents.

- **[Gemini TTS docs](https://ai.google.dev/gemini-api/docs/speech-generation)**: Gemini Developer API docs for TTS.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Gemini TTS.

- **[Google ecosystem guide](https://docs.livekit.io/agents/integrations/google.md)**: Overview of the entire Google AI and LiveKit Agents integration.

---

This document was rendered at 2025-11-19T10:48:42.283Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/gemini.md](https://docs.livekit.io/agents/models/tts/plugins/gemini.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Partner spotlight â€º Google â€º Google Cloud TTS Plugin

---

# Google Cloud TTS integration guide

> How to use the Google Cloud TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Google Cloud TTS](https://cloud.google.com/text-to-speech) as a TTS provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[google]~=1.2"

```

### Authentication

Google Cloud credentials must be provided by one of the following methods:

- Passed in the `credentials_info` dictionary.
- Saved in the `credentials_file` JSON file (`GOOGLE_APPLICATION_CREDENTIALS` environment variable).
- Application Default Credentials. To learn more, see [How Application Default Credentials works](https://cloud.google.com/docs/authentication/application-default-credentials)

### Usage

Use a Google Cloud TTS in an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import google

session = AgentSession(
  tts = google.TTS(
    gender="female",
    voice_name="en-US-Standard-H",
  ),
  # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/google/index.html.md#livekit.plugins.google.TTS) for a complete list of all available parameters.

- **`language`** _(SpeechLanguages | string)_ (optional) - Default: `en-US`: Specify output language. For a full list of languages, see [Supported voices and languages](https://cloud.google.com/text-to-speech/docs/voices).

- **`gender`** _(Gender | string)_ (optional) - Default: `neutral`: Voice gender. Valid values are `male`, `female`, and `neutral`.

- **`voice_name`** _(string)_ (optional): Name of the voice to use for speech. For a full list of voices, see [Supported voices and languages](https://cloud.google.com/text-to-speech/docs/voices).

- **`voice_cloning_key`** _(string)_ (optional): Voice cloning key to use a custom, cloned voice. This is a text string that represents your voice data. To learn more, see [Chirp 3: Instant custom voice](https://cloud.google.com/text-to-speech/docs/chirp3-instant-custom-voice).

- **`credentials_info`** _(array)_ (optional): Key-value pairs of authentication credential information.

- **`credentials_file`** _(string)_ (optional): Name of the JSON file that contains authentication credentials for Google Cloud.

## Customizing speech

Google Cloud TTS supports Speech Synthesis Markup Language (SSML) to customize pronunciation and speech. To learn more, see the [SSML reference](https://cloud.google.com/text-to-speech/docs/ssml).

## Additional resources

The following resources provide more information about using Google Cloud with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-google/)**: The `livekit-plugins-google` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/google/index.html.md#livekit.plugins.google.TTS)**: Reference for the Google Cloud TTS plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-google)**: View the source or contribute to the LiveKit Google Cloud TTS plugin.

- **[Google Cloud docs](https://cloud.google.com/text-to-speech/docs)**: Google Cloud TTS docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Google Cloud TTS.

- **[Google ecosystem guide](https://docs.livekit.io/agents/integrations/google.md)**: Overview of the entire Google AI and LiveKit Agents integration.

---

This document was rendered at 2025-11-19T10:48:46.893Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/google.md](https://docs.livekit.io/agents/models/tts/plugins/google.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Partner spotlight â€º Groq â€º Groq TTS Plugin

---

# Groq TTS plugin guide

> How to use the Groq TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Groq](https://groq.com/) as a TTS provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[groq]~=1.2"

```

### Authentication

The Groq plugin requires a [Groq API key](https://console.groq.com/keys).

Set `GROQ_API_KEY` in your `.env` file.

### Usage

Use Groq TTS in your `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import groq
   
session = AgentSession(
   tts=groq.TTS(
      model="playai-tts",
      voice="Arista-PlayAI",
   ),
   # ... stt, llm, vad, turn_detection, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/groq/index.html.md#livekit.plugins.groq.TTS) for a complete list of all available parameters.

- **`model`** _(TTSModel | string)_ (optional) - Default: `playai-tts`: Name of the TTS model. For a full list, see [Models](https://console.groq.com/docs/models).

- **`voice`** _(string)_ (optional) - Default: `Arista-PlayAI`: Name of the voice. For a full list, see [English](https://console.groq.com/docs/text-to-speech#available-english-voices) and [Arabic](https://console.groq.com/docs/text-to-speech#available-arabic-voices) voices.

## Additional resources

The following resources provide more information about using Groq with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-groq/)**: The `livekit-plugins-groq` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/groq/index.html.md#livekit.plugins.groq.TTS)**: Reference for the Groq TTS plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-groq)**: View the source or contribute to the LiveKit Groq TTS plugin.

- **[Groq docs](https://console.groq.com/docs/text-to-speech)**: Groq TTS docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Groq TTS.

- **[Groq ecosystem guide](https://docs.livekit.io/agents/integrations/groq.md)**: Overview of the entire Groq and LiveKit Agents integration.

---

This document was rendered at 2025-11-19T10:48:52.706Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/groq.md](https://docs.livekit.io/agents/models/tts/plugins/groq.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º Hume

---

# Hume TTS plugin guide

> How to use the Hume TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Hume](https://hume.ai/) as a TTS provider for your voice agents.

## Quick reference

This section includes a brief overview of the Hume TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[hume]~=1.2"

```

## Authentication

The Hume plugin requires a [Hume API key](https://platform.hume.ai/settings/keys).

Set `HUME_API_KEY` in your `.env` file.

### Usage

Use Hume TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import hume

session = AgentSession(
   tts=hume.TTS(
      voice=hume.VoiceByName(name="Colton Rivers", provider=hume.VoiceProvider.hume),
      description="The voice exudes calm, serene, and peaceful qualities, like a gentle stream flowing through a quiet forest.",
   )
   # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/hume/index.html.md#livekit.plugins.hume.TTS) for a complete list of all available parameters.

- **`voice`** _(VoiceByName | VoiceById)_ (optional): The voice, specified by name or id, to be used. When no voice is specified, a novel voice will be [generated based on the text and optionally provided description](https://dev.hume.ai/docs/text-to-speech-tts/voices#specify-a-voice-or-dont).

- **`description`** _(string)_ (optional): Natural language instructions describing how the synthesized speech should sound, including but not limited to tone, intonation, pacing, and accent. If a Voice is specified in the request, this description serves as [acting](https://dev.hume.ai/docs/text-to-speech-tts/acting-instructions) instructions. If no Voice is specified, a new voice is generated [based on this description](https://dev.hume.ai/docs/text-to-speech-tts/prompting).

- **`speed`** _(float)_ (optional) - Default: `1.0`: Adjusts the relative speaking rate on a non-linear scale from 0.25 (much slower) to 3.0 (much faster), where 1.0 represents normal speaking pace.

- **`instant_mode`** _(bool)_ (optional) - Default: `true`: Enables ultra-low latency streaming, reducing time to first chunk. Recommended for real-time applications. Only for streaming endpoints. With this enabled, requests incur 10% higher cost.

Instant mode is automatically enabled when a voice is specified in the request.

## Updating utterance options

To change the values during the session, use the `update_options` method. It accepts the same parameters as the TTS constructor. The new values take effect on the next utterance:

```python
session.tts.update_options(
   voice=hume.VoiceByName(name="Colton Rivers", provider=hume.VoiceProvider.hume),
   description="The voice exudes calm, serene, and peaceful qualities, like a gentle stream flowing through a quiet forest.",
   speed=2,
)

```

## Additional resources

The following resources provide more information about using Hume with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-hume/)**: The `livekit-plugins-hume` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/hume/index.html.md#livekit.plugins.hume.TTS)**: Reference for the Hume TTS plugin.

- **[Hume docs](https://dev.hume.ai/docs/text-to-speech-tts)**: Hume docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Hume TTS.

---

This document was rendered at 2025-11-19T10:48:58.700Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/hume.md](https://docs.livekit.io/agents/models/tts/plugins/hume.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º Inworld

---

# Inworld TTS plugin guide

> How to use the Inworld TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Inworld](https://inworld.ai/) as a TTS provider for your voice agents.

> ðŸ’¡ **LiveKit Inference**
> 
> Inworld TTS is also available in LiveKit Inference, with billing and integration handled automatically. See [the docs](https://docs.livekit.io/agents/models/tts/inference/inworld.md) for more information.

## Quick reference

This section includes a brief overview of the Inworld TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[inworld]~=1.2"

```

### Authentication

The Inworld plugin requires Base64 [Inworld API key](https://platform.inworld.ai/login).

Set `INWORLD_API_KEY` in your `.env` file.

### Usage

Use Inworld TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import inworld

session = AgentSession(
   tts=inworld.TTS(model="inworld-tts-1-max", voice="Ashley")
   # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/inworld/index.html.md#livekit.plugins.inworld.TTS) for a complete list of all available parameters.

- **`model`** _(string)_ (optional) - Default: `"inworld-tts-1"`: ID of the model to use for generation. See [supported models](https://docs.inworld.ai/docs/models#tts).

- **`voice`** _(string)_ (optional) - Default: `"Ashley"`: ID of the voice to use for generation. Use the [List voices API endpoint](https://docs.inworld.ai/api-reference/ttsAPI/texttospeech/list-voices) for possible values.

- **`temperature`** _(float)_ (optional) - Default: `0.8`: Controls randomness in the output. Recommended to set between 0.6 and 1.0. See [docs](https://docs.inworld.ai/docs/tts/tts#additional-configurations).

- **`speaking_rate`** _(float)_ (optional) - Default: `1.0`: Controls how fast the voice speaks. 1.0 is the normal native speed, while 0.5 is half the normal speed and 1.5 is 1.5x faster than the normal speed. See [docs](https://docs.inworld.ai/docs/tts/tts#additional-configurations).

- **`pitch`** _(float)_ (optional) - Default: `0.0`: Adjusts how high or low the voice sounds. Negative values make the voice deeper/lower, while positive values make it higher/squeakier. See [docs](https://docs.inworld.ai/docs/tts/tts#additional-configurations).

## Additional resources

The following resources provide more information about using Inworld with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-inworld/)**: The `livekit-plugins-inworld` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/inworld/index.html.md#livekit.plugins.inworld.TTS)**: Reference for the Inworld TTS plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-inworld)**: View the source or contribute to the LiveKit Inworld TTS plugin.

- **[Inworld docs](https://docs.inworld.ai/docs/introduction)**: Inworld TTS docs.

---

This document was rendered at 2025-11-19T10:49:03.909Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/inworld.md](https://docs.livekit.io/agents/models/tts/plugins/inworld.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º LMNT

---

# LMNT TTS plugin guide

> How to use the LMNT TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [LMNT](https://lmnt.com/) as a TTS provider for your voice agents.

## Quick reference

This section provides a quick reference for the LMNT TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[lmnt]~=1.2"

```

### Authentication

The LMNT plugin requires an [LMNT API key](https://app.lmnt.com/account).

Set `LMNT_API_KEY` in your `.env` file.

### Usage

Use LMNT TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import lmnt

session = AgentSession(
   tts=lmnt.TTS(
      voice="leah",
   )
   # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the parameters you can set when you create an LMNT TTS. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/lmnt/index.html.md#livekit.plugins.lmnt.TTS) for a complete list of all available parameters.

- **`model`** _(string)_ (optional) - Default: `blizzard`: The model to use for synthesis. Refer to the [LMNT models guide](https://docs.lmnt.com/guides/models) for possible values.

- **`voice`** _(string)_ (optional) - Default: `leah`: The voice ID to use. Find or create new voices in the [LMNT voice library](https://app.lmnt.com/voice-library).

- **`language`** _(string)_ (optional): Two-letter ISO 639-1 language code. See the [LMNT API documentation](https://docs.lmnt.com/api-reference/speech/synthesize-speech-bytes#body-language) for supported languages.

- **`temperature`** _(float)_ (optional): Influences how expressive and emotionally varied the speech becomes. Lower values (like 0.3) create more neutral, consistent speaking styles. Higher values (like 1.0) allow for more dynamic emotional range and speaking styles.

- **`top_p`** _(float)_ (optional): Controls the stability of the generated speech. A lower value (like 0.3) produces more consistent, reliable speech. A higher value (like 0.9) gives more flexibility in how words are spoken, but might occasionally produce unusual intonations or speech patterns.

## Additional resources

The following resources provide more information about using LMNT with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-lmnt/)**: The `livekit-plugins-lmnt` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/lmnt/index.html.md#livekit.plugins.lmnt.TTS)**: Reference for the LMNT TTS plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-lmnt)**: View the source or contribute to the LiveKit LMNT TTS plugin.

- **[LMNT docs](https://docs.lmnt.com/)**: LMNT API documentation.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and LMNT TTS.

---

This document was rendered at 2025-11-19T10:49:09.398Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/lmnt.md](https://docs.livekit.io/agents/models/tts/plugins/lmnt.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º MiniMax

---

# MiniMax TTS plugin guide

> How to use the MiniMax TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [MiniMax](https://www.minimax.io/) as a TTS provider for your voice agents.

## Quick reference

This section includes a brief overview of the MiniMax TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```bash
pip install "livekit-agents[minimax]~=1.2"

```

### Authentication

The MiniMax plugin requires a [MiniMax API key](https://platform.minimax.io/user-center/basic-information/interface-key).

Set `MINIMAX_API_KEY` in your `.env` file.

### Usage

Use MiniMax TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import minimax

session = AgentSession(
    tts=minimax.TTS(
    ),
    # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/minimax.md) for a complete list of all available parameters.

- **`model`** _(TTSModel | string)_ (optional) - Default: `DEFAULT_MODEL`: MiniMax TTS model to use. To learn more, see [TTS model options](https://platform.minimax.io/docs/guides/models-intro#audio).

- **`voice`** _(TTSVoice | string)_ (optional) - Default: `DEFAULT_VOICE_ID`: MiniMax TTS voice to use.

- **`emotion`** _(TTSEmotion | string)_ (optional) - Default: `None`: Control emotional tone for speech. Valid values are `happy`, `sad`, `angry`, `fearful`, `disgusted`, `surprised`, `neutral`.

- **`speed`** _(float)_ (optional): Speech speed where higher values speak faster. Range is `0.5` to `2.0`.

- **`pronunciation_dict`** _(dict[str, list[str]])_ (optional): Defines the pronunciation rules for specific characters or symbols. Read more in the [MiniMax API documentation](https://platform.minimax.io/docs/api-reference/speech-t2a-http#body-pronunciation-dict).

- **`english_normalization`** _(bool)_ (optional): Set to `true` to enable text normalization. This feature improves the model's pronunciation of numbers and dates, with a minor tradeoff in latency. Read more in the [MiniMax API FAQ](https://platform.minimax.io/docs/faq/about-apis#q%3A-the-function-of-the-english-normalization-parameter).

## Additional resources

The following resources provide more information about using MiniMax with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-minimax-ai/)**: The `livekit-plugins-minimax-ai` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/minimax.md)**: Reference for the MiniMax TTS plugin.

- **[MiniMax docs](https://platform.minimax.io/docs/guides/quickstart)**: MiniMax Open Platform documentation.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and MiniMax TTS.

---

This document was rendered at 2025-11-19T10:49:14.433Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/minimax.md](https://docs.livekit.io/agents/models/tts/plugins/minimax.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º MiniMax

---

# MiniMax TTS plugin guide

> How to use the MiniMax TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [MiniMax](https://www.minimax.io/) as a TTS provider for your voice agents.

## Quick reference

This section includes a brief overview of the MiniMax TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```bash
pip install "livekit-agents[minimax]~=1.2"

```

### Authentication

The MiniMax plugin requires a [MiniMax API key](https://platform.minimax.io/user-center/basic-information/interface-key).

Set `MINIMAX_API_KEY` in your `.env` file.

### Usage

Use MiniMax TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import minimax

session = AgentSession(
    tts=minimax.TTS(
    ),
    # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/minimax.md) for a complete list of all available parameters.

- **`model`** _(TTSModel | string)_ (optional) - Default: `DEFAULT_MODEL`: MiniMax TTS model to use. To learn more, see [TTS model options](https://platform.minimax.io/docs/guides/models-intro#audio).

- **`voice`** _(TTSVoice | string)_ (optional) - Default: `DEFAULT_VOICE_ID`: MiniMax TTS voice to use.

- **`emotion`** _(TTSEmotion | string)_ (optional) - Default: `None`: Control emotional tone for speech. Valid values are `happy`, `sad`, `angry`, `fearful`, `disgusted`, `surprised`, `neutral`.

- **`speed`** _(float)_ (optional): Speech speed where higher values speak faster. Range is `0.5` to `2.0`.

- **`pronunciation_dict`** _(dict[str, list[str]])_ (optional): Defines the pronunciation rules for specific characters or symbols. Read more in the [MiniMax API documentation](https://platform.minimax.io/docs/api-reference/speech-t2a-http#body-pronunciation-dict).

- **`english_normalization`** _(bool)_ (optional): Set to `true` to enable text normalization. This feature improves the model's pronunciation of numbers and dates, with a minor tradeoff in latency. Read more in the [MiniMax API FAQ](https://platform.minimax.io/docs/faq/about-apis#q%3A-the-function-of-the-english-normalization-parameter).

## Additional resources

The following resources provide more information about using MiniMax with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-minimax-ai/)**: The `livekit-plugins-minimax-ai` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/minimax.md)**: Reference for the MiniMax TTS plugin.

- **[MiniMax docs](https://platform.minimax.io/docs/guides/quickstart)**: MiniMax Open Platform documentation.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and MiniMax TTS.

---

This document was rendered at 2025-11-19T10:49:14.433Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/minimax.md](https://docs.livekit.io/agents/models/tts/plugins/minimax.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Partner spotlight â€º OpenAI â€º OpenAI TTS Plugin

---

# OpenAI TTS plugin guide

> How to use the OpenAI TTS plugin for LiveKit Agents.

Available in:
- [x] Node.js
- [x] Python

## Overview

This plugin allows you to use [OpenAI TTS](https://platform.openai.com/docs/guides/text-to-speech) as a TTS provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

**Python**:

```bash
pip install "livekit-agents[openai]~=1.2"

```

---

**Node.js**:

```bash
pnpm add @livekit/agents-plugin-openai@1.x

```

### Authentication

The OpenAI plugin requires an [OpenAI API key](https://platform.openai.com/api-keys).

Set `OPENAI_API_KEY` in your `.env` file.

### Usage

Use OpenAI TTS in an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

**Python**:

```python
from livekit.plugins import openai

session = AgentSession(
  tts = openai.TTS(
    model="gpt-4o-mini-tts",
    voice="ash",
    instructions="Speak in a friendly and conversational tone.",
  ),
  # ... llm, stt, etc.
)

```

---

**Node.js**:

```typescript
import * as openai from '@livekit/agents-plugin-openai';

const session = new voice.AgentSession({
    tts: new openai.TTS(
        model: "gpt-4o-mini-tts",
        voice: "ash",
        instructions: "Speak in a friendly and conversational tone.",
    ),
    // ... llm, stt, etc.
});

```

### Parameters

This section describes some of the available parameters. See the plugin reference links in the [Additional resources](#additional-resources) section for a complete list of all available parameters.

- **`model`** _(TTSModels | string)_ (optional) - Default: `gpt-4o-mini-tts`: ID of the model to use for speech generation. To learn more, see [TTS models](https://platform.openai.com/docs/models#tts).

- **`voice`** _(TTSVoice | string)_ (optional) - Default: `ash`: ID of the voice used for speech generation. To learn more, see [TTS voice options](https://platform.openai.com/docs/guides/text-to-speech#voice-options).

- **`instructions`** _(string)_ (optional) - Default: ``: Instructions to control tone, style, and other characteristics of the speech. Does not work with `tts-1` or `tts-1-hd` models.

## Additional resources

The following resources provide more information about using OpenAI with LiveKit Agents.

- **[OpenAI docs](https://platform.openai.com/docs/guides/text-to-speech)**: OpenAI TTS docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and OpenAI TTS.

- **[OpenAI ecosystem guide](https://docs.livekit.io/agents/integrations/openai.md)**: Overview of the entire OpenAI and LiveKit Agents integration.

---

This document was rendered at 2025-11-19T10:49:25.261Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/openai.md](https://docs.livekit.io/agents/models/tts/plugins/openai.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º Resemble AI

---

# Resemble AI TTS plugin guide

> How to use the Resemble AI TTS plugin for LiveKit Agents.

Available in:
- [x] Node.js
- [x] Python

## Overview

This plugin allows you to use [Resemble AI](https://resemble.ai/) as a TTS provider for your voice agents.

## Quick reference

This section includes a brief overview of the Resemble AI TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[resemble]~=1.2"

```

## Authentication

The Resemble AI plugin requires a [Resemble AI API key](https://app.resemble.ai/account/api).

Set `RESEMBLE_API_KEY` in your `.env` file.

### Usage

Use Resemble AI TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import resemble

session = AgentSession(
   tts=resemble.TTS(
      voice_uuid="55592656",
   )
   # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/resemble/index.html.md#livekit.plugins.resemble.TTS) for a complete list of all available parameters.

- **`voice_uuid`** _(string)_ - Default: `55592656`: ID of the voice to use.

## Customizing pronunciation

Resemble AI supports custom pronunciation with Speech Synthesis Markup Language (SSML), an XML-based markup language that gives you granular control over speech output. With SSML, you can leverage XML tags to craft audio content that delivers a more natural and engaging listening experience. To learn more, see [SSML](https://docs.app.resemble.ai/docs/getting_started/ssml).

## Additional resources

The following resources provide more information about using Resemble AI with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-resemble/)**: The `livekit-plugins-resemble` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/resemble/index.html.md#livekit.plugins.resemble.TTS)**: Reference for the Resemble AI TTS plugin.

- **[Resemble AI docs](https://docs.app.resemble.ai)**: Resemble AI docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Resemble AI TTS.

---

This document was rendered at 2025-11-19T10:49:30.515Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/resemble.md](https://docs.livekit.io/agents/models/tts/plugins/resemble.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º Rime

---

# Rime TTS plugin guide

> How to use the Rime TTS plugin for LiveKit Agents.

Available in:
- [x] Node.js
- [x] Python

## Overview

This plugin allows you to use [Rime](https://rime.ai/) as a TTS provider for your voice agents.

> ðŸ’¡ **LiveKit Inference**
> 
> Rime TTS is also available in LiveKit Inference, with billing and integration handled automatically. See [the docs](https://docs.livekit.io/agents/models/tts/inference/rime.md) for more information.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin:

**Python**:

```shell
uv add "livekit-agents[rime]~=1.2"

```

---

**Node.js**:

```shell
pnpm add @livekit/agents-plugin-rime@1.x

```

### Authentication

The Rime plugin requires a [Rime API key](https://rime.ai/).

Set `RIME_API_KEY` in your `.env` file.

### Usage

Use Rime TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

**Python**:

```python
from livekit.plugins import rime

session = AgentSession(
   tts=rime.TTS(
      model="mistv2",
      speaker="rainforest",
      speed_alpha=0.9,
      reduce_latency=True,
   ),
   # ... llm, stt, etc.
)

```

---

**Node.js**:

```typescript
import * as rime from '@livekit/agents-plugin-rime';

const session = new voice.AgentSession({
    tts: new rime.TTS({
      modelId: "mistv2",
      speaker: "rainforest",
      speedAlpha: 0.9,
      reduceLatency: true,
    }),
    // ... llm, tts, etc.
});

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/rime/index.html.md#livekit.plugins.rime.TTS) for a complete list of all available parameters.

- **`model`** _(string)_ (optional) - Default: `mist`: ID of the model to use. To learn more, see [Models](https://docs.rime.ai/api-reference/models).

- **`speaker`** _(string)_ (optional) - Default: `lagoon`: ID of the voice to use for speech generation. To learn more, see [Voices](https://docs.rime.ai/api-reference/voices).

- **`audio_format`** _(TTSEncoding)_ (optional) - Default: `pcm`: Audio format to use. Valid values are: `pcm` and `mp3`.

- **`sample_rate`** _(integer)_ (optional) - Default: `16000`: Sample rate of the generated audio. Set this rate to best match your application needs. To learn more, see [Recommendations for reducing response time](https://docs.rime.ai/api-reference/latency#recommendations-for-reducing-response-time).

- **`speed_alpha`** _(float)_ (optional) - Default: `1.0`: Adjusts the speed of speech. Lower than `1.0` results in faster speech; higher than `1.0` results in slower speech.

- **`reduce_latency`** _(boolean)_ (optional) - Default: `false`: When set to `true`, turns off text normalization to reduce the amount of time spent preparing input text for TTS inference. This might result in the mispronunciation of digits and abbreviations. To learn more, see [Recommendations for reducing response time](https://docs.rime.ai/api-reference/latency#recommendations-for-reducing-response-time).

- **`phonemize_between_brackets`** _(boolean)_ (optional) - Default: `false`: When set to `true`, allows the use of custom pronunciation strings in text. To learn more, see [Custom pronunciation](https://docs.rime.ai/api-reference/custom-pronunciation).

- **`api_key`** _(string)_ (optional) - Environment: `RIME_API_KEY`: Rime API Key. Required if the environment variable isn't set.

## Customizing pronunciation

Rime TTS supports customizing pronunciation. To learn more, see [Custom Pronunciation guide](https://docs.rime.ai/api-reference/custom-pronunciation).

## Additional resources

The following resources provide more information about using Rime with LiveKit Agents.

- **[Rime docs](https://docs.rime.ai)**: Rime TTS docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Rime TTS.

---

This document was rendered at 2025-11-19T10:49:35.945Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/rime.md](https://docs.livekit.io/agents/models/tts/plugins/rime.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º Sarvam

---

# Sarvam TTS plugin guide

> How to use the Sarvam TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Sarvam](https://sarvam.ai/) as a TTS provider for your voice agents.

## Quick reference

This section provides a quick reference for the Sarvam TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[sarvam]~=1.2"

```

### Authentication

The Sarvam plugin requires a [Sarvam API key](https://dashboard.sarvam.ai/key-management).

Set `SARVAM_API_KEY` in your `.env` file.

### Usage

Use Sarvam TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import sarvam

session = AgentSession(
   tts=sarvam.TTS(
      target_language_code="hi-IN",
      speaker="anushka",
   )
   # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/sarvam/index.html.md#livekit.plugins.sarvam.TTS) for a complete list of all available parameters.

- **`target_language_code`** _(string)_: BCP-47 language code for supported Indian languages. For example: `hi-IN` for Hindi, `en-IN` for Indian English. See [documentation](https://docs.sarvam.ai/api-reference-docs/text-to-speech/convert#request.body.target_language_code) for a complete list of supported languages.

- **`model`** _(string)_ (optional) - Default: `bulbul:v2`: The Sarvam TTS model to use. Currently only `bulbul:v2` is supported. See [documentation](https://docs.sarvam.ai/api-reference-docs/text-to-speech/convert#request.body.model) for a complete list of supported models.

- **`speaker`** _(string)_ (optional) - Default: `anushka`: Voice to use for synthesis. See [documentation](https://docs.sarvam.ai/api-reference-docs/text-to-speech/convert#request.body.speaker) for a complete list of supported voices.

- **`pitch`** _(float)_ (optional) - Default: `0.0`: Voice pitch adjustment. Valid range: -20.0 to 20.0.

- **`pace`** _(float)_ (optional) - Default: `1.0`: Speech rate multiplier. Valid range: 0.5 to 2.0.

- **`loudness`** _(float)_ (optional) - Default: `1.0`: Volume multiplier. Valid range: 0.5 to 2.0.

## Additional resources

The following resources provide more information about using Sarvam with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-sarvam/)**: The `livekit-plugins-sarvam` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/sarvam/index.html.md#livekit.plugins.sarvam.TTS)**: Reference for the Sarvam TTS plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-sarvam)**: View the source or contribute to the LiveKit Sarvam TTS plugin.

- **[Sarvam docs](https://docs.sarvam.ai/)**: Sarvam's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Sarvam.

- **[Sarvam STT](https://docs.livekit.io/agents/models/stt/plugins/sarvam.md)**: Guide to the Sarvam STT plugin with LiveKit Agents.

---

This document was rendered at 2025-11-19T10:49:42.097Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/sarvam.md](https://docs.livekit.io/agents/models/tts/plugins/sarvam.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º Smallest AI

---

# Smallest AI TTS plugin guide

> How to use the Smallest AI Waves TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use the [Smallest AI](https://smallest.ai/text-to-speech) Waves platform as a TTS provider for your voice agents.

## Quick reference

This section includes a brief overview of the Smallest AI TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[smallestai]~=1.2"

```

### Authentication

The Smallest AI plugin requires an [API key](https://console.smallest.ai/apikeys).

Set `SMALLEST_API_KEY` in your `.env` file.

### Usage

Use Smallest AI TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import smallestai

session = AgentSession(
    tts=smallestai.TTS(
        voice_id="irisha",
        sample_rate=24000,
        output_format="pcm",
    ),
    # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/smallestai/index.html.md) for a complete list of all available parameters.

- **`model`** _(TTSModels | string)_ (optional) - Default: `lightning`: Model to use.

- **`voice_id`** _(string)_ (optional): The voice ID for synthesis. Must be a valid Smallest AI voice identifier.

- **`sample_rate`** _(number)_ (optional) - Default: `24000`: Target audio sample rate in Hz. Match the rate to the rest of your audio pipeline to avoid resampling artifacts.

- **`output_format`** _(TTSOutputFormat | string)_ (optional) - Default: `pcm`: Encoding format for synthesized audio. Select a format based on if you want raw audio for streaming/processing or compressed for storage/playback.

## Additional resources

The following resources provide more information about using Smallest AI with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-smallestai/)**: The `livekit-plugins-smallestai` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/smallestai/index.html.md)**: Reference for the Smallest AI TTS plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-smallestai)**: View the source or contribute to the LiveKit Smallest AI TTS plugin.

- **[Smallest AI docs](https://waves-docs.smallest.ai/v3.0.1/content/introduction/introduction)**: Smallest AI's Waves TTS docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Smallest AI TTS.

---

This document was rendered at 2025-11-19T10:49:46.840Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/smallestai.md](https://docs.livekit.io/agents/models/tts/plugins/smallestai.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º Speechify

---

# Speechify TTS plugin guide

> How to use the Speechify TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Speechify](https://speechify.com) as a TTS provider for your voice agents.

## Quick reference

This section includes a brief overview of the Speechify TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[speechify]~=1.2"

```

## Authentication

The Speechify plugin requires a [Speechify API key](https://console.sws.speechify.com).

Set `SPEECHIFY_API_KEY` in your .env file.

### Usage

Use Speechify TTS within an AgentSession or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import speechify

session = AgentSession(
   tts=speechify.TTS(
      model="simba-english",
      voice_id="jack",
   )
   # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/speechify/index.html.md#livekit.plugins.speechify.TTS) for a complete list of all available parameters.

- **`voice_id`** _(string)_ - Default: `jack`: ID of the voice to be used for synthesizing speech. Refer to `list_voices()` method in the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/speechify/index.html.md#livekit.plugins.speechify.TTS.list_voices).

- **`model`** _(string)_ (optional): ID of the model to use for generation. Use `simba-english` or `simba-multilingual` To learn more, see: [supported models](https://docs.sws.speechify.com/v1/docs/get-started/models).

- **`language`** _(string)_ (optional): Language of input text in ISO-639-1 format. See the [supported languages](https://docs.sws.speechify.com/v1/docs/features/language-support).

- **`encoding`** _(string)_ (optional) - Default: `wav_48000`: Audio encoding to use. Choose between `wav_48000`, `mp3_24000`, `ogg_24000` or `aac_24000`.

- **`loudness_normalization`** _(boolean)_ (optional): Determines whether to normalize the audio loudness to a standard level. When enabled, loudness normalization aligns the audio output to the following standards: Integrated loudness: -14 LUFS True peak: -2 dBTP Loudness range: 7 LU If disabled, the audio loudness will match the original loudness of the selected voice, which may vary significantly and be either too quiet or too loud. Enabling loudness normalization can increase latency due to additional processing required for audio level adjustments.

- **`text_normalization`** _(boolean)_ (optional): Determines whether to normalize the text. If enabled, it will transform numbers, dates, etc. into words. For example, "55" is normalized into "fifty five". This can increase latency due to additional processing required for text normalization.

## Customizing pronunciation

Speechify supports custom pronunciation with Speech Synthesis Markup Language (SSML), an XML-based markup language that gives you granular control over speech output. With SSML, you can leverage XML tags to craft audio content that delivers a more natural and engaging listening experience. To learn more, see [SSML](https://docs.sws.speechify.com/v1/docs/features/ssml).

## Additional resources

The following resources provide more information about using Speechify with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-speechify/)**: The `livekit-plugins-speechify` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/speechify/index.html.md#livekit.plugins.speechify.TTS)**: Reference for the Speechify TTS plugin.

- **[Speechify docs](https://docs.sws.speechify.com/v1/docs)**: Speechify docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Speechify TTS.

---

This document was rendered at 2025-11-19T10:49:52.633Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/speechify.md](https://docs.livekit.io/agents/models/tts/plugins/speechify.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Text-to-speech (TTS) â€º Plugins â€º Spitch

---

# Spitch TTS plugin guide

> How to use the Spitch TTS plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Spitch](https://spitch.app/) as a TTS provider for your voice agents.

## Quick reference

This section provides a quick reference for the Spitch TTS plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[spitch]~=1.2"

```

### Authentication

The Spitch plugin requires a [Spitch API key](https://studio.spitch.app/api/keys).

Set `SPITCH_API_KEY` in your `.env` file.

### Usage

Use Spitch TTS within an `AgentSession` or as a standalone speech generator. For example, you can use this TTS in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import spitch

session = AgentSession(
   tts=spitch.TTS(
      language="en",
      voice="lina",
   )
   # ... llm, stt, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/spitch/index.html.md#livekit.plugins.spitch.TTS) for a complete list of all available parameters.

- **`language`** _(string)_ (optional) - Default: `en`: Language short code for the generated speech. For supported values, see [Spitch languages](https://docs.spitch.app/concepts/languages).

- **`voice`** _(string)_ (optional) - Default: `lina`: Voice to use for synthesis. For supported values, see [Spitch voices](https://docs.spitch.app/concepts/voices).

## Additional resources

The following resources provide more information about using Spitch with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-spitch/)**: The `livekit-plugins-spitch` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/spitch/index.html.md#livekit.plugins.spitch.TTS)**: Reference for the Spitch TTS plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-spitch)**: View the source or contribute to the LiveKit Spitch TTS plugin.

- **[Spitch docs](https://docs.spitch.app/)**: Spitch's official documentation.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Spitch.

- **[Spitch STT](https://docs.livekit.io/agents/models/stt/plugins/spitch.md)**: Guide to the Spitch STT plugin with LiveKit Agents.

---

This document was rendered at 2025-11-19T10:49:57.819Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/tts/plugins/spitch.md](https://docs.livekit.io/agents/models/tts/plugins/spitch.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).