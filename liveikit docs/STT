LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Overview

---

# Speech-to-text (STT) models

> Models and plugins for realtime transcription in your voice agents.

## Overview

STT models, also known as Automated Speech Recognition (ASR) models, are used for realtime transcription or translation of spoken audio. In voice AI, they form the first of three models in the core pipeline: text is transcribed by an STT model, then processed by an [LLM](https://docs.livekit.io/agents/models/llm.md) model to generate a response which is turned backed to speech using a [TTS](https://docs.livekit.io/agents/models/tts.md) model.

You can choose a model served through LiveKit Inference, which is included in LiveKit Cloud, or you can use a plugin to connect directly to a wider range of model providers with your own account.

## LiveKit Inference

The following models are available in [LiveKit Inference](https://docs.livekit.io/agents/models.md#inference). Refer to the guide for each model for more details on additional configuration options.

| Provider | Model name |  | Languages |
| -------- | -------- | --------- |
| [AssemblyAI](https://docs.livekit.io/agents/models/stt/inference/assemblyai.md) | Universal-Streaming | English only |
| [Cartesia](https://docs.livekit.io/agents/models/stt/inference/cartesia.md) | Ink Whisper | 98 languages |
| [Deepgram](https://docs.livekit.io/agents/models/stt/inference/deepgram.md) | Nova-3 | Multilingual, 8 languages |
|   | Nova-3 Medical | English only |
|   | Nova-2 | Multilingual, 33 languages |
|   | Nova-2 Medical | English only |
|   | Nova-2 Conversational AI | English only |
|   | Nova-2 Phonecall | English only |

## Usage

To set up STT in an `AgentSession`, provide a descriptor with both the desired model and language. LiveKit Inference manages the connection to the model automatically. Consult the [models list](#inference) for available models and languages.

**Python**:

```python
from livekit.agents import AgentSession

session = AgentSession(
    # AssemblyAI STT in English
    stt="assemblyai/universal-streaming:en",
    # ... llm, tts, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession, inference } from '@livekit/agents';

const session = new AgentSession({
    // AssemblyAI STT in English
    stt: "assemblyai/universal-streaming:en",
    // ... llm, tts, etc.
})

```

### Multilingual transcription

If you don't know the language of the input audio, or expect multiple languages to be used simultaneously, use `deepgram/nova-3` with the language set to `multi`. This model supports multilingual transcription.

**Python**:

```python
from livekit.agents import AgentSession

session = AgentSession(
    stt="deepgram/nova-3:multi",
    # ... llm, tts, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

const session = new AgentSession({
    stt: "deepgram/nova-3:multi",
    // ... llm, tts, etc.
})

```

### Additional parameters

More configuration options, such as custom vocabulary, are available for each model. To set additional parameters, use the `STT` class from the `inference` module. Consult each model reference for examples and available parameters.

## Plugins

The LiveKit Agents framework also includes a variety of open source [plugins](https://docs.livekit.io/agents/models.md#plugins) for a wide range of STT providers. These plugins require authentication with the provider yourself, usually via an API key. You are responsible for setting up your own account and managing your own billing and credentials. The plugins are listed below, along with their availability for Python or Node.js.

| Provider | Python | Node.js |
| -------- | ------ | ------- |
| [Amazon Transcribe](https://docs.livekit.io/agents/models/stt/plugins/aws.md) | âœ“ | â€” |
| [AssemblyAI](https://docs.livekit.io/agents/models/stt/plugins/assemblyai.md) | âœ“ | â€” |
| [Azure AI Speech](https://docs.livekit.io/agents/models/stt/plugins/azure.md) | âœ“ | â€” |
| [Azure OpenAI](https://docs.livekit.io/agents/models/stt/plugins/azure-openai.md) | âœ“ | â€” |
| [Baseten](https://docs.livekit.io/agents/models/stt/plugins/baseten.md) | âœ“ | â€” |
| [Cartesia](https://docs.livekit.io/agents/models/stt/plugins/cartesia.md) | âœ“ | â€” |
| [Clova](https://docs.livekit.io/agents/models/stt/plugins/clova.md) | âœ“ | â€” |
| [Deepgram](https://docs.livekit.io/agents/models/stt/plugins/deepgram.md) | âœ“ | âœ“ |
| [fal](https://docs.livekit.io/agents/models/stt/plugins/fal.md) | âœ“ | â€” |
| [Gladia](https://docs.livekit.io/agents/models/stt/plugins/gladia.md) | âœ“ | â€” |
| [Google Cloud](https://docs.livekit.io/agents/models/stt/plugins/google.md) | âœ“ | â€” |
| [Groq](https://docs.livekit.io/agents/models/stt/plugins/groq.md) | âœ“ | â€” |
| [Mistral AI](https://docs.livekit.io/agents/models/stt/plugins/mistralai.md) | âœ“ | â€” |
| [OpenAI](https://docs.livekit.io/agents/models/stt/plugins/openai.md) | âœ“ | âœ“ |
| [Sarvam](https://docs.livekit.io/agents/models/stt/plugins/sarvam.md) | âœ“ | â€” |
| [Soniox](https://docs.livekit.io/agents/models/stt/plugins/soniox.md) | âœ“ | â€” |
| [Speechmatics](https://docs.livekit.io/agents/models/stt/plugins/speechmatics.md) | âœ“ | â€” |
| [Spitch](https://docs.livekit.io/agents/models/stt/plugins/spitch.md) | âœ“ | â€” |

Have another provider in mind? LiveKit is open source and welcomes [new plugin contributions](https://docs.livekit.io/agents/models.md#contribute).

## Advanced features

The following sections cover more advanced topics common to all STT providers. For more detailed reference on individual provider configuration, consult the model reference or plugin documentation for that provider.

### Automatic model selection

If you don't need to use any specific model features, and are only interested in the best model available for a given language, you can specify the language alone with the special model id `auto`. LiveKit Inference will choose the best model for the given language automatically.

**Python**:

```python
from livekit.agents import AgentSession

session = AgentSession(
    # Use the best available model for Spanish
    stt="auto:es",   
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

session = new AgentSession({
    // Use the best available model for Spanish
    stt: "auto:es",
})

```

LiveKit Inference supports the following languages:

- `en`: English
- `zh`: Chinese
- `de`: German
- `es`: Spanish
- `ru`: Russian
- `ko`: Korean
- `fr`: French
- `ja`: Japanese
- `pt`: Portuguese
- `tr`: Turkish
- `pl`: Polish
- `ca`: Catalan
- `nl`: Dutch
- `ar`: Arabic
- `sv`: Swedish
- `it`: Italian
- `id`: Indonesian
- `vi`: Vietnamese
- `he`: Hebrew
- `uk`: Ukrainian
- `el`: Greek
- `ms`: Malay
- `cs`: Czech
- `ro`: Romanian
- `da`: Danish
- `hu`: Hungarian
- `ta`: Tamil
- `no`: Norwegian
- `th`: Thai
- `ur`: Urdu
- `hr`: Croatian
- `bg`: Bulgarian
- `lt`: Lithuanian
- `la`: Latin
- `mi`: Maori
- `ml`: Malayalam
- `cy`: Welsh
- `sk`: Slovak
- `te`: Telugu
- `fa`: Farsi
- `lv`: Latvian
- `bn`: Bengali
- `sr`: Serbian
- `az`: Azerbaijani
- `sl`: Slovenian
- `kn`: Kannada
- `et`: Estonian
- `mk`: Macedonian
- `br`: Breton
- `eu`: Basque
- `is`: Icelandic
- `hy`: Armenian
- `ne`: Nepali
- `mn`: Mongolian
- `bs`: Bosnian
- `kk`: Kazakh
- `sq`: Albanian
- `sw`: Swahili
- `gl`: Galician
- `mr`: Marathi
- `pa`: Punjabi
- `si`: Sinhala
- `km`: Khmer
- `sn`: Shona
- `yo`: Yoruba
- `so`: Somali
- `af`: Afrikaans
- `oc`: Occitan
- `ka`: Georgian
- `be`: Belarusian
- `tg`: Tajik
- `sd`: Sindhi
- `gu`: Gujarati
- `am`: Amharic
- `yi`: Yiddish
- `lo`: Lao
- `uz`: Uzbek
- `fo`: Faroese
- `ht`: Haitian
- `ps`: Pashto
- `tk`: Turkmen
- `nn`: Norwegian Nynorsk
- `multi`: Multilingual (automatic)
- `mt`: Maltese
- `sa`: Sanskrit
- `lb`: Luxembourgish
- `my`: Myanmar
- `bo`: Tibetan
- `tl`: Tagalog
- `mg`: Malagasy
- `as`: Assamese
- `tt`: Tatar
- `haw`: Hawaiian
- `ln`: Lingala
- `ha`: Hausa
- `ba`: Bashkir
- `jw`: Javanese
- `su`: Sundanese
- `yue`: Cantonese
- `fi`: Finnish
- `hi`: Hindi
- `en-US`: English (United States)
- `en-AU`: English (Australia)
- `en-CA`: English (Canada)
- `en-GB`: English (United Kingdom)
- `en-IE`: English (Ireland)
- `en-IN`: English (India)
- `en-NZ`: English (New Zealand)
- `es-419`: Spanish (Latin America)
- `es-MX`: Spanish (Mexico)
- `de-CH`: German (Switzerland)
- `da-DK`: Danish (Denmark)
- `fr-CA`: French (Canada)
- `ko-KR`: Korean (South Korea)
- `nl-BE`: Dutch (Belgium)
- `pt-BR`: Portuguese (Brazil)
- `pt-PT`: Portuguese (Portugal)
- `sv-SE`: Swedish (Sweden)
- `zh-Hans`: Simplified Chinese
- `zh-Hant`: Traditional Chinese
- `zh-HK`: Traditional Chinese (Hong Kong)
- `th-TH`: Thai (Thailand)
- `zh-CN`: Simplified Chinese (China)
- `zh-TW`: Traditional Chinese (Taiwan)

### Custom STT

To create an entirely custom STT, implement the [STT node](https://docs.livekit.io/agents/build/nodes.md#stt_node) in your agent.

### Standalone usage

You can use an `STT` instance in a standalone fashion, without an `AgentSession`, using the streaming interface. Use `push_frame` to add [realtime audio frames](https://docs.livekit.io/home/client/tracks.md) to the stream, and then consume a stream of `SpeechEvent` events as output.

Here is an example of a standalone STT app:

** Filename: `agent.py`**

```python
import asyncio

from dotenv import load_dotenv

from livekit import agents, rtc
from livekit.agents import AgentServer
from livekit.agents.stt import SpeechEventType, SpeechEvent
from typing import AsyncIterable
from livekit.plugins import (
    deepgram,
)

load_dotenv()

server = AgentServer()

@server.rtc_session()
async def my_agent(ctx: agents.JobContext):
    @ctx.room.on("track_subscribed")
    def on_track_subscribed(track: rtc.RemoteTrack):
        print(f"Subscribed to track: {track.name}")

        asyncio.create_task(process_track(track))

    async def process_track(track: rtc.RemoteTrack):
        stt = deepgram.STT(model="nova-2")
        stt_stream = stt.stream()
        audio_stream = rtc.AudioStream(track)

        async with asyncio.TaskGroup() as tg:
            # Create task for processing STT stream
            stt_task = tg.create_task(process_stt_stream(stt_stream))

            # Process audio stream
            async for audio_event in audio_stream:
                stt_stream.push_frame(audio_event.frame)

            # Indicates the end of the audio stream
            stt_stream.end_input()

            # Wait for STT processing to complete
            await stt_task

    async def process_stt_stream(stream: AsyncIterable[SpeechEvent]):
        try:
            async for event in stream:
                if event.type == SpeechEventType.FINAL_TRANSCRIPT:
                    print(f"Final transcript: {event.alternatives[0].text}")
                elif event.type == SpeechEventType.INTERIM_TRANSCRIPT:
                    print(f"Interim transcript: {event.alternatives[0].text}")
                elif event.type == SpeechEventType.START_OF_SPEECH:
                    print("Start of speech")
                elif event.type == SpeechEventType.END_OF_SPEECH:
                    print("End of speech")
        finally:
            await stream.aclose()


if __name__ == "__main__":
    agents.cli.run_app(server)


```

### VAD and StreamAdapter

Some STT providers or models, such as [Whisper](https://github.com/openai/whisper) don't support streaming input. In these cases, your app must determine when a chunk of audio represents a complete segment of speech. You can do this using VAD together with the `StreamAdapter` class.

The following example modifies the previous example to use VAD and `StreamAdapter` to buffer user speech until VAD detects the end of speech:

```python
from livekit import agents, rtc
from livekit.plugins import openai, silero

async def process_track(ctx: agents.JobContext, track: rtc.Track):
  whisper_stt = openai.STT()
  vad = silero.VAD.load(
    min_speech_duration=0.1,
    min_silence_duration=0.5,
  )
  vad_stream = vad.stream()
  # StreamAdapter will buffer audio until VAD emits END_SPEAKING event
  stt = agents.stt.StreamAdapter(whisper_stt, vad_stream)
  stt_stream = stt.stream()
  ...

```

## Additional resources

The following resources cover related topics that may be useful for your application.

- **[Text and transcriptions](https://docs.livekit.io/agents/build/text.md)**: Integrate realtime text features into your agent.

- **[Pipeline nodes](https://docs.livekit.io/agents/build/nodes.md)**: Learn how to customize the behavior of your agent by overriding nodes in the voice pipeline.

- **[Inference pricing](https://livekit.io/pricing/inference#stt)**: The latest pricing information for STT models in LiveKit Inference.

---

This document was rendered at 2025-11-19T10:41:49.808Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt.md](https://docs.livekit.io/agents/models/stt.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º LiveKit Inference â€º AssemblyAI

---

# AssemblyAI STT

> Reference for AssemblyAI STT in LiveKit Inference.

## Overview

LiveKit Inference offers transcription powered by AssemblyAI. Pricing information is available on the [pricing page](https://livekit.io/pricing/inference#stt).

| Model name | Model ID | Languages |
| -------- | -------- | --------- |
| Universal-Streaming | `assemblyai/universal-streaming` | `en`, `en-US` |

## Usage

To use AssemblyAI, pass a descriptor with the model and language to the `stt` argument in your `AgentSession`:

**Python**:

```python
from livekit.agents import AgentSession

session = AgentSession(
    stt="assemblyai/universal-streaming:en",
    # ... tts, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

session = new AgentSession({
    stt: "assemblyai/universal-streaming:en",
    // ... tts, stt, vad, turn_detection, etc.
});

```

### Parameters

To customize additional parameters, use the `STT` class from the `inference` module:

**Python**:

```python
from livekit.agents import AgentSession, inference

session = AgentSession(
    stt=inference.STT(
        model="assemblyai/universal-streaming", 
        language="en"
    ),
    # ... tts, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession, inference } from '@livekit/agents';

session = new AgentSession({
    stt: new inference.STT({ 
        model: "assemblyai/universal-streaming", 
        language: "en" 
    }),
    // ... tts, stt, vad, turn_detection, etc.
});

```

- **`model`** _(string)_: The model to use for the STT.

- **`language`** _(string)_ (optional): Language code for the transcription. If not set, the provider default applies.

- **`extra_kwargs`** _(dict)_ (optional): Additional parameters to pass to the AssemblyAI Universal Streaming API, including `format_turns`, `end_of_turn_confidence_threshold`, `min_end_of_turn_silence_when_confident`, `max_turn_silence`, and `keyterms_prompt`. See the provider's [documentation](#additional-resources) for more information.

In Node.js this parameter is called `modelOptions`.

## Additional resources

The following links provide more information about AssemblyAI in LiveKit Inference.

- **[AssemblyAI Plugin](https://docs.livekit.io/agents/models/stt/plugins/assemblyai.md)**: Plugin to use your own AssemblyAI account instead of LiveKit Inference.

- **[AssemblyAI docs](https://www.assemblyai.com/docs/speech-to-text/universal-streaming)**: AssemblyAI's official documentation.

---

This document was rendered at 2025-11-19T10:42:06.946Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/inference/assemblyai.md](https://docs.livekit.io/agents/models/stt/inference/assemblyai.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º LiveKit Inference â€º Cartesia

---

# Cartesia STT

> Reference for Cartesia STT in LiveKit Inference.

## Overview

LiveKit Inference offers transcription powered by Cartesia. Pricing information is available on the [pricing page](https://livekit.io/pricing/inference#stt).

| Model name | Model ID | Languages |
| -------- | -------- | --------- |
| Ink Whisper | `cartesia/ink-whisper` | `en`, `zh`, `de`, `es`, `ru`, `ko`, `fr`, `ja`, `pt`, `tr`, `pl`, `ca`, `nl`, `ar`, `sv`, `it`, `id`, `vi`, `he`, `uk`, `el`, `ms`, `cs`, `ro`, `da`, `hu`, `ta`, `no`, `th`, `ur`, `hr`, `bg`, `lt`, `la`, `mi`, `ml`, `cy`, `sk`, `te`, `fa`, `lv`, `bn`, `sr`, `az`, `sl`, `kn`, `et`, `mk`, `br`, `eu`, `is`, `hy`, `ne`, `mn`, `bs`, `kk`, `sq`, `sw`, `gl`, `mr`, `pa`, `si`, `km`, `sn`, `yo`, `so`, `af`, `oc`, `ka`, `be`, `tg`, `sd`, `gu`, `am`, `yi`, `lo`, `uz`, `fo`, `ht`, `ps`, `tk`, `nn`, `mt`, `sa`, `lb`, `my`, `bo`, `tl`, `mg`, `as`, `tt`, `haw`, `ln`, `ha`, `ba`, `jw`, `su`, `yue` |

## Usage

To use Cartesia, pass a descriptor with the model and language to the `stt` argument in your `AgentSession`:

**Python**:

```python
from livekit.agents import AgentSession

session = AgentSession(
    stt="cartesia/ink-whisper:en",
    # ... tts, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

session = new AgentSession({
    stt: "cartesia/ink-whisper:en",
    // ... tts, stt, vad, turn_detection, etc.
});

```

### Parameters

To customize additional parameters, use the `STT` class from the `inference` module:

**Python**:

```python
from livekit.agents import AgentSession, inference

session = AgentSession(
    stt=inference.STT(
        model="cartesia/ink-whisper", 
        language="en"
    ),
    # ... tts, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession, inference } from '@livekit/agents';

session = new AgentSession({
    stt: new inference.STT({ 
        model: "cartesia/ink-whisper", 
        language: "en" 
    }),
    // ... tts, stt, vad, turn_detection, etc.
});

```

- **`model`** _(string)_: The model to use for the STT.

- **`language`** _(string)_ (optional): Language code for the transcription. If not set, the provider default applies.

- **`extra_kwargs`** _(dict)_ (optional): Additional parameters to pass to the Cartesia STT API, including `min_volume`, and `max_silence_duration_secs`. See the provider's [documentation](#additional-resources) for more information.

In Node.js this parameter is called `modelOptions`.

## Additional resources

The following links provide more information about Cartesia in LiveKit Inference.

- **[Cartesia Plugin](https://docs.livekit.io/agents/models/stt/plugins/cartesia.md)**: Plugin to use your own Cartesia account instead of LiveKit Inference.

- **[Cartesia TTS models](https://docs.livekit.io/agents/models/tts/inference/cartesia.md)**: Cartesia TTS models in LiveKit Inference.

- **[Cartesia docs](https://cartesia.ai/docs)**: Cartesia's official documentation.

---

This document was rendered at 2025-11-19T10:42:12.853Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/inference/cartesia.md](https://docs.livekit.io/agents/models/stt/inference/cartesia.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º LiveKit Inference â€º Deepgram

---

# Deepgram STT

> Reference for Deepgram STT in LiveKit Inference.

## Overview

LiveKit Inference offers transcription powered by Deepgram. Pricing information is available on the [pricing page](https://livekit.io/pricing/inference#stt).

| Model name | Model ID | Languages |
| -------- | -------- | --------- |
| Nova-3 | `deepgram/nova-3` | `en`, `en-US`, `en-AU`, `en-GB`, `en-IN`, `en-NZ`, `de`, `nl`, `sv`, `sv-SE`, `da`, `da-DK`, `es`, `es-419`, `fr`, `fr-CA`, `pt`, `pt-BR`, `pt-PT`, `multi` |
| Nova-3 Medical | `deepgram/nova-3-medical` | `en`, `en-US`, `en-AU`, `en-CA`, `en-GB`, `en-IE`, `en-IN`, `en-NZ` |
| Nova-2 | `deepgram/nova-2` | `multi`, `bg`, `ca`, `zh`, `zh-CN`, `zh-Hans`, `zh-TW`, `zh-Hant`, `zh-HK`, `cs`, `da`, `da-DK`, `nl`, `en`, `en-US`, `en-AU`, `en-GB`, `en-NZ`, `en-IN`, `et`, `fi`, `nl-BE`, `fr`, `fr-CA`, `de`, `de-CH`, `el`, `hi`, `hu`, `id`, `it`, `ja`, `ko`, `ko-KR`, `lv`, `lt`, `ms`, `no`, `pl`, `pt`, `pt-BR`, `pt-PT`, `ro`, `ru`, `sk`, `es`, `es-419`, `sv`, `sv-SE`, `th`, `th-TH`, `tr`, `uk`, `vi` |
| Nova-2 Medical | `deepgram/nova-2-medical` | `en`, `en-US` |
| Nova-2 Conversational AI | `deepgram/nova-2-conversationalai` | `en`, `en-US` |
| Nova-2 Phonecall | `deepgram/nova-2-phonecall` | `en`, `en-US` |

## Usage

To use Deepgram, pass a descriptor with the model and language to the `stt` argument in your `AgentSession`:

**Python**:

```python
from livekit.agents import AgentSession

session = AgentSession(
    stt="deepgram/nova-3:en",
    # ... tts, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession } from '@livekit/agents';

session = new AgentSession({
    stt: "deepgram/nova-3:en",
    // ... tts, stt, vad, turn_detection, etc.
});

```

### Multilingual transcription

Deepgram Nova-3 and Nova-2 models support multilingual transcription. In this mode, the model automatically detects the language of each segment of speech and can accurately transcribe multiple languages in the same audio stream.

Multilingual transcription is billed at a different rate than monolingual transcription. Refer to the [pricing page](https://livekit.io/pricing/inference#stt) for more information.

To enable multilingual transcription on supported models, set the language to `multi`.

### Parameters

To customize additional parameters, including the language to use, use the `STT` class from the `inference` module:

**Python**:

```python
from livekit.agents import AgentSession, inference

session = AgentSession(
    stt=inference.STT(
        model="deepgram/nova-3", 
        language="en"
    ),
    # ... tts, stt, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import { AgentSession, inference } from '@livekit/agents';

session = new AgentSession({
    stt: new inference.STT({ 
        model: "deepgram/nova-3", 
        language: "en" 
    }),
    // ... tts, stt, vad, turn_detection, etc.
});

```

- **`model`** _(string)_: The model to use for the STT.

- **`language`** _(string)_ (optional): Language code for the transcription. If not set, the provider default applies. Set it to `multi` with supported models for multilingual transcription.

- **`extra_kwargs`** _(dict)_ (optional): Additional parameters to pass to the Deepgram STT API, including `filler_words`, `interim_results`, `endpointing`, `punctuate`, `smart_format`, `keywords`, `keyterms`, `profanity_filter`, `numerals`, and `mip_opt_out`.

In Node.js this parameter is called `modelOptions`.

## Additional resources

The following links provide more information about Deepgram in LiveKit Inference.

- **[Deepgram Plugin](https://docs.livekit.io/agents/models/stt/plugins/deepgram.md)**: Plugin to use your own Deepgram account instead of LiveKit Inference.

- **[Deepgram docs](https://developers.deepgram.com/docs)**: Deepgram service documentation.

---

This document was rendered at 2025-11-19T10:42:17.644Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/inference/deepgram.md](https://docs.livekit.io/agents/models/stt/inference/deepgram.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Plugins â€º AssemblyAI

---

# AssemblyAI plugin guide

> How to use the AssemblyAI STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [AssemblyAI](https://www.assemblyai.com/docs/speech-to-text/streaming) as an STT provider for your voice agents.

> ðŸ’¡ **LiveKit Inference**
> 
> AssemblyAI is also available in LiveKit Inference, with billing and integration handled automatically. See [the docs](https://docs.livekit.io/agents/models/stt/inference/assemblyai.md) for more information.

## Quick reference

This section provides a brief overview of the AssemblyAI STT plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[assemblyai]~=1.2"

```

### Authentication

The AssemblyAI plugin requires an [AssemblyAI API key](https://www.assemblyai.com/docs/api-reference/overview#authorization).

Set `ASSEMBLYAI_API_KEY` in your `.env` file.

### Usage

Use AssemblyAI STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import assemblyai

session = AgentSession(
    stt = assemblyai.STT(),
    # ... vad, llm, tts, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/assemblyai/stt.html.md) for a complete list of all available parameters.

- **`format_turns`** _(bool)_ (optional) - Default: `True`: Whether to return formatted final transcripts. If enabled, formatted final transcripts are emitted shortly following an end-of-turn detection.

- **`end_of_turn_confidence_threshold`** _(float)_ (optional) - Default: `0.7`: The confidence threshold to use when determining if the end of a turn has been reached.

- **`min_end_of_turn_silence_when_confident`** _(int)_ (optional) - Default: `160`: The minimum duration of silence required to detect end of turn when confident.

- **`max_turn_silence`** _(int)_ (optional) - Default: `2400`: The maximum duration of silence allowed in a turn before end of turn is triggered.

## Turn detection

AssemblyAI includes a custom phrase endpointing model that uses both audio and linguistic information to detect turn boundaries. To use this model for [turn detection](https://docs.livekit.io/agents/build/turns.md), set `turn_detection="stt"` in the `AgentSession` constructor. You should also provide a VAD plugin for responsive interruption handling.

```python
session = AgentSession(
    turn_detection="stt",
    stt=assemblyai.STT(
      end_of_turn_confidence_threshold=0.7,
      min_end_of_turn_silence_when_confident=160,
      max_turn_silence=2400,
    ),
    vad=silero.VAD.load(), # Recommended for responsive interruption handling
    # ... llm, tts, etc.
)

```

## Additional resources

The following resources provide more information about using AssemblyAI with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-assemblyai/)**: The `livekit-plugins-assemblyai` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/assemblyai/stt.html.md)**: Reference for the AssemblyAI STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-assemblyai)**: View the source or contribute to the LiveKit AssemblyAI STT plugin.

- **[AssemblyAI docs](https://www.assemblyai.com/docs/speech-to-text/universal-streaming)**: AssemblyAI's full docs for the Universal Streaming API.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and AssemblyAI.

- **[AssemblyAI LiveKit guide](https://www.assemblyai.com/docs/integrations/livekit)**: Guide to using AssemblyAI Universal Streaming STT with LiveKit.

---

This document was rendered at 2025-11-19T10:42:29.673Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/assemblyai.md](https://docs.livekit.io/agents/models/stt/plugins/assemblyai.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Partner spotlight â€º AWS â€º Amazon Transcribe STT Plugin

---

# Amazon Transcribe plugin guide

> How to use the Amazon Transcribe STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Amazon Transcribe](https://docs.aws.amazon.com/transcribe/latest/dg/what-is.html) as an STT provider for your voice agents.

## Quick reference

This section provides a brief overview of the Amazon Transcribe STT plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[aws]~=1.2"

```

### Authentication

The Amazon Transcribe plugin requires an [AWS API key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html).

Set the following environment variables in your `.env` file:

```shell
AWS_ACCESS_KEY_ID=<aws-access-key-id>
AWS_SECRET_ACCESS_KEY=<aws-secret-access-key>
AWS_DEFAULT_REGION=<aws-deployment-region>

```

### Usage

Use Amazon Transcribe STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import aws

session = AgentSession(
   stt = aws.STT(
      session_id="my-session-id",
      language="en-US",
      vocabulary_name="my-vocabulary",
      vocab_filter_name="my-vocab-filter",
      vocab_filter_method="mask",
   ),
   # ... llm, tts, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/aws/index.html.md#livekit.plugins.aws.STT) for a complete list of all available parameters.

- **`speech_region`** _(string)_ (optional) - Default: `us-east-1` - Environment: `AWS_DEFAULT_REGION`: The region of the AWS deployment. Required if the environment variable isn't set.

- **`language`** _(string)_ (optional) - Default: `en-US`: The language of the audio. For a full list of supported languages, see the [Supported languages](https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html) page.

- **`vocabulary_name`** _(string)_ (optional) - Default: `None`: Name of the custom vocabulary you want to use when processing your transcription. To learn more, see [Custom vocabularies](https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html).

- **`session_id`** _(string)_ (optional): Name for your transcription session. If left empty, Amazon Transcribe generates an ID and returns it in the response.

- **`vocab_filter_name`** _(string)_ (optional) - Default: `None`: Name of the custom vocabulary filter that you want to use when processing your transcription. To learn more, see [Using custom vocabulary filters to delete, mask, or flag words](https://docs.aws.amazon.com/transcribe/latest/dg/vocabulary-filtering.html).

- **`vocab_filter_method`** _(string)_ (optional) - Default: `None`: Display method for the vocabulary filter. To learn more, see [Using custom vocabulary filters to delete, mask, or flag words](https://docs.aws.amazon.com/transcribe/latest/dg/vocabulary-filtering.html).

## Additional resources

The following resources provide more information about using Amazon Transcribe with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-aws/)**: The `livekit-plugins-aws` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/aws/index.html.md#livekit.plugins.aws.STT)**: Reference for the Amazon Transcribe STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-aws)**: View the source or contribute to the LiveKit Amazon Transcribe STT plugin.

- **[AWS docs](https://docs.aws.amazon.com/transcribe/latest/dg/what-is.html)**: Amazon Transcribe's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Amazon Transcribe.

---

This document was rendered at 2025-11-19T10:42:36.505Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/aws.md](https://docs.livekit.io/agents/models/stt/plugins/aws.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Partner spotlight â€º Azure â€º Azure AI Speech STT Plugin

---

# Azure Speech STT plugin guide

> How to use the Azure Speech STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Azure Speech](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/overview) as an STT provider for your voice agents.

## Quick reference

This section provides a brief overview of the Azure Speech STT plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[azure]~=1.2"

```

### Authentication

The Azure Speech plugin requires an [Azure Speech key](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-to-text?tabs=macos,terminal&pivots=programming-language-python#prerequisites).

Set the following environment variables in your `.env` file:

```shell
AZURE_SPEECH_KEY=<azure-speech-key>
AZURE_SPEECH_REGION=<azure-speech-region>
AZURE_SPEECH_HOST=<azure-speech-host>

```

### Usage

Use Azure Speech STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import azure

azure_stt = stt.STT(
  speech_key="<speech_service_key>",
  speech_region="<speech_service_region>",
)

```

> â„¹ï¸ **Note**
> 
> To create an instance of `azure.STT`, one of the following options must be met:
> 
> - `speech_host` must be set, _or_
> - `speech_key` _and_ `speech_region` must both be set, _or_
> - `speech_auth_token` _and_ `speech_region` must both be set

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/azure/index.html.md#livekit.plugins.azure.STT) for a complete list of all available parameters.

- **`speech_key`** _(string)_ (optional) - Environment: `AZURE_SPEECH_KEY`: Azure Speech speech-to-text key. To learn more, see [Azure Speech prerequisites](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-to-text?tabs=macos,terminal&pivots=programming-language-python#prerequisites).

- **`speech_region`** _(string)_ (optional) - Environment: `AZURE_SPEECH_REGION`: Azure Speech speech-to-text region. To learn more, see [Azure Speech prerequisites](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-to-text#prerequisites).

- **`speech_host`** _(string)_ (optional) - Environment: `AZURE_SPEECH_HOST`: Azure Speech endpoint.

- **`speech_auth_token`** _(string)_ (optional): Azure Speech authentication token.

- **`languages`** _(list[string])_ (optional): List of potential source languages. To learn more, see [Standard locale names](https://learn.microsoft.com/en-us/globalization/locale/standard-locale-names).

## Additional resources

The following resources provide more information about using Azure Speech with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-azure/)**: The `livekit-plugins-azure` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/azure/index.html.md#livekit.plugins.azure.STT)**: Reference for the Azure Speech STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-azure)**: View the source or contribute to the LiveKit Azure Speech STT plugin.

- **[Azure Speech docs](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/overview)**: Azure Speech's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Azure Speech.

- **[Azure ecosystem guide](https://docs.livekit.io/agents/integrations/azure.md)**: Overview of the entire Azure AI and LiveKit Agents integration.

---

This document was rendered at 2025-11-19T10:42:43.115Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/azure.md](https://docs.livekit.io/agents/models/stt/plugins/azure.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Partner spotlight â€º Azure â€º Azure OpenAI STT Plugin

---

# Azure OpenAI STT plugin guide

> How to use the Azure OpenAI STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service/) as an STT provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Support for Azure OpenAI STT is available in the `openai` plugin.

Install the plugin from PyPI:

```shell
uv add "livekit-agents[openai]~=1.2"

```

### Authentication

The Azure OpenAI TTS requires [authentication](https://learn.microsoft.com/en-us/azure/api-management/api-management-authenticate-authorize-azure-openai) using an API key or a managed identity.

Set the following environment variables in your `.env` file:

```shell
AZURE_OPENAI_API_KEY=<azure-openai-api-key>
AZURE_OPENAI_AD_TOKEN=<azure-openai-ad-token>
AZURE_OPENAI_ENDPOINT=<azure-openai-endpoint>

```

### Usage

Use Azure OpenAI STT within an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import openai

session = AgentSession(
  stt = openai.STT.with_azure(
    model="gpt-4o-transcribe",
  ),
  # ... llm, tts, etc.
)

```

### Parameters

This section describes some of the available parameters. For a complete reference of all available parameters, see the [plugin reference](https://docs.livekit.io/reference/python/livekit/plugins/openai/index.html.md#livekit.plugins.openai.STT.with_azure).

- **`language`** _(string)_ (optional) - Default: `en`: Language code for the transcription.

- **`model`** _(STTModels | string)_ (optional) - Default: `gpt-4o-mini-transcribe`: ID of the model to use for speech-to-text.

- **`prompt`** _(string)_ (optional): Initial prompt to guide the transcription.

- **`azure_endpoint`** _(string)_ (optional) - Environment: `AZURE_OPENAI_ENDPOINT`: Azure OpenAI endpoint in the following format: `https://{your-resource-name}.openai.azure.com`.

- **`azure_deployment`** _(string)_ (optional): Name of your model deployment.

- **`api_version`** _(string)_ (optional) - Environment: `OPENAI_API_VERSION`: OpenAI REST API version used for the request.

- **`api_key`** _(string)_ (optional) - Environment: `AZURE_OPENAI_API_KEY`: Azure OpenAI API key.

- **`azure_ad_token`** _(string)_ (optional) - Environment: `AZURE_OPENAI_AD_TOKEN`: Azure Active Directory token.

- **`organization`** _(string)_ (optional) - Environment: `OPENAI_ORG_ID`: OpenAI organization ID.

- **`project`** _(string)_ (optional) - Environment: `OPENAI_PROJECT_ID`: OpenAI project ID.

## Additional resources

The following resources provide more information about using Azure OpenAI with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-openai/)**: The `livekit-plugins-openai` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/openai/index.html.md#livekit.plugins.openai.STT.with_azure)**: Reference for the Azure OpenAI STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-openai)**: View the source or contribute to the LiveKit Azure OpenAI plugin.

- **[Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/)**: Azure OpenAI documentation.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Azure OpenAI.

- **[Azure ecosystem guide](https://docs.livekit.io/agents/integrations/azure.md)**: Overview of the entire Azure AI and LiveKit Agents integration.

---

This document was rendered at 2025-11-19T10:42:48.818Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/azure-openai.md](https://docs.livekit.io/agents/models/stt/plugins/azure-openai.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Plugins â€º Baseten

---

# Baseten STT plugin guide

> How to use the Baseten STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Baseten](https://www.baseten.co/) as an STT provider for your voice agents.

## Quick reference

This section provides a quick reference for the Baseten STT plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[baseten]~=1.2"

```

### Authentication

The Baseten plugin requires a [Baseten API key](https://app.baseten.co/settings/api-keys).

Set the following in your `.env` file:

```shell
BASETEN_API_KEY=<your-baseten-api-key>

```

### Model deployment

You must deploy a websocket-based STT model to Baseten to use it with LiveKit Agents. The standard Whisper deployments available in the Baseten library are not suitable for realtime use. Contact Baseten support for help deploying a websocket-compatible Whisper model.

Your model endpoint may show as an HTTP URL such as `https://model-<id>.api.baseten.co/environments/production/predict`. The domain is correct but you must change the protocol to `wss` and the path to `/v1/websocket` to use it as the `model_endpoint` parameter for the Baseten STT plugin.

The correct websocket URL format is:

```
wss://<your-model-id>.api.baseten.co/v1/websocket

```

### Usage

Use Baseten STT within an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import baseten

session = AgentSession(
   stt=baseten.STT(
      model_endpoint="wss://<your-model-id>.api.baseten.co/v1/websocket",
   )
   # ... llm, tts, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/baseten/index.html.md#livekit.plugins.baseten.STT) for a complete list of all available parameters.

- **`model_endpoint`** _(string)_ (optional) - Environment: `BASETEN_MODEL_ENDPOINT`: The endpoint URL for your deployed model. You can find this in your Baseten dashboard. Note that this must be a websocket URL (starts with `wss://`). See [Model deployment](#model-deployment) for more details.

- **`language`** _(string)_ (optional) - Default: `en`: Language of input audio in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) format.

- **`vad_threshold`** _(float)_ (optional) - Default: `0.5`: Threshold for voice activity detection.

- **`vad_min_silence_duration_ms`** _(int)_ (optional) - Default: `300`: Minimum duration of silence in milliseconds to consider speech ended.

- **`vad_speech_pad_ms`** _(int)_ (optional) - Default: `30`: Duration in milliseconds to pad speech segments.

## Additional resources

The following resources provide more information about using Baseten with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-baseten/)**: The `livekit-plugins-baseten` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/baseten/index.html.md#livekit.plugins.baseten.STT)**: Reference for the Baseten STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-baseten)**: View the source or contribute to the LiveKit Baseten STT plugin.

- **[Baseten docs](https://docs.baseten.co/)**: Baseten's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Baseten.

- **[Baseten TTS](https://docs.livekit.io/agents/models/tts/plugins/baseten.md)**: Guide to the Baseten TTS plugin with LiveKit Agents.

---

This document was rendered at 2025-11-19T10:42:55.062Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/baseten.md](https://docs.livekit.io/agents/models/stt/plugins/baseten.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Plugins â€º Cartesia

---

# Cartesia STT plugin guide

> How to use the Cartesia STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Cartesia](https://www.cartesia.ai/) as an STT provider for your voice agents.

> ðŸ’¡ **LiveKit Inference**
> 
> Cartesia STT is also available in LiveKit Inference, with billing and integration handled automatically. See [the docs](https://docs.livekit.io/agents/models/stt/inference/cartesia.md) for more information.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[cartesia]~=1.2"

```

### Authentication

The Cartesia plugin requires a [Cartesia API key](https://play.cartesia.ai/keys).

Set `CARTESIA_API_KEY` in your `.env` file.

### Usage

Use Cartesia STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import cartesia

session = AgentSession(
   stt = cartesia.STT(
      model="ink-whisper"
   ),
   # ... llm, tts, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/cartesia/index.html.md#livekit.plugins.cartesia.STT) for a complete list of all available parameters.

- **`model`** _(string)_ (optional) - Default: `ink-whisper`: Selected model to use for STT. See [Cartesia STT models](https://docs.cartesia.ai/build-with-cartesia/models/stt) for supported values.

- **`language`** _(string)_ (optional) - Default: `en`: Language of input audio in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) format. See [Cartesia STT models](https://docs.cartesia.ai/build-with-cartesia/models/stt) for supported values.

## Additional resources

The following resources provide more information about using Cartesia with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-cartesia/)**: The `livekit-plugins-cartesia` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/cartesia/index.html.md#livekit.plugins.cartesia.STT)**: Reference for the Cartesia STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-cartesia)**: View the source or contribute to the LiveKit Cartesia STT plugin.

- **[Cartesia docs](https://docs.cartesia.ai/build-with-cartesia/models/stt)**: Cartesia STT docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Cartesia STT.

- **[Cartesia TTS](https://docs.livekit.io/agents/models/tts/plugins/cartesia.md)**: Guide to the Cartesia TTS plugin with LiveKit Agents.

---

This document was rendered at 2025-11-19T10:43:00.476Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/cartesia.md](https://docs.livekit.io/agents/models/stt/plugins/cartesia.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Plugins â€º Clova

---

# CLOVA STT plugin guide

> How to use the Clova STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [CLOVA Speech Recognition](https://guide.ncloud-docs.com/docs/en/csr-overview) as an STT provider for your voice agents.

## Quick reference

This section provides a brief overview of the CLOVA STT plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[clova]~=1.2"

```

### Authentication

The CLOVA plugin requires the following keys, which may set as environment variables or passed to the constructor.

```shell
CLOVA_STT_SECRET_KEY=<your-api-key>
CLOVA_STT_INVOKE_URL=<your-invoke-url>

```

### Usage

Create a CLOVA STT to use within an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import clova

session = AgentSession(
    stt = clova.STT(
      word_boost=["LiveKit"],
    ),
    # ... llm, tts, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/clova/index.html.md#livekit.plugins.clova.STT) for a complete list of all available parameters.

- **`language`** _(ClovaSttLanguages)_ (optional) - Default: `en-US`: Speech recognition language. Clova supports English, Korean, Japanese, and Chinese. Valid values are `ko-KR`, `en-US`, `enko`, `ja`, `zh-cn`, `zh-tw`.

## Additional resources

The following resources provide more information about using CLOVA with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-clova/)**: The `livekit-plugins-clova` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/clova/index.html.md#livekit.plugins.clova.STT)**: Reference for the CLOVA STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-clova)**: View the source or contribute to the LiveKit CLOVA STT plugin.

- **[CLOVA docs](https://guide.ncloud-docs.com/docs/en/csr-overview)**: CLOVA's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and CLOVA.

---

This document was rendered at 2025-11-19T10:43:05.967Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/clova.md](https://docs.livekit.io/agents/models/stt/plugins/clova.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Plugins â€º Deepgram

---

# Deepgram STT plugin guide

> How to use the Deepgram STT plugin for LiveKit Agents.

Available in:
- [x] Node.js
- [x] Python

## Overview

This plugin allows you to use [Deepgram](https://deepgram.com/) as an STT provider for your voice agents.

> ðŸ’¡ **LiveKit Inference**
> 
> Deepgram STT is also available in LiveKit Inference, with billing and integration handled automatically. See [the docs](https://docs.livekit.io/agents/models/stt/inference/deepgram.md) for more information.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

**Python**:

```shell
uv add "livekit-agents[deepgram]~=1.2"

```

---

**Node.js**:

```shell
pnpm add @livekit/agents-plugin-deepgram@1.x

```

### Authentication

The Deepgram plugin requires a [Deepgram API key](https://console.deepgram.com/).

Set `DEEPGRAM_API_KEY` in your `.env` file.

### Usage

Use Deepgram STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

**Python**:

```python
from livekit.plugins import deepgram

session = AgentSession(
   stt=deepgram.STTv2(
      model="flux-general-en",
      eager_eot_threshold=0.4,
   ),
   # ... llm, tts, etc.
)

```

---

**Node.js**:

```typescript
import * as deepgram from '@livekit/agents-plugin-deepgram';

const session = new voice.AgentSession({
    stt: new deepgram.STT(
        model: "nova-3"
    ),
    // ... llm, tts, etc.
});

```

### Parameters

This section describes some of the available parameters. See the plugin reference links in the [Additional resources](#additional-resources) section for more details.

- **`model`** _(string)_ (optional) - Default: `nova-3`: The Deepgram model to use for speech recognition.

- **`keyterms`** _(list[string])_ (optional) - Default: `[]`: List of key terms to improve recognition accuracy. Supported by Nova-3 models.

## Additional resources

The following resources provide more information about using Deepgram with LiveKit Agents.

- **[Deepgram docs](https://developers.deepgram.com/docs)**: Deepgram's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Deepgram.

- **[Deepgram TTS](https://docs.livekit.io/agents/models/tts/plugins/deepgram.md)**: Guide to the Deepgram TTS plugin with LiveKit Agents.

---

This document was rendered at 2025-11-19T10:43:11.899Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/deepgram.md](https://docs.livekit.io/agents/models/stt/plugins/deepgram.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Plugins â€º fal

---

# fal STT plugin guide

> How to use the fal STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [fal](https://fal.ai/) as an STT provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[fal]~=1.2"

```

### Authentication

The fal plugin requires a [fal API key](https://fal.ai/dashboard/keys).

Set `FAL_KEY` in your `.env` file.

### Usage

Use fal STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import fal

session = AgentSession(
   stt = fal.STT(
      language="de",
   ),
   # ... llm, tts, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/fal/index.html.md#livekit.plugins.fal.STT) for a complete list of all available parameters.

- **`language`** _(str)_ (optional) - Default: `en`: Speech recognition language.

## Additional resources

The following resources provide more information about using fal with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-fal/)**: The `livekit-plugins-fal` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/fal/index.html.md#livekit.plugins.fal.STT)**: Reference for the fal STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-fal)**: View the source or contribute to the LiveKit fal STT plugin.

- **[fal docs](https://fal.ai/docs)**: fal's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and fal.

---

This document was rendered at 2025-11-19T10:43:17.199Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/fal.md](https://docs.livekit.io/agents/models/stt/plugins/fal.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Plugins â€º Gladia

---

# Gladia STT plugin guide

> How to use the Gladia STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Gladia](https://gladia.io/) as an STT provider for your voice agents.

## Quick reference

This section provides a brief overview of the Gladia STT plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[gladia]~=1.2"

```

### Authentication

The Gladia plugin requires a [Gladia API key](https://app.gladia.io/account).

Set `GLADIA_API_KEY` in your `.env` file.

### Initialization

Use Gladia STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import gladia

session = AgentSession(
    stt = gladia.STT(),
    # ... llm, tts, etc.
)

```

### Realtime translation

To use realtime translation, set `translation_enabled` to `True` and specify the expected audio languages in `languages` and the desired target language in `translation_target_languages`.

For example, to transcribe and translate a mixed English and French audio stream into English, set the following options:

```python
gladia.STT(
    translation_enabled=True,
    languages=["en", "fr"],
    translation_target_languages=["en"]
)

```

Note that if you specify more than one target language, the plugin emits a separate transcription event for each. When used in an `AgentSession`, this adds each transcription to the conversation history, in order, which might confuse the LLM.

### Updating options

Use the `update_options` method to configure the STT on the fly:

```python

gladia_stt = gladia.STT()

gladia_stt.update_options(
    languages=["ja", "en"],
    translation_enabled=True,
    translation_target_languages=["fr"]
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/gladia/index.html.md#livekit.plugins.gladia.STT) for a complete list of all available parameters.

- **`languages`** _(list[string])_ (optional) - Default: `[]`: List of languages to use for transcription. If empty, Gladia will auto-detect the language.

- **`code_switching`** _(bool)_ (optional) - Default: `false`: Enable switching between languages during recognition.

- **`translation_enabled`** _(bool)_ (optional) - Default: `false`: Enable real-time translation.

- **`translation_target_languages`** _(list[string])_ (optional) - Default: `[]`: List of target languages for translation.

## Additional resources

The following resources provide more information about using Gladia with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-gladia/)**: The `livekit-plugins-gladia` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/gladia/index.html.md#livekit.plugins.gladia.STT)**: Reference for the Gladia STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-gladia)**: View the source or contribute to the LiveKit Gladia STT plugin.

- **[Gladia documentation](https://docs.gladia.io/)**: Gladia's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Gladia.

---

This document was rendered at 2025-11-19T10:43:22.273Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/gladia.md](https://docs.livekit.io/agents/models/stt/plugins/gladia.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Partner spotlight â€º Google â€º Google Cloud STT Plugin

---

# Google Cloud STT plugin guide

> How to use the Google Cloud STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Google Cloud STT](https://cloud.google.com/speech-to-text) as an STT provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[google]~=1.2"

```

### Authentication

Google Cloud credentials must be provided by one of the following methods:

- Passed in the `credentials_info` dictionary.
- Saved in the `credentials_file` JSON file (`GOOGLE_APPLICATION_CREDENTIALS` environment variable).
- Application Default Credentials. To learn more, see [How Application Default Credentials works](https://cloud.google.com/docs/authentication/application-default-credentials)

### Usage

Use a Google Cloud STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import google

session = AgentSession(
  stt = google.STT(
    model="chirp",
    spoken_punctuation=False,
  ),
  # ... llm, tts, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/google/index.html.md#livekit.plugins.google.STT) for a complete list of all available parameters.

- **`languages`** _(LanguageCode)_ (optional) - Default: `en-US`: Specify input languages. For a full list of supported languages, see [Speech-to-text supported languages](https://cloud.google.com/speech-to-text/docs/speech-to-text-supported-languages).

- **`spoken_punctuation`** _(boolean)_ (optional) - Default: `True`: Replace spoken punctuation with punctuation characters in text.

- **`model`** _(SpeechModels | string)_ (optional) - Default: `long`: Model to use for speech-to-text. To learn more, see [Select a transcription model](https://cloud.google.com/speech-to-text/docs/transcription-model).

- **`credentials_info`** _(array)_ (optional): Key-value pairs of authentication credential information.

- **`credentials_file`** _(string)_ (optional): Name of the JSON file that contains authentication credentials for Google Cloud.

## Additional resources

The following resources provide more information about using Google Cloud with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-google/)**: The `livekit-plugins-google` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/google/index.html.md#livekit.plugins.google.STT)**: Reference for the Google Cloud STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-google)**: View the source or contribute to the LiveKit Google Cloud STT plugin.

- **[Google Cloud docs](https://cloud.google.com/speech-to-text/docs)**: Google Cloud STT docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Google Cloud STT.

- **[Google ecosystem guide](https://docs.livekit.io/agents/integrations/google.md)**: Overview of the entire Google AI and LiveKit Agents integration.

---

This document was rendered at 2025-11-19T10:43:29.716Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/google.md](https://docs.livekit.io/agents/models/stt/plugins/google.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Partner spotlight â€º Groq â€º Groq STT Plugin

---

# Groq STT plugin guide

> How to use the Groq STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Groq](https://groq.com/) as an STT provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

**Python**:

```shell
uv add "livekit-agents[groq]~=1.2"

```

---

**Node.js**:

For Node.js, support for Groq STT is available using the OpenAI plugin.

```shell
pnpm add @livekit/agents-plugin-openai@1.x

```

### Authentication

The Groq plugin requires a [Groq API key](https://console.groq.com/keys).

Set `GROQ_API_KEY` in your `.env` file.

### Usage

Use Groq STT in your `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

**Python**:

```python
from livekit.plugins import groq
   
session = AgentSession(
   stt=groq.STT(
      model="whisper-large-v3-turbo",
      language="en",
   ),
   # ... tts, llm, vad, turn_detection, etc.
)

```

---

**Node.js**:

```typescript
import * as openai from '@livekit/agents-plugin-openai';

const session = new voice.AgentSession({
    stt: new openai.STT.withGroq(
        model: "whisper-large-v3-turbo"
    ),
    // ... tts, llm, vad, turn_detection, etc.
});

```

### Parameters

This section describes some of the available parameters. See the plugin reference links in the [Additional resources](#additional-resources) section for a complete list of all available parameters.

- **`model`** _(string)_ (optional) - Default: `whisper-large-v3-turbo`: Name of the STT model to use. For help with model selection, see the [Groq STT documentation](https://console.groq.com/docs/speech-to-text).

- **`language`** _(string)_ (optional) - Default: `en`: Language of the input audio in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) format.

- **`prompt`** _(string)_ (optional): Prompt to guide the model's style or specify how to spell unfamiliar words. 224 tokens max.

## Additional resources

The following resources provide more information about using Groq with LiveKit Agents.

- **[Groq docs](https://console.groq.com/docs/speech-to-text)**: Groq STT docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Groq STT.

- **[Groq ecosystem guide](https://docs.livekit.io/agents/integrations/groq.md)**: Overview of the entire Groq and LiveKit Agents integration.

---

This document was rendered at 2025-11-19T10:43:35.680Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/groq.md](https://docs.livekit.io/agents/models/stt/plugins/groq.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Plugins â€º Mistral AI

---

# Mistral STT plugin guide

> How to use the Mistral STT plugin for LiveKit Agents.

## Overview

This plugin allows you to use [Voxtral](https://mistral.ai/products/voxtral) as an STT provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

#### Installation

Install the LiveKit Mistral AI plugin from PyPI:

```shell
uv add "livekit-agents[mistralai]~=1.2"

```

### Authentication

The Mistral AI integration requires a [Mistral AI API key](https://console.mistral.ai/api-keys/).

Set the `MISTRAL_API_KEY` in your `.env` file.

### Usage

Use Mistral AI STT in your `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import mistralai

session = AgentSession(
    stt=mistralai.STT(
        model="voxtral-mini-2507"   
    ),
    # ... llm, tts, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/mistralai.md#livekit.plugins.mistralai.STT) for a complete list of all available parameters.

- **`model`** _(string)_ (optional) - Default: `voxtral-mini-latest`: Name of the Voxtral STT model to use.

## Additional resources

The following resources provide more information about using Groq with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-mistralai)**: The `livekit-plugins-mistralai` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/mistralai.md#livekit.plugins.mistralai.STT)**: Reference for the Mistral AI STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-mistralai)**: View the source or contribute to the LiveKit Mistral AI LLM plugin.

- **[Mistral AI LLM plugin](https://docs.livekit.io/agents/models/llm/plugins/mistralai.md)**: Mistral AI LLM plugin documentation.

- **[Mistral AI platform docs](https://docs.mistral.ai/)**: Mistral AI platform documentation.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Mistral AI.

---

This document was rendered at 2025-11-19T10:43:41.619Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/mistralai.md](https://docs.livekit.io/agents/models/stt/plugins/mistralai.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Partner spotlight â€º OpenAI â€º OpenAI STT Plugin

---

# OpenAI STT plugin guide

> How to use the OpenAI STT plugin for LiveKit Agents.

Available in:
- [x] Node.js
- [x] Python

## Overview

This plugin allows you to use [OpenAI](https://platform.openai.com) as an STT provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

**Python**:

```shell
uv add "livekit-agents[openai]~=1.2"

```

---

**Node.js**:

```shell
pnpm add @livekit/agents-plugin-openai@1.x

```

### Authentication

The OpenAI plugin requires an [OpenAI API key](https://platform.openai.com/api-keys).

Set `OPENAI_API_KEY` in your `.env` file.

### Usage

Use OpenAI STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

**Python**:

```python
from livekit.plugins import openai

session = AgentSession(
  stt = openai.STT(
    model="gpt-4o-transcribe",
  ),
  # ... llm, tts, etc.
)

```

---

**Node.js**:

```typescript
import * as openai from '@livekit/agents-plugin-openai';

const session = new voice.AgentSession({
    stt: new openai.STT(
        model: "gpt-4o-transcribe"
    ),
    // ... llm, tts, etc.
});

```

### Parameters

This section describes some of the available parameters. See the plugin reference links in the [Additional resources](#additional-resources) section for a complete list of all available parameters.

- **`model`** _(WhisperModels | string)_ (optional) - Default: `gpt-4o-transcribe`: Model to use for transcription. See OpenAI's documentation for a list of [supported models](https://platform.openai.com/docs/models#transcription).

- **`language`** _(string)_ (optional) - Default: `en`: Language of input audio in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) format. See OpenAI's documentation for a list of [supported languages](https://platform.openai.com/docs/guides/speech-to-text#supported-languages).

## Additional resources

The following resources provide more information about using OpenAI with LiveKit Agents.

- **[OpenAI docs](https://platform.openai.com/docs/guides/speech-to-text)**: OpenAI STT docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and OpenAI STT.

- **[OpenAI ecosystem guide](https://docs.livekit.io/agents/integrations/openai.md)**: Overview of the entire OpenAI and LiveKit Agents integration.

---

This document was rendered at 2025-11-19T10:43:46.746Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/openai.md](https://docs.livekit.io/agents/models/stt/plugins/openai.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Plugins â€º Sarvam

---

# Sarvam STT plugin guide

> How to use the Sarvam STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Sarvam](https://docs.sarvam.ai/) as an STT provider for your voice agents.

## Quick reference

This section provides a quick reference for the Sarvam STT plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[sarvam]~=1.2"

```

### Authentication

The Sarvam plugin requires a [Sarvam API key](https://docs.sarvam.ai/).

Set `SARVAM_API_KEY` in your `.env` file.

### Usage

Use Sarvam STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import sarvam

session = AgentSession(
   stt=sarvam.STT(
      language="hi-IN",
      model="saarika:v2.5",
   ),
   # ... llm, tts, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/sarvam/index.html.md#livekit.plugins.sarvam.STT) for a complete list of all available parameters.

- **`language`** _(string)_ (optional): BCP-47 language code for supported Indian languages. See [documentation](https://docs.sarvam.ai/api-reference-docs/speech-to-text/transcribe#request.body.language_code.language_code) for a complete list of supported languages.

- **`model`** _(string)_ (optional) - Default: `saarika:v2.5`: The Sarvam STT model to use. See [documentation](https://docs.sarvam.ai/api-reference-docs/speech-to-text/transcribe#request.body.model) for a complete list of supported models.

## Additional resources

The following resources provide more information about using Sarvam with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-sarvam/)**: The `livekit-plugins-sarvam` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/sarvam/index.html.md#livekit.plugins.sarvam.STT)**: Reference for the Sarvam STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-sarvam)**: View the source or contribute to the LiveKit Sarvam STT plugin.

- **[Sarvam docs](https://docs.sarvam.ai/)**: Sarvam's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Sarvam.

- **[Sarvam TTS](https://docs.livekit.io/agents/models/tts/plugins/sarvam.md)**: Guide to the Sarvam TTS plugin with LiveKit Agents.

---

This document was rendered at 2025-11-19T10:43:52.441Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/sarvam.md](https://docs.livekit.io/agents/models/stt/plugins/sarvam.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Plugins â€º Soniox

---

# Soniox STT plugin guide

> How to use the Soniox STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Soniox](https://soniox.com/) as an STT provider for your voice agents.

## Quick reference

This section provides a quick reference for the Soniox STT plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[soniox]~=1.2"

```

### Authentication

The Soniox plugin requires an API key from the [Soniox console](https://console.soniox.com/).

Set `SONIOX_API_KEY` in your `.env` file.

### Usage

Use Soniox STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import soniox

session = AgentSession(
   stt=soniox.STT(),
   # ... llm, tts, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/soniox/index.html.md) for a complete list of all available parameters.

- **`model`** _(string)_ (optional) - Default: `stt-rt-preview`: The Soniox STT model to use. See [documentation](https://soniox.com/docs/stt/models) for a complete list of supported models.

- **`context`** _(string)_ (optional) - Default: `None`: Free-form text that provides additional context or vocabulary to bias transcription towards domain-specific terms.

- **`enable_language_identification`** _(boolean)_ (optional) - Default: `true`: When `true`, the Soniox attempts to identify the language of the input audio.

## Additional resources

The following resources provide more information about using Soniox with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-soniox/)**: The `livekit-plugins-soniox` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/soniox/index.html.md)**: Reference for the Soniox STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-soniox)**: View the source or contribute to the LiveKit Soniox STT plugin.

- **[Soniox docs](https://soniox.com/docs)**: Soniox's full docs site.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Soniox.

---

This document was rendered at 2025-11-19T10:43:57.405Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/soniox.md](https://docs.livekit.io/agents/models/stt/plugins/soniox.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Plugins â€º Speechmatics

---

# Speechmatics STT plugin guide

> How to use the Speechmatics STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Speechmatics](https://www.speechmatics.com/) as an STT provider for your voice agents.

## Quick reference

This section includes a basic usage example and some reference material. For links to more detailed documentation, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[speechmatics]~=1.2"

```

### Authentication

The Speechmatics plugin requires an [API key](https://docs.speechmatics.com/introduction/authentication).

Set `SPEECHMATICS_API_KEY` in your `.env` file.

### Usage

Use Speechmatics STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import speechmatics

session = AgentSession(
   stt = speechmatics.STT(),
   # ... llm, tts, etc.
)

```

### Speaker diarization

You can enable [speaker diarization](https://docs.speechmatics.com/features/diarization#speaker-diarization) to identify individual speakers and their speech. When enabled, the transcription output can include this information through the `speaker_id` and `text` attributes.

See the following for example configurations and outputs:

- `<{speaker_id}>{text}</{speaker_id}>`: `<S1>Hello</S1>`.
- `[Speaker {speaker_id}] {text}`: `[Speaker S1] Hello`.

```python
stt = speechmatics.STT(
   enable_diarization=True,
   speaker_active_format="<{speaker_id}>{text}</{speaker_id}>",
)

```

Inform the LLM of the format for speaker identification by including it in your agent instructions. For a an example, see the following:

- **[Speechmatics STT speaker diarization](https://github.com/livekit/agents/blob/main/examples/voice_agents/speaker_id_multi_speaker.py)**: An example of using Speechmatics to identify speakers in a multi-speaker conversation.

### Parameters

This section describes the key parameters for the Speechmatics STT plugin. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/speechmatics/index.html.md#livekit.plugins.speechmatics.STT) for a complete list of all available parameters.

- **`operating_point`** _(string)_ (optional) - Default: `enhanced`: Operating point to use for the transcription. This parameter balances accuracy, speed, and resource usage. To learn more, see [Operating points](https://docs.speechmatics.com/speech-to-text/#operating-points).

- **`language`** _(string)_ (optional) - Default: `en`: ISO 639-1 language code. All languages are global, meaning that regardless of which language you select, the system can recognize different dialects and accents. To see the full list, see [Supported Languages](https://docs.speechmatics.com/introduction/supported-languages).

- **`enable_partials`** _(bool)_ (optional) - Default: `true`: Enable partial transcripts. Partial transcripts allow you to receive preliminary transcriptions and update as more context is available until the higher-accuracy [final transcript](https://docs.speechmatics.com/rt-api-ref#addtranscript) is returned. Partials are returned faster but without any post-processing such as formatting. When enabled, the STT service emits `INTERIM_TRANSCRIPT` events.

- **`enable_diarization`** _(bool)_ (optional) - Default: `false`: Enable speaker diarization. When enabled, spoken words are attributed to unique speakers. You can use the `speaker_sensitivity` parameter to adjust the sensitivity of diarization. To learn more, see [Diarization](https://docs.speechmatics.com/speech-to-text/features/diarization).

- **`max_delay`** _(number)_ (optional) - Default: `1.0`: The maximum delay in seconds between the end of a spoken word and returning the final transcript results. Lower values can have an impact on accuracy.

- **`end_of_utterance_silence_trigger`** _(float)_ (optional) - Default: `0.5`: The maximum delay in seconds of silence after the end of turn before the STT service returns the final transcript.

- **`end_of_utterance_mode`** _(EndOfUtteranceMode)_ (optional) - Default: `EndOfUtteranceMode.FIXED`: The delay mode to use for triggering end of turn. Valid values are:

- `EndOfUtteranceMode.FIXED`: Delay is fixed to the value of `end_of_utterance_silence_trigger`.
- `EndOfUtteranceMode.ADAPTIVE`: Delay can be adjusted by the content of what the most recent speaker has said, including rate of speech and speaking patterns (for example, pauses).
- `EndOfUtteranceMode.NONE`: Disables end of turn detection and uses a fallback timer.
To use LiveKit's [end of turn detector model](https://docs.livekit.io/agents/build/turns.md#turn-detector-model), set this parameter to `EndOfUtteranceMode.NONE`.

- **`speaker_active_format`** _(string)_ (optional): Formatter for speaker identification in transcription output. The following attributes are available:

- `{speaker_id}`: The ID of the speaker.
- `{text}`: The text spoken by the speaker.
By default, if speaker diarization is enabled and this parameter is not set, the transcription output is _not_ formatted for speaker identification.

The system instructions for the language model might need to include any necessary instructions to handle the formatting. To learn more, see [Speaker diarization](#speaker-diarization).

- **`diarization_sensitivity`** _(float)_ (optional) - Default: `0.5`: Sensitivity of speaker detection. Valid values are between `0` and `1`. Higher values increase sensitivity and can help when two or more speakers have similar voices. To learn more, see [Speaker sensitivity](https://docs.speechmatics.com/speech-to-text/features/diarization#speaker-sensitivity).

The `enable_diarization` parameter must be set to `True` for this parameter to take effect.

- **`prefer_current_speaker`** _(bool)_ (optional) - Default: `false`: When speaker diarization is enabled and this is set to `True`, it reduces the likelihood of switching between similar sounding speakers. To learn more, see [Prefer current speaker](https://docs.speechmatics.com/speech-to-text/features/diarization#prefer-current-speaker).

## Additional resources

The following resources provide more information about using Speechmatics with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-speechmatics/)**: The `livekit-plugins-speechmatics` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/speechmatics/index.html.md#livekit.plugins.speechmatics.STT)**: Reference for the Speechmatics STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-speechmatics)**: View the source or contribute to the LiveKit Speechmatics STT plugin.

- **[Speechmatics docs](https://docs.speechmatics.com/introduction/)**: Speechmatics STT docs.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Speechmatics STT.

---

This document was rendered at 2025-11-19T10:44:02.806Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/speechmatics.md](https://docs.livekit.io/agents/models/stt/plugins/speechmatics.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Models â€º Speech-to-text (STT) â€º Plugins â€º Spitch

---

# Spitch STT plugin guide

> How to use the Spitch STT plugin for LiveKit Agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

This plugin allows you to use [Spitch](https://spitch.app/) as an STT provider for your voice agents.

## Quick reference

This section provides a quick reference for the Spitch STT plugin. For more information, see [Additional resources](#additional-resources).

### Installation

Install the plugin from PyPI:

```shell
uv add "livekit-agents[spitch]~=1.2"

```

### Authentication

The Spitch plugin requires a [Spitch API key](https://studio.spitch.app/api/keys).

Set `SPITCH_API_KEY` in your `.env` file.

### Usage

Use Spitch STT in an `AgentSession` or as a standalone transcription service. For example, you can use this STT in the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md).

```python
from livekit.plugins import spitch

session = AgentSession(
   stt=spitch.STT(
      language="en",
   ),
   # ... llm, tts, etc.
)

```

### Parameters

This section describes some of the available parameters. See the [plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/spitch/index.html.md#livekit.plugins.spitch.STT) for a complete list of all available parameters.

- **`language`** _(string)_ (optional) - Default: `en`: Language short code of the input speech. For supported values, see [Spitch languages](https://docs.spitch.app/concepts/languages).

## Additional resources

The following resources provide more information about using Spitch with LiveKit Agents.

- **[Python package](https://pypi.org/project/livekit-plugins-spitch/)**: The `livekit-plugins-spitch` package on PyPI.

- **[Plugin reference](https://docs.livekit.io/reference/python/v1/livekit/plugins/spitch/index.html.md#livekit.plugins.spitch.STT)**: Reference for the Spitch STT plugin.

- **[GitHub repo](https://github.com/livekit/agents/tree/main/livekit-plugins/livekit-plugins-spitch)**: View the source or contribute to the LiveKit Spitch STT plugin.

- **[Spitch docs](https://docs.spitch.app/)**: Spitch's official documentation.

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Get started with LiveKit Agents and Spitch.

- **[Spitch TTS](https://docs.livekit.io/agents/models/tts/plugins/spitch.md)**: Guide to the Spitch TTS plugin with LiveKit Agents.

---

This document was rendered at 2025-11-19T10:44:08.814Z.
For the latest version of this document, see [https://docs.livekit.io/agents/models/stt/plugins/spitch.md](https://docs.livekit.io/agents/models/stt/plugins/spitch.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).