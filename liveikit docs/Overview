LiveKit docs â€º Building voice agents â€º Overview

---

# Building voice agents

> In-depth guide to voice AI with LiveKit Agents.

## Overview

Building a great voice AI app requires careful orchestration of multiple components. LiveKit Agents is built on top of the [Realtime SDK](https://github.com/livekit/python-sdks) to provide dedicated abstractions that simplify development while giving you full control over the underlying code.

## Voice AI providers

You can choose from a variety of providers for each part of the voice pipeline to fit your needs. The framework supports both high-performance STT-LLM-TTS pipelines and speech-to-speech models. In either case, the framework automatically manages interruptions, transcription forwarding, turn detection, and more.

You may add these components to the `AgentSession`, where they act as global defaults within the app, or to each individual `Agent` if needed.

- **[TTS](https://docs.livekit.io/agents/models/tts.md)**: Text-to-speech models

- **[STT](https://docs.livekit.io/agents/models/stt.md)**: Speech-to-text models

- **[LLM](https://docs.livekit.io/agents/models/llm.md)**: Language model models

- **[Realtime](https://docs.livekit.io/agents/models/realtime.md)**: Realtime models

## Capabilities

The following guides, in addition to others in this section, cover the core capabilities of the `AgentSession` and how to leverage them in your app.

- **[Workflows](https://docs.livekit.io/agents/build/workflows.md)**: Core constructs for building complex voice AI workflows.

- **[Agent sessions](https://docs.livekit.io/agents/build/sessions.md)**: An agent session orchestrates your voice AI app's lifecycle.

- **[Agents & handoffs](https://docs.livekit.io/agents/build/agents-handoffs.md)**: Define agents and agent handoffs to build multi-agent voice AI workflows.

- **[Tool definition & use](https://docs.livekit.io/agents/build/tools.md)**: Use tools to call external services, inject custom logic, and more.

- **[Tasks & task groups](https://docs.livekit.io/agents/build/tasks.md)**: Use tasks and task groups to execute discrete operations and build complex workflows.

- **[Pipeline nodes](https://docs.livekit.io/agents/build/nodes.md)**: Add custom behavior to any component of the voice pipeline.

---

This document was rendered at 2025-11-19T10:56:32.583Z.
For the latest version of this document, see [https://docs.livekit.io/agents/build.md](https://docs.livekit.io/agents/build.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Building voice agents â€º Overview

---

# Building voice agents

> In-depth guide to voice AI with LiveKit Agents.

## Overview

Building a great voice AI app requires careful orchestration of multiple components. LiveKit Agents is built on top of the [Realtime SDK](https://github.com/livekit/python-sdks) to provide dedicated abstractions that simplify development while giving you full control over the underlying code.

## Voice AI providers

You can choose from a variety of providers for each part of the voice pipeline to fit your needs. The framework supports both high-performance STT-LLM-TTS pipelines and speech-to-speech models. In either case, the framework automatically manages interruptions, transcription forwarding, turn detection, and more.

You may add these components to the `AgentSession`, where they act as global defaults within the app, or to each individual `Agent` if needed.

- **[TTS](https://docs.livekit.io/agents/models/tts.md)**: Text-to-speech models

- **[STT](https://docs.livekit.io/agents/models/stt.md)**: Speech-to-text models

- **[LLM](https://docs.livekit.io/agents/models/llm.md)**: Language model models

- **[Realtime](https://docs.livekit.io/agents/models/realtime.md)**: Realtime models

## Capabilities

The following guides, in addition to others in this section, cover the core capabilities of the `AgentSession` and how to leverage them in your app.

- **[Workflows](https://docs.livekit.io/agents/build/workflows.md)**: Core constructs for building complex voice AI workflows.

- **[Agent sessions](https://docs.livekit.io/agents/build/sessions.md)**: An agent session orchestrates your voice AI app's lifecycle.

- **[Agents & handoffs](https://docs.livekit.io/agents/build/agents-handoffs.md)**: Define agents and agent handoffs to build multi-agent voice AI workflows.

- **[Tool definition & use](https://docs.livekit.io/agents/build/tools.md)**: Use tools to call external services, inject custom logic, and more.

- **[Tasks & task groups](https://docs.livekit.io/agents/build/tasks.md)**: Use tasks and task groups to execute discrete operations and build complex workflows.

- **[Pipeline nodes](https://docs.livekit.io/agents/build/nodes.md)**: Add custom behavior to any component of the voice pipeline.

---

This document was rendered at 2025-11-19T10:56:32.583Z.
For the latest version of this document, see [https://docs.livekit.io/agents/build.md](https://docs.livekit.io/agents/build.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Building voice agents â€º Agent sessions

---

# Agent session

> How to use AgentSession to orchestrate your voice AI app.

## Overview

The `AgentSession` is the main orchestrator for your voice AI app. The session is responsible for collecting user input, managing the voice pipeline, invoking the LLM, sending the output back to the user, and emits events for observability and control.

Each session requires at least one `Agent` to orchestrate. The agent is responsible for defining the core AI logic - instructions, tools, etc - of your app. The framework supports the design of custom [workflows](https://docs.livekit.io/agents/build/workflows.md) to orchestrate handoff and delegation between multiple agents.

The following example shows how to begin a simple single-agent session:

**Python**:

```python
from livekit.agents import AgentSession, Agent, RoomInputOptions, inference
from livekit.plugins import noise_cancellation, silero
from livekit.plugins.turn_detector.multilingual import MultilingualModel

session = AgentSession(
    stt="assemblyai/universal-streaming:en",
    llm="openai/gpt-4.1-mini",
    tts="cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc",
    vad=silero.VAD.load(),
    turn_detection=MultilingualModel(),
)

await session.start(
    room=ctx.room,
    agent=Agent(instructions="You are a helpful voice AI assistant."),
    room_input_options=RoomInputOptions(
        noise_cancellation=noise_cancellation.BVC(),
    ),
)

```

---

**Node.js**:

```ts
import { voice, inference } from '@livekit/agents';
import * as livekit from '@livekit/agents-plugin-livekit';
import * as silero from '@livekit/agents-plugin-silero';
import { BackgroundVoiceCancellation } from '@livekit/noise-cancellation-node';

const vad = await silero.VAD.load();

const session = new voice.AgentSession({
  vad,
  stt: "assemblyai/universal-streaming:en",
  llm: "openai/gpt-4.1-mini",
  tts: "cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc",
  turnDetection: new livekit.turnDetector.MultilingualModel(),
});

await session.start({
  room: ctx.room,
  agent: new voice.Agent({
    instructions: "You are a helpful voice AI assistant.",
  }),
  inputOptions: {
    noiseCancellation: BackgroundVoiceCancellation(),
  },
});

```

## Lifecycle

An `AgentSession` progresses through several distinct phases during its operation:

- **Initializing**: The session is setting up. During initialization, no audio or video processing occurs yet. Agent state is set to `initializing`.
- **Starting**: The session is started using the `start()` method. It sets up I/O connections, initializes agent activity tracking, and begins forwarding audio and video frames. In this phase, the agent is transitioned into the `listening` state.
- **Running**: The session is actively processing user input and generating agent responses. During this phase, your agent controls the session and can transfer control to other agents. In this phase, the agent transitions between `listening`, `thinking`, and `speaking` states.
- **Closing**: When a session is closed, the cleanup process includes gracefully draining pending speech (if requested), waiting for any queued operations to complete, committing any remaining user transcripts, and closing all I/O connections. The session emits a `close` event and resets internal state.

The following diagram shows the lifecycle of an `AgentSession` using agent states:

```mermaid
stateDiagram-v2
initializing --> listening : session started
listening --> thinking : user input received
thinking --> speaking : response generated
speaking --> listening : response complete
speaking --> listening : interrupted
listening --> initializing : session shutdown requested and states resetnote right of initializing
Session setup in progress
(no media I/O yet)
end notenote right of speaking
Agent outputs synthesized
audio response
end note
```

You can monitor agent state changes via the [`agent_state_changed` event](https://docs.livekit.io/agents/build/events.md#agent_state_changed).

## Events

`AgentSession` emits events throughout its lifecycle to provide visibility into the conversation flow. For more information, select the event name to see the properties and example code.

| **Event** | **Description** |
| [`agent_state_changed`](https://docs.livekit.io/agents/build/events.md#agent_state_changed) | Emitted when the agent's state changes (for example, from `listening` to `thinking` or `speaking`). |
| [`user_state_changed`](https://docs.livekit.io/agents/build/events.md#user_state_changed) | Emitted when the user's state changes (for example, from `listening` to `speaking`). |
| [`user_input_transcribed`](https://docs.livekit.io/agents/build/events.md#user_input_transcribed) | Emitted when user speech is transcribed to text. |
| [`conversation_item_added`](https://docs.livekit.io/agents/build/events.md#conversation_item_added) | Emitted when a message is added to the conversation history. |
| [`close`](https://docs.livekit.io/agents/build/events.md#close) | Emitted when the session closes, either gracefully or due to an error. |

## Session options

The `AgentSession` constructor accepts numerous options to configure behavior. The following sections describe the available options grouped by category.

### AI models

Configure the default speech and language models for your agent session. You can override these models for specific agents or tasks. To learn more about models, see the [models](https://docs.livekit.io/agents/models.md) topic.

### Turn detection & interruptions

Turn detection and interruptions are critical for managing conversation flow. The session provides several options to configure this behavior. For more information, see [Session configuration](https://docs.livekit.io/agents/build/turns.md#session-configuration).

### Tools and capabilities

Extend agent capabilities with [tools](https://docs.livekit.io/agents/build/tools.md):

- `tools`: List of `FunctionTool` or `RawFunctionTool` objects shared by all agents in the session.
- `mcp_servers`: List of MCP (Model Context Protocol) servers providing external tools.
- `max_tool_steps`: Maximum consecutive tool calls per LLM turn. Default: `3`.
- `ivr_detection`: Whether to detect if the agent is interacting with an Interactive Voice Response (IVR) system. Default: `False`. To learn more, see [DTMF](https://docs.livekit.io/sip/dtmf.md).

### User interaction

Configure user state and timing:

- `user_away_timeout`: Time in seconds of silence before setting user state to `away`. Set to `None` to turn off. Default: `15.0` seconds.
- `min_consecutive_speech_delay`: Minimum delay in seconds between consecutive agent utterances. Default: `0.0` seconds.

### Text processing

Control how [text](https://docs.livekit.io/agents/build/text.md) is processed:

- `tts_text_transforms`: Transforms to apply to TTS input text. Built-in transforms include `"filter_markdown"` and `"filter_emoji"`. Set to `None` to turn off. When not given, all filters are applied by default.
- `use_tts_aligned_transcript`: Whether to use TTS-aligned transcript as input for the transcription node. Only applies if the TTS supports aligned transcripts. Default: turned off.

### Performance optimization

Optimize response latency:

[`preemptive_generation`](https://docs.livekit.io/agents/build/audio.md#preemptive-generation): Whether to speculatively begin LLM and TTS requests before an end-of-turn is detected. When `True`, the agent sends inference calls as soon as a user transcript is received. This can reduce response latency but can incur extra compute costs if the user interrupts. Default: `False`.

### Video sampling

Control video frame processing:

`video_sampler`: Custom video sampler function or `None`. When not given, uses `VoiceActivityVideoSampler` which captures at ~1 fps while speaking and ~0.3 fps when silent. To learn more, see [Video](https://docs.livekit.io/agents/build/vision.md).

### Other options

`userdata`: Arbitrary per-session user data accessible via `session.userdata`. To learn more, see [Passing state](https://docs.livekit.io/agents/build/agents-handoffs.md#passing-state).

## rtc_session options

The following optional parameters are available when you define your entrypoint function using the `rtc_session` decorator:

- `agent_name`: Name of agent for agent disaptch. If this is set, the agent must be explicitly dispatched to a room. To learn more, see [Agent dispatch](https://docs.livekit.io/agents/server/agent-dispatch.md).
- `type`: Agent server type determines when a new instance of the agent is created: for each room or for each publisher in a room. To learn more, see [Agent server type](https://docs.livekit.io/agents/server/options.md#agent-server-type).
- `on_session_end`: Callback function to be called when the session ends. To learn more, see [Session reports](https://docs.livekit.io/agents/observability/data.md#session-reports).
- `on_request`: Callback function to be called when a new request is received. To learn more see [Request handler](https://docs.livekit.io/agents/server/options.md#request-handler).

## RoomIO

Communication between agent and user participants happens using media streams, also known as tracks. For voice AI apps, this is primarily audio, but can include vision. By default, track management is handled by `RoomIO`, a utility class that serves as a bridge between the agent session and the LiveKit room. When an AgentSession is initiated, it automatically creates a `RoomIO` object that enables all room participants to subscribe to available audio tracks.

When starting an `AgentSession`, you can configure how the session interacts with the LiveKit room by passing `room_input_options` and `room_output_options` to the `start()` method. These options control media track management, participant linking, and I/O behavior.

### Room input options

Configure how the agent receives input from room participants:

#### Media configuration

- `audio_enabled`: Whether to receive audio from the room. Default: `True`.
- [`video_enabled`](https://docs.livekit.io/agents/build/vision.md#video): Whether to receive video from the room. Default: `False`.
- [`text_enabled`](https://docs.livekit.io/agents/build/text.md#text-input): Whether to receive text input from the room. Default: `True`.
- `audio_sample_rate`: Audio sample rate in Hz. Default: `24000`.
- `audio_num_channels`: Number of audio channels. Default: `1`.
- `noise_cancellation`: [Noise cancellation](https://docs.livekit.io/home/cloud/noise-cancellation.md) options to apply to incoming audio.

#### Connection behavior

- `close_on_disconnect`: Whether to close the `AgentSession` if the linked participant disconnects. Default: `True`.
- `delete_room_on_close`: Whether to delete the room when the `AgentSession` closes. Default: `True`.

#### Callbacks

`text_input_cb`: Callback function to handle text input from participants. To learn more, see [Custom handling](https://docs.livekit.io/agents/build/text.md#custom-handling) of text input.

### Room output options

Configure how the agent publishes output to the room:

#### Media configuration

- `audio_enabled`: Whether to publish audio to the room. Default: `True`.
- `transcription_enabled`: Whether to publish transcription to the room. Default: `True`.
- `audio_sample_rate`: Audio sample rate in Hz. Default: `24000`.
- `audio_num_channels`: Number of audio channels. Default: `1`.

#### Audio publishing

- `audio_track_name`: The name of the audio track to publish. Default: `"roomio_audio"`.
- `audio_publish_options`: Track publish options for the audio track (source, encryption, etc.).

#### Transcription synchronization

- `sync_transcription`: Whether to synchronize transcription with audio output. When `True`, transcription is emitted in sync with the audio. When `False`, transcription is emitted as quickly as available. Default: `True`.
- `transcription_speed_factor`: Speed factor for transcription synchronization (only effective if `sync_transcription` is `True`). Default: `1.0`.

### Example usage

```python
from livekit.agents import voice, RoomInputOptions, RoomOutputOptions
from livekit.plugins import noise_cancellation

input_options = RoomInputOptions(
    video_enabled=True,
    noise_cancellation=noise_cancellation.BVC(),
    participant_identity="user_123",
)

output_options = RoomOutputOptions(
    sync_transcription=False,
    audio_track_name="agent_voice",
)

await session.start(
    agent=my_agent,
    room=room,
    room_input_options=input_options,
    room_output_options=output_options,
)

```

To learn more about publishing audio and video, see the following topics:

- **[Agent speech and audio](https://docs.livekit.io/agents/build/audio.md)**: Add speech, audio, and background audio to your agent.

- **[Vision](https://docs.livekit.io/agents/build/vision.md)**: Give your agent the ability to see images and live video.

- **[Text and transcription](https://docs.livekit.io/agents/build/text.md)**: Send and receive text messages and transcription to and from your agent.

- **[Realtime media](https://docs.livekit.io/home/client/tracks.md)**: Tracks are a core LiveKit concept. Learn more about publishing and subscribing to media.

- **[Camera and microphone](https://docs.livekit.io/home/client/tracks/publish.md)**: Use the LiveKit SDKs to publish audio and video tracks from your user's device.

### Custom RoomIO

For greater control over media sharing in a room,  you can create a custom `RoomIO` object. For example, you might want to manually control which input and output devices are used, or to control which participants an agent listens to or responds to.

To replace the default one created in `AgentSession`, create a `RoomIO` object in your entrypoint function and pass it an instance of the `AgentSession` in the constructor. For examples, see the following in the repository:

- **[Toggling audio](https://github.com/livekit/agents/blob/main/examples/voice_agents/push_to_talk.py)**: Create a push-to-talk interface to toggle audio input and output.

- **[Toggling input and output](https://github.com/livekit/agents/blob/main/examples/voice_agents/toggle_io.py)**: Toggle both audio and text input and output.

---

This document was rendered at 2025-11-19T10:56:43.668Z.
For the latest version of this document, see [https://docs.livekit.io/agents/build/sessions.md](https://docs.livekit.io/agents/build/sessions.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Building voice agents â€º Agent sessions

---

# Agent session

> How to use AgentSession to orchestrate your voice AI app.

## Overview

The `AgentSession` is the main orchestrator for your voice AI app. The session is responsible for collecting user input, managing the voice pipeline, invoking the LLM, sending the output back to the user, and emits events for observability and control.

Each session requires at least one `Agent` to orchestrate. The agent is responsible for defining the core AI logic - instructions, tools, etc - of your app. The framework supports the design of custom [workflows](https://docs.livekit.io/agents/build/workflows.md) to orchestrate handoff and delegation between multiple agents.

The following example shows how to begin a simple single-agent session:

**Python**:

```python
from livekit.agents import AgentSession, Agent, RoomInputOptions, inference
from livekit.plugins import noise_cancellation, silero
from livekit.plugins.turn_detector.multilingual import MultilingualModel

session = AgentSession(
    stt="assemblyai/universal-streaming:en",
    llm="openai/gpt-4.1-mini",
    tts="cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc",
    vad=silero.VAD.load(),
    turn_detection=MultilingualModel(),
)

await session.start(
    room=ctx.room,
    agent=Agent(instructions="You are a helpful voice AI assistant."),
    room_input_options=RoomInputOptions(
        noise_cancellation=noise_cancellation.BVC(),
    ),
)

```

---

**Node.js**:

```ts
import { voice, inference } from '@livekit/agents';
import * as livekit from '@livekit/agents-plugin-livekit';
import * as silero from '@livekit/agents-plugin-silero';
import { BackgroundVoiceCancellation } from '@livekit/noise-cancellation-node';

const vad = await silero.VAD.load();

const session = new voice.AgentSession({
  vad,
  stt: "assemblyai/universal-streaming:en",
  llm: "openai/gpt-4.1-mini",
  tts: "cartesia/sonic-3:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc",
  turnDetection: new livekit.turnDetector.MultilingualModel(),
});

await session.start({
  room: ctx.room,
  agent: new voice.Agent({
    instructions: "You are a helpful voice AI assistant.",
  }),
  inputOptions: {
    noiseCancellation: BackgroundVoiceCancellation(),
  },
});

```

## Lifecycle

An `AgentSession` progresses through several distinct phases during its operation:

- **Initializing**: The session is setting up. During initialization, no audio or video processing occurs yet. Agent state is set to `initializing`.
- **Starting**: The session is started using the `start()` method. It sets up I/O connections, initializes agent activity tracking, and begins forwarding audio and video frames. In this phase, the agent is transitioned into the `listening` state.
- **Running**: The session is actively processing user input and generating agent responses. During this phase, your agent controls the session and can transfer control to other agents. In this phase, the agent transitions between `listening`, `thinking`, and `speaking` states.
- **Closing**: When a session is closed, the cleanup process includes gracefully draining pending speech (if requested), waiting for any queued operations to complete, committing any remaining user transcripts, and closing all I/O connections. The session emits a `close` event and resets internal state.

The following diagram shows the lifecycle of an `AgentSession` using agent states:

```mermaid
stateDiagram-v2
initializing --> listening : session started
listening --> thinking : user input received
thinking --> speaking : response generated
speaking --> listening : response complete
speaking --> listening : interrupted
listening --> initializing : session shutdown requested and states resetnote right of initializing
Session setup in progress
(no media I/O yet)
end notenote right of speaking
Agent outputs synthesized
audio response
end note
```

You can monitor agent state changes via the [`agent_state_changed` event](https://docs.livekit.io/agents/build/events.md#agent_state_changed).

## Events

`AgentSession` emits events throughout its lifecycle to provide visibility into the conversation flow. For more information, select the event name to see the properties and example code.

| **Event** | **Description** |
| [`agent_state_changed`](https://docs.livekit.io/agents/build/events.md#agent_state_changed) | Emitted when the agent's state changes (for example, from `listening` to `thinking` or `speaking`). |
| [`user_state_changed`](https://docs.livekit.io/agents/build/events.md#user_state_changed) | Emitted when the user's state changes (for example, from `listening` to `speaking`). |
| [`user_input_transcribed`](https://docs.livekit.io/agents/build/events.md#user_input_transcribed) | Emitted when user speech is transcribed to text. |
| [`conversation_item_added`](https://docs.livekit.io/agents/build/events.md#conversation_item_added) | Emitted when a message is added to the conversation history. |
| [`close`](https://docs.livekit.io/agents/build/events.md#close) | Emitted when the session closes, either gracefully or due to an error. |

## Session options

The `AgentSession` constructor accepts numerous options to configure behavior. The following sections describe the available options grouped by category.

### AI models

Configure the default speech and language models for your agent session. You can override these models for specific agents or tasks. To learn more about models, see the [models](https://docs.livekit.io/agents/models.md) topic.

### Turn detection & interruptions

Turn detection and interruptions are critical for managing conversation flow. The session provides several options to configure this behavior. For more information, see [Session configuration](https://docs.livekit.io/agents/build/turns.md#session-configuration).

### Tools and capabilities

Extend agent capabilities with [tools](https://docs.livekit.io/agents/build/tools.md):

- `tools`: List of `FunctionTool` or `RawFunctionTool` objects shared by all agents in the session.
- `mcp_servers`: List of MCP (Model Context Protocol) servers providing external tools.
- `max_tool_steps`: Maximum consecutive tool calls per LLM turn. Default: `3`.
- `ivr_detection`: Whether to detect if the agent is interacting with an Interactive Voice Response (IVR) system. Default: `False`. To learn more, see [DTMF](https://docs.livekit.io/sip/dtmf.md).

### User interaction

Configure user state and timing:

- `user_away_timeout`: Time in seconds of silence before setting user state to `away`. Set to `None` to turn off. Default: `15.0` seconds.
- `min_consecutive_speech_delay`: Minimum delay in seconds between consecutive agent utterances. Default: `0.0` seconds.

### Text processing

Control how [text](https://docs.livekit.io/agents/build/text.md) is processed:

- `tts_text_transforms`: Transforms to apply to TTS input text. Built-in transforms include `"filter_markdown"` and `"filter_emoji"`. Set to `None` to turn off. When not given, all filters are applied by default.
- `use_tts_aligned_transcript`: Whether to use TTS-aligned transcript as input for the transcription node. Only applies if the TTS supports aligned transcripts. Default: turned off.

### Performance optimization

Optimize response latency:

[`preemptive_generation`](https://docs.livekit.io/agents/build/audio.md#preemptive-generation): Whether to speculatively begin LLM and TTS requests before an end-of-turn is detected. When `True`, the agent sends inference calls as soon as a user transcript is received. This can reduce response latency but can incur extra compute costs if the user interrupts. Default: `False`.

### Video sampling

Control video frame processing:

`video_sampler`: Custom video sampler function or `None`. When not given, uses `VoiceActivityVideoSampler` which captures at ~1 fps while speaking and ~0.3 fps when silent. To learn more, see [Video](https://docs.livekit.io/agents/build/vision.md).

### Other options

`userdata`: Arbitrary per-session user data accessible via `session.userdata`. To learn more, see [Passing state](https://docs.livekit.io/agents/build/agents-handoffs.md#passing-state).

## rtc_session options

The following optional parameters are available when you define your entrypoint function using the `rtc_session` decorator:

- `agent_name`: Name of agent for agent disaptch. If this is set, the agent must be explicitly dispatched to a room. To learn more, see [Agent dispatch](https://docs.livekit.io/agents/server/agent-dispatch.md).
- `type`: Agent server type determines when a new instance of the agent is created: for each room or for each publisher in a room. To learn more, see [Agent server type](https://docs.livekit.io/agents/server/options.md#agent-server-type).
- `on_session_end`: Callback function to be called when the session ends. To learn more, see [Session reports](https://docs.livekit.io/agents/observability/data.md#session-reports).
- `on_request`: Callback function to be called when a new request is received. To learn more see [Request handler](https://docs.livekit.io/agents/server/options.md#request-handler).

## RoomIO

Communication between agent and user participants happens using media streams, also known as tracks. For voice AI apps, this is primarily audio, but can include vision. By default, track management is handled by `RoomIO`, a utility class that serves as a bridge between the agent session and the LiveKit room. When an AgentSession is initiated, it automatically creates a `RoomIO` object that enables all room participants to subscribe to available audio tracks.

When starting an `AgentSession`, you can configure how the session interacts with the LiveKit room by passing `room_input_options` and `room_output_options` to the `start()` method. These options control media track management, participant linking, and I/O behavior.

### Room input options

Configure how the agent receives input from room participants:

#### Media configuration

- `audio_enabled`: Whether to receive audio from the room. Default: `True`.
- [`video_enabled`](https://docs.livekit.io/agents/build/vision.md#video): Whether to receive video from the room. Default: `False`.
- [`text_enabled`](https://docs.livekit.io/agents/build/text.md#text-input): Whether to receive text input from the room. Default: `True`.
- `audio_sample_rate`: Audio sample rate in Hz. Default: `24000`.
- `audio_num_channels`: Number of audio channels. Default: `1`.
- `noise_cancellation`: [Noise cancellation](https://docs.livekit.io/home/cloud/noise-cancellation.md) options to apply to incoming audio.

#### Connection behavior

- `close_on_disconnect`: Whether to close the `AgentSession` if the linked participant disconnects. Default: `True`.
- `delete_room_on_close`: Whether to delete the room when the `AgentSession` closes. Default: `True`.

#### Callbacks

`text_input_cb`: Callback function to handle text input from participants. To learn more, see [Custom handling](https://docs.livekit.io/agents/build/text.md#custom-handling) of text input.

### Room output options

Configure how the agent publishes output to the room:

#### Media configuration

- `audio_enabled`: Whether to publish audio to the room. Default: `True`.
- `transcription_enabled`: Whether to publish transcription to the room. Default: `True`.
- `audio_sample_rate`: Audio sample rate in Hz. Default: `24000`.
- `audio_num_channels`: Number of audio channels. Default: `1`.

#### Audio publishing

- `audio_track_name`: The name of the audio track to publish. Default: `"roomio_audio"`.
- `audio_publish_options`: Track publish options for the audio track (source, encryption, etc.).

#### Transcription synchronization

- `sync_transcription`: Whether to synchronize transcription with audio output. When `True`, transcription is emitted in sync with the audio. When `False`, transcription is emitted as quickly as available. Default: `True`.
- `transcription_speed_factor`: Speed factor for transcription synchronization (only effective if `sync_transcription` is `True`). Default: `1.0`.

### Example usage

```python
from livekit.agents import voice, RoomInputOptions, RoomOutputOptions
from livekit.plugins import noise_cancellation

input_options = RoomInputOptions(
    video_enabled=True,
    noise_cancellation=noise_cancellation.BVC(),
    participant_identity="user_123",
)

output_options = RoomOutputOptions(
    sync_transcription=False,
    audio_track_name="agent_voice",
)

await session.start(
    agent=my_agent,
    room=room,
    room_input_options=input_options,
    room_output_options=output_options,
)

```

To learn more about publishing audio and video, see the following topics:

- **[Agent speech and audio](https://docs.livekit.io/agents/build/audio.md)**: Add speech, audio, and background audio to your agent.

- **[Vision](https://docs.livekit.io/agents/build/vision.md)**: Give your agent the ability to see images and live video.

- **[Text and transcription](https://docs.livekit.io/agents/build/text.md)**: Send and receive text messages and transcription to and from your agent.

- **[Realtime media](https://docs.livekit.io/home/client/tracks.md)**: Tracks are a core LiveKit concept. Learn more about publishing and subscribing to media.

- **[Camera and microphone](https://docs.livekit.io/home/client/tracks/publish.md)**: Use the LiveKit SDKs to publish audio and video tracks from your user's device.

### Custom RoomIO

For greater control over media sharing in a room,  you can create a custom `RoomIO` object. For example, you might want to manually control which input and output devices are used, or to control which participants an agent listens to or responds to.

To replace the default one created in `AgentSession`, create a `RoomIO` object in your entrypoint function and pass it an instance of the `AgentSession` in the constructor. For examples, see the following in the repository:

- **[Toggling audio](https://github.com/livekit/agents/blob/main/examples/voice_agents/push_to_talk.py)**: Create a push-to-talk interface to toggle audio input and output.

- **[Toggling input and output](https://github.com/livekit/agents/blob/main/examples/voice_agents/toggle_io.py)**: Toggle both audio and text input and output.

---

This document was rendered at 2025-11-19T10:56:43.668Z.
For the latest version of this document, see [https://docs.livekit.io/agents/build/sessions.md](https://docs.livekit.io/agents/build/sessions.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs â€º Building voice agents â€º Tasks & task groups

---

# Tasks and task groups

> Use tasks to build complex workflows for your voice AI agents.

Available in:
- [ ] Node.js
- [x] Python

## Overview

Tasks are focused, reusable units that perform a specific objective and return a typed result. They run inside an agent and take control of the session only until their goal is achieved. A task can define its own [tools](https://docs.livekit.io/agents/build/tools.md) and starts executing when it's created within the context of an agent.

For multi-step flows, the framework provides `TaskGroup`. A task group executes an ordered sequence of tasks while allowing users to return to earlier steps for corrections. All tasks in a group share conversation context, and when the group finishes, a summarized result is returned to the agent that started it.

Tasks and task groups are core building blocks for complex voice AI [workflows](https://docs.livekit.io/agents/build/workflows.md). Common use cases for tasks include:

- Obtaining recording consent at the start of a call.
- Collecting structured information such as an address or payment details.
- Walking through a series of questions one step at a time.
- Any discrete action that should complete and yield control.
- Any multi-step process that can be decomposed into ordered tasks.

## Defining a task

Define a task by extending the `AgentTask` class and specifying a result type using [generics](https://typing.python.org/en/latest/reference/generics.html). Use the `on_enter` method to begin the task's interaction with the user, and call the `complete` method with a result when complete. The task has full support for tools, similar to an agent.

```python
from livekit.agents import AgentTask, function_tool

class CollectConsent(AgentTask[bool]):
    def __init__(self, chat_ctx=None):
        super().__init__(
            instructions="""
            Ask for recording consent and get a clear yes or no answer.
            Be polite and professional.
            """,
            chat_ctx=chat_ctx,
        )

    async def on_enter(self) -> None:
        await self.session.generate_reply(
            instructions="""
            Briefly introduce yourself, then ask for permission to record the call for quality assurance and training purposes.
            Make it clear that they can decline.
            """
        )

    @function_tool
    async def consent_given(self) -> None:
        """Use this when the user gives consent to record."""
        self.complete(True)

    @function_tool
    async def consent_denied(self) -> None:
        """Use this when the user denies consent to record."""
        self.complete(False)

```

### Running a task

A task must be created within the context of an [active](https://docs.livekit.io/agents/build/agents-handoffs.md#active-agent) `Agent`, and runs automatically when it's created. The task takes control of the session until it returns a result. Await the task to receive its result.

```python
from livekit.agents import Agent, function_tool, get_job_context

class CustomerServiceAgent(Agent):
    def __init__(self):
        super().__init__(instructions="You are a friendly customer service representative.")

    async def on_enter(self) -> None:
        if await CollectConsent(chat_ctx=self.chat_ctx):
            await self.session.generate_reply(instructions="Offer your assistance to the user.")
        else:
            await self.session.generate_reply(instructions="Inform the user that you are unable to proceed and will end the call.")
            job_ctx = get_job_context()
            await job_ctx.api.room.delete_room(api.DeleteRoomRequest(room=job_ctx.room.name))

```

### Task results

Use any result type you want. For complex results, use a custom dataclass.

```python
from dataclasses import dataclass

@dataclass
class ContactInfoResult:
    name: str
    email_address: str
    phone_number: str

class GetContactInfoTask(AgentTask[ContactInfoResult]):
    # ....

```

### Unordered collection within tasks

You can use a single task to collect multiple pieces of information in any order. The following example collects strengths, weaknesses, and work style in a hypothetical interview. Candidates can answer the questions in any order:

```python
@dataclass
class BehavioralResults:
    strengths: str
    weaknesses: str
    work_style: str

class BehavioralTask(AgentTask[BehavioralResults]):
    def __init__(self) -> None:
        super().__init__(
            instructions="Collect strengths, weaknesses, and work style in any order."
        )
        self._results = {}
    
    @function_tool()
    async def record_strengths(self, strengths_summary: str):
        """Record candidate's strengths"""
        self._results["strengths"] = strengths_summary
        self._check_completion()
    
    @function_tool()
    async def record_weaknesses(self, weaknesses_summary: str):
        """Record candidate's weaknesses"""
        self._results["weaknesses"] = weaknesses_summary
        self._check_completion()
    
    @function_tool()
    async def record_work_style(self, work_style: str):
        """Record candidate's work style"""
        self._results["work_style"] = work_style
        self._check_completion()
    
    def _check_completion(self):
        required_keys = {"strengths", "weaknesses", "work_style"}
        if self._results.keys() == required_keys:
            results = BehavioralResults(
                strengths=self._results["strengths"],
                weaknesses=self._results["weaknesses"],
                work_style=self._results["work_style"]
            )
            self.complete(results)
        else:
            self.session.generate_reply(
                instructions="Continue collecting remaining information."
            )

```

## Prebuilt tasks

Available in (BETA):
- [ ] Node.js
- [x] Python

The framework includes prebuilt tasks for common use cases within the module [livekit.agents.beta.workflows](https://docs.livekit.io/reference/python/v1/livekit/agents/beta/workflows/index.html.md). These include the following tasks:

- [GetEmailTask](#getemailtask)
- [GetAddressTask](#getaddresstask)
- [GetDtmfTask](#getdtmftask)

### Customizing prebuilt tasks

You can customize the behavior of prebuilt tasks by passing in extra instructions for the LLM. Use the `extra_instructions` parameter when you create the task. For an example, see the [Customize GetEmailTask](#customize-getemailtask) section.

### GetEmailTask

Use `GetEmailTask` to reliably collect and validate an email address from the user.

```python
from livekit.agents.beta.workflows import GetEmailTask

# ... within your agent ...
email_result = await GetEmailTask(chat_ctx=self.chat_ctx)
print(f"Collected email: {email_result.email_address}")

```

#### Customize GetEmailTask

In addition to the `extra_instructions` parameter, `GetEmailTask` also accepts a list of additional tools to use. Customize the behavior of this specific task by passing additional parameters:

- `extra_instructions`: Additional instructions for the LLM
- `tools`: Additional tools to use

By default `GetEmailTask` calls its `decline_email_capture()` tool when the user doesn't provide an email address. The following example customizes the task to instead collect alternative contact information by passing extra instructions and an alternate tool:

```python
from livekit.agents.beta.workflows import GetEmailTask
from livekit.agents import function_tool, RunContext
    
@function_tool()
async def get_alternate_contact_info(context: RunContext, contact_method: str, contact_value: str) -> None:
    """Collect alternative contact information when email isn't available"""
    # Store the alternative contact info
    context.session.userdata.alternate_contact_method = contact_method
    context.session.userdata.alternate_contact_value = contact_value
    
    await context.session.generate_reply(
        instructions=f"Acknowledge that you've recorded their {contact_method}: {contact_value}. Let them know this will be used for communication instead of email."
    )

# Customize GetEmailTask with extra instructions and tools
# ... within your agent ...
@function_tool()
async def collect_contact_info(context: RunContext) -> None:
    """Collect email or alternative contact information"""
    email_result = await GetEmailTask(
        chat_ctx=self.chat_ctx,
        extra_instructions="If the user cannot provide an email, call get_alternate_contact_info() instead of decline_email_capture().",
        tools=[get_alternate_contact_info]
    )

    return f"Collected email: {email_result.email_address}"

```

### GetAddressTask

Use `GetAddressTask` to collect and validate a complete mailing address from the user. The task supports international addresses and automatically normalizes spoken address formats.

It returns a `GetAddressResult` dataclass with one field: `address`.

#### Example

The following example uses `GetAddressTask` to collect a user's shipping address:

```python
from livekit.agents.beta.workflows import GetAddressTask
from livekit.agents import Agent, function_tool
    
@function_tool()
async def collect_shipping_address(self) -> str:
    """Collect the user's shipping address"""
    address_result = await GetAddressTask(
        chat_ctx=self.chat_ctx,
        extra_instructions="Emphasize that this is for shipping purposes and accuracy is important."
    )
    
    return f"Shipping address recorded: {address_result.address}"

```

### GetDtmfTask

Use `GetDtmfTask` to collect a series of keypad inputs from callers. The task can handle both Dual-tone multi-frequency (DTMF) tones and spoken digits. This is essential for Interactive Voice Response (IVR) systems and telephony apps. To learn more, see [Handling DTMF](https://docs.livekit.io/sip/dtmf.md).

The following example asks the caller to provide a 10-digit phone number and confirms the number with the caller:

**Python**:

```python
from livekit.agents.beta.workflows.dtmf_inputs import GetDtmfTask
from livekit.agents import function_tool, RunContext

@function_tool
async def ask_for_phone_number(self, context: RunContext) -> str:
    """Ask user to provide a phone number."""
    result = await GetDtmfTask(
        num_digits=10,
        chat_ctx=self.chat_ctx.copy(
            exclude_instructions=True, 
            exclude_function_call=True
        ),
        ask_for_confirmation=True,
        extra_instructions=(
            "Let the caller know you'll record their 10-digit phone number "
            "and that they can speak or dial it. Provide an example such as "
            "415 555 0199, then capture the digits."
        ),
    )
    
    return f"User's phone number is {result.user_input}"

```

#### Configuration options

The following parameters are supported for `GetDtmfTask`:

- `num_digits`: Number of digits to collect
- `ask_for_confirmation`: Whether to confirm inputs with the user
- `dtmf_input_timeout`: Timeout between digit inputs (default: 4.0 seconds)
- `dtmf_stop_event`: Event to stop collection (default: `#`)
- `extra_instructions`: Additional instructions for the LLM

#### Additional resources

The following additional resources provide more information about the topics discussed in this section:

- **[DTMF example](https://github.com/livekit/agents/blob/main/examples/dtmf/basic_dtmf_agent.py)**: A menu-based example that demonstrates using DTMF to collect user input.

- **[Handling DTMF](https://docs.livekit.io/sip/dtmf.md)**: Sending and receiving DTMF in LiveKit telephony apps.

## Task group

> ğŸ”¥ **Experimental feature**
> 
> `TaskGroup` is currently experimental and the API might change in a future release.

Task groups let you build complex, user-friendly workflows that mirror real conversational behaviorâ€”where users might need to revisit or correct earlier steps without losing context. They're designed as ordered, multi-step flows that can be broken into discrete tasks, with built-in regression support for safely moving backward.

`TaskGroup` supports task chaining, which allows tasks to call or re-enter other tasks dynamically while maintaining the overall flow order. This lets users return to earlier steps as often as needed. All tasks in the group share the same conversation context, and when the group finishes, the summarized context is passed back to the controlling agent.

### Basic usage

Initialize and set up a `TaskGroup` by adding tasks to it. Add tasks in the order they should be executed:

```python
from livekit.agents.beta.workflows import GetEmailTask, TaskGroup

# Create and configure TaskGroup
task_group = TaskGroup()

# Add tasks using lambda factories
task_group.add(
    lambda: GetEmailTask(), 
    id="get_email_task", 
    description="Collects the user's email"
)
task_group.add(
    lambda: GetCommuteTask(), 
    id="get_commute_task", 
    description="Records the user's commute flexibility"
)

# Execute the task group
results = await task_group  # Returns TaskGroupResult object
task_results = results.task_results

# Access results by task ID
print(task_results)
# Output: {
#   "get_email_task": GetEmailResult(email="john.doe@gmail.com"), 
#   "get_commute_task": CommuteResult(can_commute=True, commute_method="subway")
# }

```

The `TaskGroup.add()` method takes three parameters:

- `task_factory`: A callable that returns a task instance (typically a lambda function).
- `id`: A string identifier for the task used to access results.
- `description`: A string description that helps the LLM understand when to regress to this task.

The lambda function allows for tasks to be reinitialized with the same arguments when revisited. The task id and description are passed to the LLM as task identifiers when the LLM needs to regress to a previous task. This allows the LLM to understand the task's purpose and context when revisiting it. Task chaining is supported, allowing users to return to earlier steps as often as needed.

All tasks share the same conversation context. The context is summarized and passed back to the controlling agent when the group finishes. This option can be disabled by passing `summarize_chat_ctx=False` when initializing the task group:

```python
# Disable context summarization
task_group = TaskGroup(summarize_chat_ctx=False)

```

### Complete workflow example

The following is a complete example showing how to build an interview workflow with `TaskGroup`. It collects basic candidate information and then asks about their commute flexibility:

```python
from livekit.agents import AgentTask, function_tool, RunContext
from livekit.agents.beta.workflows import TaskGroup
from dataclasses import dataclass

@dataclass
class IntroResults:
    name: str
    intro: str

@dataclass 
class CommuteResults:
    can_commute: bool
    commute_method: str

class IntroTask(AgentTask[IntroResults]):
    def __init__(self) -> None:
        super().__init__(
            instructions="Welcome the candidate and collect their name and introduction."
        )
    
    async def on_enter(self) -> None:
        await self.session.generate_reply(
            instructions="Welcome the candidate and gather their name."
        )
    
    @function_tool()
    async def record_intro(self, context: RunContext, name: str, intro_notes: str) -> None:
        """Record the candidate's name and introduction"""
        context.session.userdata.candidate_name = name
        results = IntroResults(name=name, intro=intro_notes)
        self.complete(results)

class CommuteTask(AgentTask[CommuteResults]):
    def __init__(self) -> None:
        super().__init__(
            instructions="Ask about the candidate's ability to commute to the office."
        )
    
    @function_tool()
    async def record_commute_flexibility(
        self, 
        context: RunContext, 
        can_commute: bool, 
        commute_method: str
    ) -> None:
        """Record commute flexibility and transportation method"""
        results = CommuteResults(can_commute=can_commute, commute_method=commute_method)
        self.complete(results)

# Set up the workflow
task_group = TaskGroup()
task_group.add(
    lambda: IntroTask(), 
    id="intro_task", 
    description="Collects name and introduction"
)
task_group.add(
    lambda: CommuteTask(), 
    id="commute_task", 
    description="Asks about commute flexibility"
)

# Execute and get results
results = await task_group
task_results = results.task_results

```

## Additional resources

The following topics provider more information on creating complex workflows for your voice AI agents.

- **[Workflows](https://docs.livekit.io/agents/build/workflows.md)**: Complete guide to defining and using workflows in your agents.

- **[Tool definition and use](https://docs.livekit.io/agents/build/tools.md)**: Complete guide to defining and using tools in your agents.

- **[Nodes](https://docs.livekit.io/agents/build/nodes.md)**: Add custom behavior to any component of the voice pipeline.

- **[Testing & evaluation](https://docs.livekit.io/agents/build/testing.md)**: Test every aspect of your agents with a custom test suite.

---

This document was rendered at 2025-11-19T10:56:53.402Z.
For the latest version of this document, see [https://docs.livekit.io/agents/build/tasks.md](https://docs.livekit.io/agents/build/tasks.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

