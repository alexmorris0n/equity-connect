LiveKit docs › Agent observability › Insights in LiveKit Cloud

---

# Agent insights in LiveKit Cloud

> View transcripts, traces, logs, and audio recordings in LiveKit Cloud.

> ℹ️ **Beta feature**
> 
> Agent observability is currently in beta and free until the end of 2025.

## Overview

LiveKit Cloud includes a built-in observability stack optimized for voice agents. It includes transcripts, traces, and logs in a unified timeline with actual audio recordings for each of your agent sessions. This gives you access to comprehensive insights on your agent's behavior and user experience.

## Availability

Agent observability is available on all LiveKit Cloud plans, and works for agents deployed to LiveKit Cloud and those with custom deployments. For complete information on pricing, see the [LiveKit Cloud pricing page](https://livekit.io/pricing).

To enable agent observability, ensure the following conditions are met:

1. The **Agent observability** feature is enabled within the **Data and privacy** section in your [project's settings](https://cloud.livekit.io/projects/p_/settings/project).
2. Your agent uses the latest version of the LiveKit Agents SDK- Python SDK version 1.3.0 or higher
- Node.js SDK version 1.0.18 or higher
- Or the [LiveKit Agent Builder](https://docs.livekit.io/agents/start/builder.md)

Agent observability is found in the **Agent insights** tab in your [project's sessions dashboard](https://cloud.livekit.io/projects/p_/sessions).

## Observation events

The timeline for each agent session combines transcripts, traces, logs, audio clips, and the per-event metrics emitted by the LiveKit Agents SDK. Trace data streams in while the session runs, while transcripts and recordings are uploaded once the session wraps up.

### Transcripts

Turn-by-turn transcripts for the user and agent. Tool calls and handoffs also appear in the timeline so you can correlate them with traces and logs. Thes events are enriched with additional metadata and metrics in the detail pane of the timeline.

### Session traces and metrics

Traces capture the execution flow of a session, broken into spans for every stage of the voice pipeline. Each span is enriched with metrics—token counts, durations, speech identifiers, and more—that you can inspect in the **Details** panel of the LiveKit Cloud timeline.

Session traces include events including user and agent turns, STT-LLM-TTS pipeline steps, tool calls, and more. Each event is enriched with relevant metrics and other metadata, available in the detail pane of the timeline.

### Logs

Runtime logs from the agent server are uploaded to LiveKit Cloud and available in the session timeline. The logs are collected according to the [log level](https://docs.livekit.io/agents/server/options.md#log-levels) configured for your agent server.

## Audio recordings

Audio recordings are collected for each agent session, and are available for playback in the browser, as well as for download. They are collected locally, and uploaded to LiveKit Cloud after the session ends along with the transcripts. Recordings include both the agent and the user audio.

If [noise cancellation](https://docs.livekit.io/home/cloud/noise-cancellation.md) is enabled, user audio recording is collected after noise cancellation is applied. The recording reflects what the STT or realtime model heard.

## Retention window

All agent observability data is subject to a **30-day retention window**. Data older than 30 days is automatically deleted from LiveKit Cloud.

### Model improvement program

Projects on the free LiveKit Cloud **Build** plan are included in the LiveKit model improvement program. This means that some anonymized session data may be retained by LiveKit for longer than the 30-day retention window, for the purposes of improving models such as the [LiveKit turn detector](https://docs.livekit.io/agents/build/turns/turn-detector.md). Projects on paid plans, including **Ship**, **Scale**, and **Enterprise**, are not included in the program and their data is fully deleted after the 30-day retention window.

## Disabling at the session level

To turn off recording for a specific session, pass `record=False` to the `start` method of the `AgentSession`. This disables upload of audio, transcripts, traces, and logs for the entire session.

**Python**:

```python
await session.start(
    # ... agent, room_options, etc.
    record=False
)

```

---

**Node.js**:

```typescript
await session.start({ 
    // ... agent, roomOptions, etc.
    record: false,
});

```

> ℹ️ **Enabling at the session level**
> 
> Passing `True` to this parameter has no effect. If the feature is turned off at the project level, you cannot enable it from an individual session. When the feature is on at the project level, the default behavior is to record each session.

## Data download

Complete data for each session is available for download within the timeline view. Click the **Download data** button in the top right corner of the timeline view to download a zip file containing the audio recording, transcripts, traces, and logs for the session.

## Custom data collection

To collect observability data within your agent itself, for export to external systems or custom logging, see the [Custom data collection](https://docs.livekit.io/agents/observability/data.md) guide.

---

This document was rendered at 2025-11-19T10:53:09.789Z.
For the latest version of this document, see [https://docs.livekit.io/agents/observability.md](https://docs.livekit.io/agents/observability.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).

LiveKit docs › Agent observability › Data hooks

---

# Observability data hooks

> Collect session recordings, transcripts, metrics, and other data within the LiveKit Agents SDK.

## Overview

The LiveKit Agents SDK includes access to extensive detail about each session, which you can collect locally and integrate with other systems. For information about data collected in LiveKit Cloud, see the [Insights in LiveKit Cloud](https://docs.livekit.io/agents/observability.md) overview.

## Metrics and usage data

`AgentSession` emits a `metrics_collected` event whenever new metrics are available. You can log these events directly or forward them to external services.

### Subscribe to metrics events

**Python**:

```python
from livekit.agents import metrics, MetricsCollectedEvent

@session.on("metrics_collected")
def _on_metrics_collected(ev: MetricsCollectedEvent):
    metrics.log_metrics(ev.metrics)

```

---

**Node.js**:

```ts
import { voice, metrics } from '@livekit/agents';

session.on(voice.AgentSessionEventTypes.MetricsCollected, (ev) => {
  metrics.logMetrics(ev.metrics);
});

```

### Aggregate usage with `UsageCollector`

Use `UsageCollector` to accumulate LLM, TTS, and STT usage across a session for cost estimation or billing exports.

**Python**:

```python
from livekit.agents import metrics, MetricsCollectedEvent

usage_collector = metrics.UsageCollector()

@session.on("metrics_collected")
def _on_metrics_collected(ev: MetricsCollectedEvent):
    usage_collector.collect(ev.metrics)

async def log_usage():
    summary = usage_collector.get_summary()
    logger.info(f"Usage: {summary}")

ctx.add_shutdown_callback(log_usage)

```

---

**Node.js**:

```ts
import { voice, metrics } from '@livekit/agents';

const usageCollector = new metrics.UsageCollector();

session.on(voice.AgentSessionEventTypes.MetricsCollected, (ev) => {
  metrics.logMetrics(ev.metrics);
  usageCollector.collect(ev.metrics);
});

const logUsage = async () => {
  const summary = usageCollector.getSummary();
  console.log(`Usage: ${JSON.stringify(summary)}`);
};

ctx.addShutdownCallback(logUsage);

```

### Metrics reference

Each metrics event is included in the LiveKit Cloud trace spans and surfaced as JSON in the dashboard. Use the tables below when you emit the data elsewhere.

![Diagram where metrics are measured.](/images/agents/agents-capturing-metrics-v1.svg)

#### Speech-to-text (STT)

`STTMetrics` is emitted after the STT model processes the audio input. This metrics event is only available when an STT component is configured (Realtime APIs do not emit it).

| Metric | Description |
| `audio_duration` | The duration (seconds) of the audio input received by the STT model. |
| `duration` | For non-streaming STT, the amount of time (seconds) it took to create the transcript. Always `0` for streaming STT. |
| `streamed` | `True` if the STT is in streaming mode. |

#### LLM

`LLMMetrics` is emitted after each LLM inference completes. Tool calls that run after the initial completion emit their own `LLMMetrics` events.

| Metric | Description |
| `duration` | The amount of time (seconds) it took for the LLM to generate the entire completion. |
| `completion_tokens` | The number of tokens generated by the LLM in the completion. |
| `prompt_tokens` | The number of tokens provided in the prompt sent to the LLM. |
| `prompt_cached_tokens` | The number of cached tokens in the input prompt. |
| `speech_id` | A unique identifier representing a turn in the user input. |
| `total_tokens` | Total token usage for the completion. |
| `tokens_per_second` | The rate of token generation (tokens/second) by the LLM to generate the completion. |
| `ttft` | The amount of time (seconds) that it took for the LLM to generate the first token of the completion. |

#### Text-to-speech (TTS)

`TTSMetrics` is emitted after the TTS model generates speech from text input.

| Metric | Description |
| `audio_duration` | The duration (seconds) of the audio output generated by the TTS model. |
| `characters_count` | The number of characters in the text input to the TTS model. |
| `duration` | The amount of time (seconds) it took for the TTS model to generate the entire audio output. |
| `ttfb` | The amount of time (seconds) that it took for the TTS model to generate the first byte of its audio output. |
| `speech_id` | An identifier linking to a user's turn. |
| `streamed` | `True` if the TTS is in streaming mode. |

#### End-of-utterance (EOU)

`EOUMetrics` is emitted when the user is determined to have finished speaking. It includes metrics related to end-of-turn detection and transcription latency.

EOU metrics are available in Realtime APIs when `turn_detection` is set to VAD or LiveKit's turn detector plugin. When using server-side turn detection, `EOUMetrics` is not emitted.

| Metric | Description |
| `end_of_utterance_delay` | Time (in seconds) from the end of speech (as detected by VAD) to the point when the user's turn is considered complete. This includes any `transcription_delay`. |
| `transcription_delay` | Time (seconds) between the end of speech and when the final transcript is available. |
| `on_user_turn_completed_delay` | Time (in seconds) taken to execute the `on_user_turn_completed` callback. |
| `speech_id` | A unique identifier indicating the user's turn. |

### Measure conversation latency

Total conversation latency is the time it takes for the agent to respond to a user's utterance. Approximate it with the following metrics:

**Python**:

```python
total_latency = eou.end_of_utterance_delay + llm.ttft + tts.ttfb

```

---

**Node.js**:

```ts
const totalLatency = eou.endOfUtteranceDelay + llm.ttft + tts.ttfb;

```

## Session transcripts and reports

The `session.history` object contains the full conversation, and the SDK raises events like `conversation_item_added` and `user_input_transcribed` as turns progress. Use these hooks to build live dashboards or persist transcripts once a session ends. When you need a structured post-session artifact, call `ctx.make_session_report()` inside `on_session_end` to gather identifiers, history, events, and recording metadata in one JSON payload.

### Save conversation history example

The following Python example augments the [Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md) to save the transcript as JSON when the session closes.

** Filename: `agent.py`**

```python
from datetime import datetime
import json

def entrypoint(ctx: JobContext):
    async def write_transcript():
        current_date = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"/tmp/transcript_{ctx.room.name}_{current_date}.json"

        with open(filename, 'w') as f:
            json.dump(session.history.to_dict(), f, indent=2)

        print(f"Transcript for {ctx.room.name} saved to {filename}")

    ctx.add_shutdown_callback(write_transcript)
    # ... continue with ctx.connect(), agent setup, etc.

```

### Capture a session report

Available in:
- [ ] Node.js
- [x] Python

Use the `on_session_end` callback to capture a structured `SessionReport` with identifiers, conversation history, events, recording metadata, and agent configuration.

** Filename: `agent.py`**

```python
import json
from datetime import datetime
from livekit.agents import JobContext, AgentServer

server = AgentServer()

async def on_session_end(ctx: JobContext) -> None:
    report = ctx.make_session_report()
    report_dict = report.to_dict()

    current_date = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"/tmp/session_report_{ctx.room.name}_{current_date}.json"

    with open(filename, 'w') as f:
        json.dump(report_dict, f, indent=2)

    print(f"Session report for {ctx.room.name} saved to {filename}")

@server.rtc_session(on_session_end=on_session_end)
async def entrypoint(ctx: JobContext):
    await ctx.connect()
    # ...

```

The report includes fields such as:

- Job, room, and participant identifiers
- Complete conversation history with timestamps
- All session events (transcription, speech detection, handoffs, etc.)
- Audio recording metadata and paths (when recording is enabled)
- Agent session options and configuration

## Record audio or video

Use [LiveKit Egress](https://docs.livekit.io/home/egress/overview.md) to capture audio and video directly to your storage provider. The simplest pattern is to start a [room composite recorder](https://docs.livekit.io/home/egress/composite-recording.md) when your agent joins the room.

** Filename: `agent.py`**

```python
from livekit import api

async def entrypoint(ctx: JobContext):
    req = api.RoomCompositeEgressRequest(
        room_name=ctx.room.name,
        audio_only=True,
        file_outputs=[
            api.EncodedFileOutput(
                file_type=api.EncodedFileType.OGG,
                filepath="livekit/my-room-test.ogg",
                s3=api.S3Upload(
                    bucket=os.getenv("AWS_BUCKET_NAME"),
                    region=os.getenv("AWS_REGION"),
                    access_key=os.getenv("AWS_ACCESS_KEY_ID"),
                    secret=os.getenv("AWS_SECRET_ACCESS_KEY"),
                ),
            )
        ],
    )

    lkapi = api.LiveKitAPI()
    await lkapi.egress.start_room_composite_egress(req)
    await lkapi.aclose()
    # ... continue with your agent logic

```

** Filename: `agent.ts`**

```ts
import {
  EgressClient,
  EncodedFileOutput,
  EncodedFileType,
  EncodingOptionsPreset,
} from 'livekit-server-sdk';

const egressClient = new EgressClient(
  process.env.LIVEKIT_URL.replace('wss://', 'https://'),
  process.env.LIVEKIT_API_KEY,
  process.env.LIVEKIT_API_SECRET,
);

const output = new EncodedFileOutput({
  fileType: EncodedFileType.MP4,
  filepath: 'livekit/my-room-test.mp4',
  output: {
    case: 's3',
    value: {
      accessKey: process.env.AWS_ACCESS_KEY_ID,
      secret: process.env.AWS_SECRET_ACCESS_KEY,
      bucket: process.env.AWS_BUCKET_NAME,
      region: process.env.AWS_REGION,
      forcePathStyle: true,
    },
  },
});

export default defineAgent({
  entry: async (ctx: JobContext) => {
    await egressClient.startRoomCompositeEgress(
      ctx.room.name ?? 'open-room',
      output,
      {
        layout: 'grid',
        encodingOptions: EncodingOptionsPreset.H264_1080P_30,
        audioOnly: false,
      },
    );

    // ... continue with your agent logic
  },
});

```

## OpenTelemetry integration

Available in:
- [ ] Node.js
- [x] Python

Set a tracer provider to export the same spans used by LiveKit Cloud to any OpenTelemetry-compatible backend. The example below sends spans to LangFuse.

```python
import base64
import os

from livekit.agents.telemetry import set_tracer_provider

def setup_langfuse(
    host: str | None = None, public_key: str | None = None, secret_key: str | None = None
):
    from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
    from opentelemetry.sdk.trace import TracerProvider
    from opentelemetry.sdk.trace.export import BatchSpanProcessor

    public_key = public_key or os.getenv("LANGFUSE_PUBLIC_KEY")
    secret_key = secret_key or os.getenv("LANGFUSE_SECRET_KEY")
    host = host or os.getenv("LANGFUSE_HOST")

    if not public_key or not secret_key or not host:
        raise ValueError("LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY, and LANGFUSE_HOST must be set")

    langfuse_auth = base64.b64encode(f"{public_key}:{secret_key}".encode()).decode()
    os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = f"{host.rstrip('/')}/api/public/otel"
    os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {langfuse_auth}"

    trace_provider = TracerProvider()
    trace_provider.add_span_processor(BatchSpanProcessor(OTLPSpanExporter()))
    set_tracer_provider(trace_provider)

async def entrypoint(ctx: JobContext):
    setup_langfuse()
    # start your agent

```

For an end-to-end script, see the [LangFuse trace example on GitHub](https://github.com/livekit/agents/blob/main/examples/voice_agents/langfuse_trace.py).

---

This document was rendered at 2025-11-19T10:53:20.258Z.
For the latest version of this document, see [https://docs.livekit.io/agents/observability/data.md](https://docs.livekit.io/agents/observability/data.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).